{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 29 - QAEmb Karaoke Inspection (All Questions)\\n",
        "\\n",
        "This notebook inspects QAEmb scoring validity and renders one karaoke-style MP4 containing **all 29 QA questions** in a single grid.\\n",
        "\\n",
        "It uses existing QAEmb outputs plus the story transcript to show: active bin words, evolving timeseries per question, and per-panel current bin score.\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import time\n",
        "import textwrap\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "PROJECT_ROOT = Path('/flash/PaoU/seann/fmri-edm-ccm')\n",
        "os.chdir(PROJECT_ROOT)\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "from src.decoding import load_transcript_words\n",
        "from src.day19_category_builder import apply_smoothing_kernel, build_smoothing_kernel\n",
        "from src.utils import load_yaml\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration\n",
        "# -----------------------------\n",
        "SUBJECT = 'UTS01'\n",
        "STORY = 'wheretheressmoke'\n",
        "\n",
        "# Optional direct transcript file override (.csv/.tsv/.json/.TextGrid)\n",
        "TRANSCRIPT_PATH = None\n",
        "\n",
        "# Optional transcript root override (directory containing TextGrids/transcripts)\n",
        "TRANSCRIPTS_ROOT = Path('/flash/PaoU/seann/ds003020_copy/derivative/TextGrids')\n",
        "\n",
        "# QAEmb output root used by your pipeline artifacts\n",
        "QAEMB_ROOT = PROJECT_ROOT / 'features_qaemb' / 'featuresqaemb' / 'qaemb'\n",
        "\n",
        "USE_DOMAIN = 'tr'  # 'tr' or 'canonical'\n",
        "FPS = 8\n",
        "PLAYBACK_SPEED = 7.0\n",
        "WINDOW_SEC = 40.0\n",
        "PAD_LEFT_SEC = 0.5\n",
        "PAD_RIGHT_SEC = 2.0\n",
        "BIN_WORDS_MAX = 30\n",
        "Z_SCORE = False\n",
        "LOG_EVERY_FRAMES = None\n",
        "# 'qa_context' highlights the exact previous-n-gram context scored by QAEmb for the active token.\n",
        "# 'domain_bin' keeps the old TR/canonical bin highlight.\n",
        "HIGHLIGHT_MODE = 'qa_context'\n",
        "\n",
        "# Show all 29 questions in one video: 6x5 = 30 panels\n",
        "GRID_ROWS = 6\n",
        "GRID_COLS = 5\n",
        "# None => keep full question text in titles (no word truncation)\n",
        "TITLE_MAX_WORDS = None\n",
        "TITLE_WRAP_CHARS = 42\n",
        "\n",
        "VIDEO_DIR = QAEMB_ROOT.parent / 'videos'\n",
        "VIDEO_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUT_MP4 = VIDEO_DIR / f'karaoke_qaemb_allq_{SUBJECT}_{STORY}.mp4'\n",
        "\n",
        "# -----------------------------\n",
        "# Load artifacts\n",
        "# -----------------------------\n",
        "subject_dir = QAEMB_ROOT / 'subjects' / SUBJECT / STORY\n",
        "story_dir = QAEMB_ROOT / 'stories' / STORY\n",
        "token_dir = QAEMB_ROOT / 'tokens' / SUBJECT\n",
        "\n",
        "tr_path = subject_dir / 'qaemb_timeseries.csv'\n",
        "canonical_path = story_dir / 'qaemb_timeseries_seconds.csv'\n",
        "meta_path = subject_dir / 'qaemb_metadata.json'\n",
        "questions_path = token_dir / f'{STORY}_qaemb_questions.json'\n",
        "qa_tokens_path = token_dir / f'{STORY}_qaemb_tokens.npy'\n",
        "\n",
        "required = [tr_path, canonical_path, meta_path, questions_path, qa_tokens_path]\n",
        "missing = [str(p) for p in required if not p.exists()]\n",
        "if missing:\n",
        "    raise FileNotFoundError('Missing required QAEmb artifacts:\\n' + '\\n'.join(missing))\n",
        "\n",
        "tr_df = pd.read_csv(tr_path)\n",
        "canonical_df = pd.read_csv(canonical_path)\n",
        "metadata = json.loads(meta_path.read_text())\n",
        "questions = json.loads(questions_path.read_text())\n",
        "qa_matrix = np.load(qa_tokens_path)\n",
        "\n",
        "qa_cols = [c for c in tr_df.columns if c.startswith('qa_q')]\n",
        "qa_cols = sorted(qa_cols)\n",
        "\n",
        "if not qa_cols:\n",
        "    raise ValueError(f'No QA columns found in {tr_path}')\n",
        "\n",
        "if len(qa_cols) != len(questions):\n",
        "    raise ValueError(f'Question count mismatch: columns={len(qa_cols)}, questions={len(questions)}')\n",
        "\n",
        "if qa_matrix.shape[1] != len(questions):\n",
        "    raise ValueError(f'qa_matrix columns mismatch: matrix={qa_matrix.shape[1]}, questions={len(questions)}')\n",
        "\n",
        "if GRID_ROWS * GRID_COLS < len(qa_cols):\n",
        "    raise ValueError(f'Grid too small for all questions: {GRID_ROWS}x{GRID_COLS} < {len(qa_cols)}')\n",
        "\n",
        "print('Loaded artifacts:')\n",
        "print(f'  TR series:         {tr_path} -> {tr_df.shape}')\n",
        "print(f'  Canonical series:  {canonical_path} -> {canonical_df.shape}')\n",
        "print(f'  Token QA matrix:   {qa_tokens_path} -> {qa_matrix.shape}')\n",
        "print(f'  Questions:         {len(questions)}')\n",
        "print(f'  Metadata checkpoint: {metadata.get(\"checkpoint\")}')\n",
        "CONTEXT_NGRAM = int(metadata.get('context_ngram', 10))\n",
        "TIME_ANCHOR = str(metadata.get('time_anchor', 'onset')).lower()\n",
        "print(f'  QA context ngram: {CONTEXT_NGRAM} | time_anchor: {TIME_ANCHOR}')\n",
        "\n",
        "# -----------------------------\n",
        "# Load transcript tokens\n",
        "# -----------------------------\n",
        "def _load_textgrid_events(path: Path):\n",
        "    try:\n",
        "        import textgrid as _textgrid\n",
        "    except Exception as exc:\n",
        "        raise ImportError(\n",
        "            'Reading .TextGrid requires the `textgrid` package in this notebook kernel. '\n",
        "            'Install it (e.g., `pip install textgrid`) or provide CSV/TSV/JSON via TRANSCRIPT_PATH.'\n",
        "        ) from exc\n",
        "\n",
        "    tg = _textgrid.TextGrid.fromFile(str(path))\n",
        "    events = []\n",
        "    for tier in tg:\n",
        "        name = str(getattr(tier, 'name', '')).lower()\n",
        "        if 'word' not in name:\n",
        "            continue\n",
        "        for item in tier:\n",
        "            mark = str(getattr(item, 'mark', '')).strip()\n",
        "            if not mark:\n",
        "                continue\n",
        "            start = float(getattr(item, 'minTime', 0.0))\n",
        "            end = float(getattr(item, 'maxTime', start))\n",
        "            events.append((mark, start, end))\n",
        "    if not events:\n",
        "        raise ValueError(f'No word-tier events parsed from TextGrid: {path}')\n",
        "    events.sort(key=lambda x: x[1])\n",
        "    return events\n",
        "\n",
        "\n",
        "def _story_variants(story: str):\n",
        "    variants = {\n",
        "        story,\n",
        "        story.replace(' ', ''),\n",
        "        story.replace('-', ''),\n",
        "        story.replace('_', ''),\n",
        "        story.lower(),\n",
        "        story.lower().replace(' ', ''),\n",
        "        story.lower().replace('-', ''),\n",
        "        story.lower().replace('_', ''),\n",
        "    }\n",
        "    return sorted(v for v in variants if v)\n",
        "\n",
        "\n",
        "def _load_transcript_from_explicit_path(path: Path):\n",
        "    suffix = path.suffix.lower()\n",
        "    if suffix == '.json':\n",
        "        data = json.loads(path.read_text())\n",
        "        events = [\n",
        "            (\n",
        "                str(entry['word']),\n",
        "                float(entry['onset']),\n",
        "                float(entry.get('offset', entry['onset'])),\n",
        "            )\n",
        "            for entry in data\n",
        "        ]\n",
        "    elif suffix in {'.csv', '.tsv'}:\n",
        "        delimiter = '\t' if suffix == '.tsv' else ','\n",
        "        with path.open('r', newline='') as fh:\n",
        "            reader = csv.DictReader(fh, delimiter=delimiter)\n",
        "            events = [\n",
        "                (\n",
        "                    str(row['word']),\n",
        "                    float(row['onset']),\n",
        "                    float(row.get('offset', row['onset'])),\n",
        "                )\n",
        "                for row in reader\n",
        "            ]\n",
        "    elif suffix == '.textgrid':\n",
        "        events = _load_textgrid_events(path)\n",
        "    else:\n",
        "        raise ValueError(f'Unsupported transcript extension: {suffix}')\n",
        "    events.sort(key=lambda x: x[1])\n",
        "    return events\n",
        "cfg = load_yaml(PROJECT_ROOT / 'configs' / 'demo.yaml')\n",
        "paths = cfg.get('paths', {}) or {}\n",
        "\n",
        "if TRANSCRIPTS_ROOT is not None:\n",
        "    tr_root = Path(TRANSCRIPTS_ROOT)\n",
        "    if not tr_root.exists():\n",
        "        raise FileNotFoundError(f'TRANSCRIPTS_ROOT does not exist: {tr_root}')\n",
        "    # load_transcript_words checks paths['transcripts'] first via candidate path generation.\n",
        "    paths = dict(paths)\n",
        "    paths['transcripts'] = str(tr_root)\n",
        "\n",
        "if TRANSCRIPT_PATH is not None:\n",
        "    transcript_events = _load_transcript_from_explicit_path(Path(TRANSCRIPT_PATH))\n",
        "    print(f'Loaded transcript from explicit path: {TRANSCRIPT_PATH}')\n",
        "else:\n",
        "    try:\n",
        "        transcript_events = load_transcript_words(paths, SUBJECT, STORY)\n",
        "        print('Loaded transcript via load_transcript_words(paths, subject, story).')\n",
        "        if TRANSCRIPTS_ROOT is not None:\n",
        "            print(f'  transcript root override: {Path(TRANSCRIPTS_ROOT)}')\n",
        "    except FileNotFoundError as exc:\n",
        "        if TRANSCRIPTS_ROOT is None:\n",
        "            raise\n",
        "\n",
        "        tr_root = Path(TRANSCRIPTS_ROOT)\n",
        "        candidates = []\n",
        "        for base in _story_variants(STORY):\n",
        "            for ext in ('.TextGrid', '.textgrid', '.csv', '.tsv', '.json'):\n",
        "                candidates.append(tr_root / f'{base}{ext}')\n",
        "        existing = [p for p in candidates if p.exists()]\n",
        "\n",
        "        if existing:\n",
        "            chosen = existing[0]\n",
        "            transcript_events = _load_transcript_from_explicit_path(chosen)\n",
        "            print('load_transcript_words fallback activated.')\n",
        "            print(f'  loaded from explicit candidate: {chosen}')\n",
        "        else:\n",
        "            sample = sorted(p.name for p in tr_root.glob('*') if p.is_file())[:20]\n",
        "            sample_txt = '\\n'.join(sample) if sample else '<no files>'\n",
        "            raise FileNotFoundError(\n",
        "                f\"{exc}\\n\"\n",
        "                f\"TRANSCRIPTS_ROOT={tr_root}\\n\"\n",
        "                f\"No candidate transcript matched story='{STORY}'.\\n\"\n",
        "                f\"Sample files:\\n{sample_txt}\"\n",
        "            )\n",
        "if not transcript_events:\n",
        "    raise ValueError('Transcript is empty.')\n",
        "\n",
        "token_df = pd.DataFrame(transcript_events, columns=['word', 'start', 'end'])\n",
        "token_df['word'] = token_df['word'].astype(str).str.strip()\n",
        "token_df = token_df[token_df['word'] != ''].reset_index(drop=True)\n",
        "token_df['midpoint'] = 0.5 * (token_df['start'] + token_df['end'])\n",
        "token_df['duration'] = token_df['end'] - token_df['start']\n",
        "token_df['token_index'] = np.arange(len(token_df), dtype=int)\n",
        "\n",
        "if len(token_df) != qa_matrix.shape[0]:\n",
        "    raise ValueError(\n",
        "        'Token count mismatch between transcript and qa_matrix. '\n",
        "        f'transcript_nonempty={len(token_df)}, qa_matrix_rows={qa_matrix.shape[0]}. '\n",
        "        'Use TRANSCRIPT_PATH to force the exact transcript used during QAEmb generation if needed.'\n",
        "    )\n",
        "\n",
        "print(f'Transcript tokens (non-empty): {len(token_df)}')\n",
        "\n",
        "# -----------------------------\n",
        "# Validate saved QA series against recomputation from token matrix\n",
        "# -----------------------------\n",
        "def _corr(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    m = np.isfinite(a) & np.isfinite(b)\n",
        "    if m.sum() < 2:\n",
        "        return np.nan\n",
        "    aa = a[m]\n",
        "    bb = b[m]\n",
        "    sa = aa.std()\n",
        "    sb = bb.std()\n",
        "    if sa == 0 or sb == 0:\n",
        "        return np.nan\n",
        "    return float(np.corrcoef(aa, bb)[0, 1])\n",
        "\n",
        "\n",
        "def _resample_irregular_to_edges(token_times: np.ndarray, token_values: np.ndarray, target_edges: np.ndarray) -> np.ndarray:\n",
        "    if token_values.size == 0:\n",
        "        return np.empty((len(target_edges) - 1, 0), dtype=float)\n",
        "    order = np.argsort(token_times)\n",
        "    times_sorted = token_times[order]\n",
        "    values_sorted = token_values[order]\n",
        "    centers = 0.5 * (target_edges[:-1] + target_edges[1:])\n",
        "    out = np.empty((centers.size, token_values.shape[1]), dtype=float)\n",
        "    for j in range(token_values.shape[1]):\n",
        "        out[:, j] = np.interp(\n",
        "            centers,\n",
        "            times_sorted,\n",
        "            values_sorted[:, j],\n",
        "            left=values_sorted[0, j],\n",
        "            right=values_sorted[-1, j],\n",
        "        )\n",
        "    return out\n",
        "\n",
        "\n",
        "def _sample_tokens_to_edges_step(token_times: np.ndarray, token_values: np.ndarray, target_edges: np.ndarray) -> np.ndarray:\n",
        "    if token_values.size == 0:\n",
        "        return np.empty((len(target_edges) - 1, 0), dtype=float)\n",
        "    order = np.argsort(token_times, kind='stable')\n",
        "    times_sorted = token_times[order]\n",
        "    values_sorted = token_values[order]\n",
        "    centers = 0.5 * (target_edges[:-1] + target_edges[1:])\n",
        "    idx = np.searchsorted(times_sorted, centers, side='right') - 1\n",
        "    idx = np.clip(idx, 0, len(times_sorted) - 1)\n",
        "    return values_sorted[idx].astype(float, copy=True)\n",
        "\n",
        "\n",
        "def _build_edges_from_df(df: pd.DataFrame) -> np.ndarray:\n",
        "    starts = df['start_sec'].to_numpy(dtype=float)\n",
        "    ends = df['end_sec'].to_numpy(dtype=float)\n",
        "    if len(starts) == 0:\n",
        "        raise ValueError('No rows in timeseries dataframe.')\n",
        "    edges = np.concatenate([starts, [ends[-1]]])\n",
        "    if not np.all(np.diff(edges) > 0):\n",
        "        raise ValueError('Non-monotone edges detected in series dataframe.')\n",
        "    return edges\n",
        "\n",
        "tr_edges = _build_edges_from_df(tr_df)\n",
        "canonical_edges = _build_edges_from_df(canonical_df)\n",
        "\n",
        "anchor_mode = str(metadata.get('time_anchor', 'onset')).lower()\n",
        "if anchor_mode not in {'onset', 'midpoint'}:\n",
        "    anchor_mode = 'onset'\n",
        "\n",
        "if anchor_mode == 'midpoint':\n",
        "    token_times = token_df['midpoint'].to_numpy(dtype=float)\n",
        "else:\n",
        "    token_times = token_df['start'].to_numpy(dtype=float)\n",
        "\n",
        "resample_method = str(metadata.get('resample_method', 'interp_linear')).lower()\n",
        "if resample_method in {'step_previous', 'step', 'previous'}:\n",
        "    expected_raw = _sample_tokens_to_edges_step(token_times, qa_matrix, tr_edges)\n",
        "else:\n",
        "    expected_raw = _resample_irregular_to_edges(token_times, qa_matrix, tr_edges)\n",
        "\n",
        "tr_seconds = float(metadata.get('tr_seconds', cfg.get('TR', 2.0)))\n",
        "smoothing_seconds = float(metadata.get('smoothing_seconds', 1.0))\n",
        "smoothing_method = str(metadata.get('smoothing_method', 'moving_average')).lower()\n",
        "gaussian_sigma_seconds = metadata.get('gaussian_sigma_seconds', None)\n",
        "pad_mode = str(metadata.get('smoothing_pad_mode', 'reflect'))\n",
        "postprocess = str(metadata.get('postprocess', '')).lower()\n",
        "\n",
        "if postprocess == 'none' or smoothing_seconds <= 0:\n",
        "    expected_selected = expected_raw.copy()\n",
        "else:\n",
        "    kernel = build_smoothing_kernel(\n",
        "        tr_seconds,\n",
        "        smoothing_seconds,\n",
        "        method=smoothing_method,\n",
        "        gaussian_sigma_seconds=gaussian_sigma_seconds,\n",
        "    )\n",
        "    if expected_raw.size and kernel.size > 1:\n",
        "        expected_selected = apply_smoothing_kernel(expected_raw, kernel, pad_mode=pad_mode)\n",
        "    else:\n",
        "        expected_selected = expected_raw.copy()\n",
        "\n",
        "observed_tr = tr_df[qa_cols].to_numpy(dtype=float)\n",
        "observed_canonical = canonical_df[qa_cols].to_numpy(dtype=float)\n",
        "\n",
        "if observed_tr.shape != expected_selected.shape:\n",
        "    raise ValueError(f'Shape mismatch for TR validation: observed={observed_tr.shape}, expected={expected_selected.shape}')\n",
        "\n",
        "if observed_canonical.shape != expected_selected.shape:\n",
        "    raise ValueError(f'Shape mismatch for canonical validation: observed={observed_canonical.shape}, expected={expected_selected.shape}')\n",
        "\n",
        "rows = []\n",
        "for j, col in enumerate(qa_cols):\n",
        "    obs = observed_tr[:, j]\n",
        "    exp = expected_selected[:, j]\n",
        "    diff = obs - exp\n",
        "    m = np.isfinite(diff)\n",
        "    rows.append(\n",
        "        {\n",
        "            'column': col,\n",
        "            'question': questions[j],\n",
        "            'corr_tr_vs_expected': _corr(obs, exp),\n",
        "            'mae_tr_vs_expected': float(np.nanmean(np.abs(diff[m]))) if m.any() else np.nan,\n",
        "            'max_abs_tr_vs_expected': float(np.nanmax(np.abs(diff[m]))) if m.any() else np.nan,\n",
        "            'corr_canonical_vs_expected': _corr(observed_canonical[:, j], exp),\n",
        "            'mean_tr_score': float(np.nanmean(obs)),\n",
        "            'std_tr_score': float(np.nanstd(obs)),\n",
        "        }\n",
        "    )\n",
        "\n",
        "validation_df = pd.DataFrame(rows)\n",
        "\n",
        "print('\\nValidation summary:')\n",
        "print(f'  global max abs diff (TR vs expected): {np.nanmax(np.abs(observed_tr - expected_selected)):.6g}')\n",
        "print(f'  global mean abs diff (TR vs expected): {np.nanmean(np.abs(observed_tr - expected_selected)):.6g}')\n",
        "print(f'  global max abs diff (canonical vs expected): {np.nanmax(np.abs(observed_canonical - expected_selected)):.6g}')\n",
        "\n",
        "print('\\nWorst 10 questions by MAE (TR vs expected):')\n",
        "display(validation_df.sort_values('mae_tr_vs_expected', ascending=False).head(10))\n",
        "\n",
        "# -----------------------------\n",
        "# Karaoke rendering with all questions in one video\n",
        "# -----------------------------\n",
        "def abbreviate_question(q: str, max_words: int | None = None) -> str:\n",
        "    q = str(q).strip()\n",
        "    if q.endswith('?'):\n",
        "        q = q[:-1]\n",
        "    if max_words is None:\n",
        "        return q\n",
        "    words = q.split()\n",
        "    return ' '.join(words[:max_words])\n",
        "\n",
        "\n",
        "def make_karaoke_qaemb_video_all_questions(\n",
        "    *,\n",
        "    token_df: pd.DataFrame,\n",
        "    tr_df: pd.DataFrame,\n",
        "    canonical_df: pd.DataFrame,\n",
        "    tr_edges: np.ndarray,\n",
        "    canonical_edges: np.ndarray,\n",
        "    qa_cols: list,\n",
        "    questions: list,\n",
        "    out_mp4: Path,\n",
        "    use_domain: str = 'tr',\n",
        "    fps: int = 8,\n",
        "    playback_speed: float = 7.0,\n",
        "    window_sec: float = 40.0,\n",
        "    pad_left_sec: float = 0.5,\n",
        "    pad_right_sec: float = 2.0,\n",
        "    bin_words_max: int | None = 30,\n",
        "    zscore: bool = False,\n",
        "    rows: int = 6,\n",
        "    cols: int = 5,\n",
        "    title_max_words: int | None = None,\n",
        "    title_wrap_chars: int = 42,\n",
        "    log_every_frames: int | None = None,\n",
        "    highlight_mode: str = 'qa_context',\n",
        "    context_ngram: int = 10,\n",
        "    time_anchor: str = 'onset',\n",
        "):\n",
        "    if use_domain not in {'tr', 'canonical'}:\n",
        "        raise ValueError(\"use_domain must be 'tr' or 'canonical'\")\n",
        "    if rows * cols < len(qa_cols):\n",
        "        raise ValueError(f'Grid {rows}x{cols} too small for {len(qa_cols)} questions')\n",
        "    if fps <= 0:\n",
        "        raise ValueError('fps must be > 0')\n",
        "    if playback_speed <= 0:\n",
        "        raise ValueError('playback_speed must be > 0')\n",
        "    if highlight_mode not in {'qa_context', 'domain_bin'}:\n",
        "        raise ValueError(\"highlight_mode must be 'qa_context' or 'domain_bin'\")\n",
        "    if context_ngram < 1:\n",
        "        raise ValueError('context_ngram must be >= 1')\n",
        "    time_anchor = str(time_anchor).lower()\n",
        "    if time_anchor not in {'onset', 'midpoint'}:\n",
        "        raise ValueError(\"time_anchor must be 'onset' or 'midpoint'\")\n",
        "\n",
        "    if use_domain == 'canonical':\n",
        "        df = canonical_df\n",
        "        edges = canonical_edges\n",
        "        t = 0.5 * (canonical_edges[:-1] + canonical_edges[1:])\n",
        "    else:\n",
        "        df = tr_df\n",
        "        edges = tr_edges\n",
        "        t = tr_edges[:-1]\n",
        "\n",
        "    t = np.asarray(t, dtype=float)\n",
        "    edges = np.asarray(edges, dtype=float)\n",
        "\n",
        "    Y = np.vstack([df[c].to_numpy(dtype=float) for c in qa_cols])\n",
        "    if zscore:\n",
        "        mean = np.nanmean(Y, axis=1, keepdims=True)\n",
        "        std = np.nanstd(Y, axis=1, keepdims=True)\n",
        "        std = np.where(np.isfinite(std) & (std > 0), std, 1.0)\n",
        "        Y = (Y - mean) / std\n",
        "\n",
        "    ylims = []\n",
        "    for i in range(Y.shape[0]):\n",
        "        vals = Y[i]\n",
        "        finite = vals[np.isfinite(vals)]\n",
        "        if finite.size == 0:\n",
        "            ylims.append((-1.0, 1.0))\n",
        "            continue\n",
        "        lo = float(np.min(finite))\n",
        "        hi = float(np.max(finite))\n",
        "        if np.isclose(lo, hi):\n",
        "            pad = 1.0 if lo == 0 else abs(lo) * 0.1\n",
        "        else:\n",
        "            pad = 0.05 * (hi - lo)\n",
        "        ylims.append((lo - pad, hi + pad))\n",
        "\n",
        "    words = token_df['word'].astype(str).tolist()\n",
        "    starts = token_df['start'].to_numpy(dtype=float)\n",
        "    ends = token_df['end'].to_numpy(dtype=float)\n",
        "    anchors = starts if time_anchor == 'onset' else 0.5 * (starts + ends)\n",
        "\n",
        "    transcript_end = float(token_df['end'].max())\n",
        "    video_duration = (transcript_end + pad_right_sec) / playback_speed\n",
        "    n_frames = int(np.ceil(video_duration * fps))\n",
        "\n",
        "    if log_every_frames is None:\n",
        "        log_every_frames = max(1, int(fps * 5))\n",
        "\n",
        "    fig = plt.figure(figsize=(24, 14), dpi=130)\n",
        "    gs = fig.add_gridspec(rows + 1, cols, height_ratios=[0.95] + [1.0] * rows, hspace=0.36, wspace=0.24)\n",
        "\n",
        "    ax_text = fig.add_subplot(gs[0, :])\n",
        "    ax_text.axis('off')\n",
        "\n",
        "    axes = []\n",
        "    for r in range(1, rows + 1):\n",
        "        for c in range(cols):\n",
        "            axes.append(fig.add_subplot(gs[r, c]))\n",
        "\n",
        "    lines = []\n",
        "    cursors = []\n",
        "    spans = []\n",
        "    val_texts = []\n",
        "\n",
        "    title_labels = []\n",
        "    for i, q in enumerate(questions):\n",
        "        q_text = abbreviate_question(q, max_words=title_max_words)\n",
        "        if title_wrap_chars and title_wrap_chars > 0:\n",
        "            q_text = textwrap.fill(q_text, width=title_wrap_chars)\n",
        "        title_labels.append(f'{qa_cols[i]}\\n{q_text}')\n",
        "\n",
        "    for i, ax in enumerate(axes):\n",
        "        if i >= len(qa_cols):\n",
        "            ax.axis('off')\n",
        "            lines.append(None)\n",
        "            cursors.append(None)\n",
        "            spans.append(None)\n",
        "            val_texts.append(None)\n",
        "            continue\n",
        "\n",
        "        ax.set_title(title_labels[i], fontsize=6, linespacing=1.05, pad=2.5)\n",
        "        ax.set_xlim(0, window_sec)\n",
        "        ax.set_ylim(*ylims[i])\n",
        "        ax.grid(True, alpha=0.22)\n",
        "        (line,) = ax.plot([], [], linewidth=1.2)\n",
        "        cursor = ax.axvline(0, linewidth=0.9, alpha=0.8)\n",
        "        span = mpatches.Rectangle(\n",
        "            (0, 0),\n",
        "            0,\n",
        "            1,\n",
        "            transform=ax.get_xaxis_transform(),\n",
        "            facecolor='#f4c542',\n",
        "            alpha=0.22,\n",
        "            zorder=0,\n",
        "        )\n",
        "        ax.add_patch(span)\n",
        "        val_text = ax.text(\n",
        "            0.98,\n",
        "            0.92,\n",
        "            '',\n",
        "            transform=ax.transAxes,\n",
        "            ha='right',\n",
        "            va='top',\n",
        "            fontsize=6,\n",
        "            bbox={'facecolor': 'white', 'alpha': 0.55, 'edgecolor': 'none', 'pad': 1.2},\n",
        "        )\n",
        "\n",
        "        lines.append(line)\n",
        "        cursors.append(cursor)\n",
        "        spans.append(span)\n",
        "        val_texts.append(val_text)\n",
        "\n",
        "    txt_bin = ax_text.text(0.01, 0.74, '', fontsize=15, fontweight='bold', ha='left', va='center')\n",
        "    txt_words = ax_text.text(0.01, 0.38, '', fontsize=13, ha='left', va='center')\n",
        "    txt_meta = ax_text.text(0.01, 0.09, '', fontsize=11, ha='left', va='center', alpha=0.85)\n",
        "\n",
        "    start_wall = None\n",
        "    last_progress_len = 0\n",
        "    max_rendered_frame = -1\n",
        "\n",
        "    def _bin_idx(t_now: float) -> int:\n",
        "        idx = int(np.searchsorted(edges, t_now, side='right') - 1)\n",
        "        return int(np.clip(idx, 0, len(edges) - 2))\n",
        "\n",
        "    def _qa_context_window(t_now: float):\n",
        "        token_idx = int(np.searchsorted(anchors, t_now, side='right') - 1)\n",
        "        token_idx = int(np.clip(token_idx, 0, len(anchors) - 1))\n",
        "\n",
        "        ctx_start_idx = max(0, token_idx + 1 - int(context_ngram))\n",
        "        if token_idx > ctx_start_idx:\n",
        "            context_idxs = np.arange(ctx_start_idx, token_idx, dtype=int)\n",
        "            h0 = float(starts[ctx_start_idx])\n",
        "            h1 = float(ends[token_idx - 1])\n",
        "            context_words = [words[k] for k in context_idxs]\n",
        "        else:\n",
        "            context_idxs = np.empty((0,), dtype=int)\n",
        "            h0 = float(anchors[token_idx])\n",
        "            h1 = h0\n",
        "            context_words = []\n",
        "\n",
        "        return token_idx, context_idxs, h0, h1, context_words\n",
        "\n",
        "    def _format_words(bin_words: list[str]) -> str:\n",
        "        if not bin_words:\n",
        "            return '(no words in active bin)'\n",
        "        if bin_words_max is not None and len(bin_words) > bin_words_max:\n",
        "            return ' '.join(bin_words[:bin_words_max]) + ' ...'\n",
        "        return ' '.join(bin_words)\n",
        "\n",
        "    def init():\n",
        "        artists = []\n",
        "        for i in range(len(axes)):\n",
        "            if lines[i] is None:\n",
        "                continue\n",
        "            lines[i].set_data([], [])\n",
        "            val_texts[i].set_text('')\n",
        "            artists.extend([lines[i], cursors[i], spans[i], val_texts[i]])\n",
        "        txt_bin.set_text('')\n",
        "        txt_words.set_text('')\n",
        "        txt_meta.set_text('')\n",
        "        artists.extend([txt_bin, txt_words, txt_meta])\n",
        "        return artists\n",
        "\n",
        "    def update(frame: int):\n",
        "        nonlocal start_wall\n",
        "        if start_wall is None:\n",
        "            start_wall = time.perf_counter()\n",
        "\n",
        "        t_now = (frame / fps) * playback_speed\n",
        "        w0 = max(0.0, t_now - window_sec + pad_left_sec)\n",
        "        w1 = w0 + window_sec\n",
        "\n",
        "        mask = (t >= w0) & (t <= w1)\n",
        "        if not np.any(mask):\n",
        "            return []\n",
        "\n",
        "        tw = t[mask] - w0\n",
        "\n",
        "        b = _bin_idx(t_now)\n",
        "        b0 = float(edges[b])\n",
        "        b1 = float(edges[b + 1])\n",
        "\n",
        "        if highlight_mode == 'qa_context':\n",
        "            token_idx, context_idxs, h0, h1, bin_words = _qa_context_window(t_now)\n",
        "            view0 = max(h0, w0)\n",
        "            view1 = min(h1, w1)\n",
        "            span_x = view0 - w0\n",
        "            span_w = max(0.0, view1 - view0)\n",
        "            highlight_label = (\n",
        "                f'QA context token={token_idx} anchor={anchors[token_idx]:6.2f}s '\n",
        "                f'({len(context_idxs)} prior words)'\n",
        "            )\n",
        "            token_label = words[token_idx] if 0 <= token_idx < len(words) else '<na>'\n",
        "        else:\n",
        "            view0 = max(b0, w0)\n",
        "            view1 = min(b1, w1)\n",
        "            span_x = view0 - w0\n",
        "            span_w = max(0.0, view1 - view0)\n",
        "            bin_mask = (starts < b1) & (ends > b0)\n",
        "            bin_words = [words[i] for i in np.where(bin_mask)[0]]\n",
        "            highlight_label = f'Bin {b0:6.2f}-{b1:6.2f}s'\n",
        "            token_label = '<domain-bin>'\n",
        "\n",
        "        current_scores = Y[:, b]\n",
        "\n",
        "        top_idx = np.argsort(np.nan_to_num(current_scores, nan=-np.inf))[::-1][:5]\n",
        "        top_text = ' | '.join(\n",
        "            [f'q{j:02d}:{current_scores[j]:.2f}' for j in top_idx if np.isfinite(current_scores[j])]\n",
        "        )\n",
        "\n",
        "        artists = []\n",
        "        for i in range(len(axes)):\n",
        "            if lines[i] is None:\n",
        "                continue\n",
        "            yw = Y[i, mask]\n",
        "            lines[i].set_data(tw, yw)\n",
        "            cursors[i].set_xdata([t_now - w0, t_now - w0])\n",
        "            spans[i].set_x(span_x)\n",
        "            spans[i].set_width(span_w)\n",
        "            val = current_scores[i]\n",
        "            val_texts[i].set_text(f'{val:.2f}' if np.isfinite(val) else 'nan')\n",
        "            artists.extend([lines[i], cursors[i], spans[i], val_texts[i]])\n",
        "\n",
        "        txt_bin.set_text(f'{highlight_label} | {len(bin_words)} words')\n",
        "        txt_words.set_text(_format_words(bin_words))\n",
        "        txt_meta.set_text(\n",
        "            f't={t_now:6.2f}s | bin {b + 1}/{len(edges) - 1} | mode={highlight_mode} | '\n",
        "            f'token={token_label} | top5 {top_text}'\n",
        "        )\n",
        "\n",
        "        nonlocal last_progress_len, max_rendered_frame\n",
        "        max_rendered_frame = max(max_rendered_frame, frame)\n",
        "\n",
        "        # Inline progress tracker (single-line refresh) so render health is always visible.\n",
        "        if frame % log_every_frames == 0 or frame == n_frames - 1:\n",
        "            elapsed = time.perf_counter() - start_wall\n",
        "            progress = (frame + 1) / n_frames\n",
        "            eta = elapsed / progress - elapsed if progress > 0 else float('inf')\n",
        "            msg = (\n",
        "                f'Rendered {frame + 1}/{n_frames} frames '\n",
        "                f'({progress * 100:5.1f}%) | elapsed {elapsed / 60:5.1f}m | ETA {eta / 60:5.1f}m'\n",
        "            )\n",
        "            print('\\r' + msg.ljust(last_progress_len), end='', flush=True)\n",
        "            last_progress_len = max(last_progress_len, len(msg))\n",
        "            if frame == n_frames - 1:\n",
        "                print()\n",
        "\n",
        "        artists.extend([txt_bin, txt_words, txt_meta])\n",
        "        return artists\n",
        "\n",
        "    anim = animation.FuncAnimation(\n",
        "        fig,\n",
        "        update,\n",
        "        init_func=init,\n",
        "        frames=n_frames,\n",
        "        interval=1000 / fps,\n",
        "        blit=False,\n",
        "    )\n",
        "\n",
        "    writer = animation.FFMpegWriter(\n",
        "        fps=fps,\n",
        "        codec='libx264',\n",
        "        bitrate=7000,\n",
        "        extra_args=['-pix_fmt', 'yuv420p', '-movflags', '+faststart'],\n",
        "    )\n",
        "\n",
        "    out_mp4.parent.mkdir(parents=True, exist_ok=True)\n",
        "    anim.save(str(out_mp4), writer=writer)\n",
        "    plt.close(fig)\n",
        "\n",
        "    rendered_frames = max_rendered_frame + 1\n",
        "    if rendered_frames != n_frames:\n",
        "        print(\n",
        "            f'WARNING: expected {n_frames} frames but update() saw {rendered_frames}. '\n",
        "            'Video was still written; inspect output if this is unexpected.'\n",
        "        )\n",
        "    else:\n",
        "        print(f'Frame render check: {rendered_frames}/{n_frames} frames rendered.')\n",
        "\n",
        "    print(f'Saved: {out_mp4}')\n",
        "\n",
        "\n",
        "if shutil.which('ffmpeg') is None:\n",
        "    raise RuntimeError('ffmpeg not found on PATH. Install ffmpeg before rendering MP4.')\n",
        "\n",
        "make_karaoke_qaemb_video_all_questions(\n",
        "    token_df=token_df,\n",
        "    tr_df=tr_df,\n",
        "    canonical_df=canonical_df,\n",
        "    tr_edges=tr_edges,\n",
        "    canonical_edges=canonical_edges,\n",
        "    qa_cols=qa_cols,\n",
        "    questions=questions,\n",
        "    out_mp4=OUT_MP4,\n",
        "    use_domain=USE_DOMAIN,\n",
        "    fps=FPS,\n",
        "    playback_speed=PLAYBACK_SPEED,\n",
        "    window_sec=WINDOW_SEC,\n",
        "    pad_left_sec=PAD_LEFT_SEC,\n",
        "    pad_right_sec=PAD_RIGHT_SEC,\n",
        "    bin_words_max=BIN_WORDS_MAX,\n",
        "    zscore=Z_SCORE,\n",
        "    rows=GRID_ROWS,\n",
        "    cols=GRID_COLS,\n",
        "    title_max_words=TITLE_MAX_WORDS,\n",
        "    title_wrap_chars=TITLE_WRAP_CHARS,\n",
        "    log_every_frames=LOG_EVERY_FRAMES,\n",
        "    highlight_mode=HIGHLIGHT_MODE,\n",
        "    context_ngram=CONTEXT_NGRAM,\n",
        "    time_anchor=TIME_ANCHOR,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}