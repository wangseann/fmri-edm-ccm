{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc4abb7",
   "metadata": {},
   "source": [
    "# Day 23 \u2013 Transcript Repair for TextGrid Imports\n",
    "\n",
    "Convert the problematic story transcripts into standard Praat TextGrid files so downstream\n",
    "notebooks stop skipping them. This workflow normalizes headers, resolves floating-point\n",
    "overlaps, and re-serializes the legacy \"Praat chronological\" format into the short\n",
    "TextGrid representation used elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99986785",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "project_root = Path('/flash/PaoU/seann/fmri-edm-ccm')\n",
    "project_root.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(project_root)\n",
    "\n",
    "sys.path.append(str(project_root))\n",
    "sys.path.append('/flash/PaoU/seann/pyEDM/src')\n",
    "sys.path.append('/flash/PaoU/seann/MDE-main/src')\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    def display(obj):  # type: ignore\n",
    "        print(obj)\n",
    "\n",
    "try:\n",
    "    import textgrid  # type: ignore\n",
    "except Exception as exc:\n",
    "    raise ImportError('textgrid package is required to repair transcripts.') from exc\n",
    "\n",
    "from src.utils import load_yaml\n",
    "\n",
    "np.random.seed(42)\n",
    "pd.options.display.max_columns = 80\n",
    "\n",
    "cfg = load_yaml('configs/demo.yaml')\n",
    "paths = dict(cfg.get('paths', {}) or {})\n",
    "data_root = Path(paths.get('data_root', '/bucket/PaoU/seann/openneuro/ds003020'))\n",
    "\n",
    "DEFAULT_TARGET_STORIES: Sequence[str] = [\n",
    "    'exorcism',\n",
    "    'food',\n",
    "    'haveyoumethimyet',\n",
    "    'legacy',\n",
    "    'theshower',\n",
    "]\n",
    "\n",
    "candidate_roots = [\n",
    "    data_root / 'derivative' / 'TextGrids',\n",
    "    data_root / 'derivatives' / 'TextGrids',\n",
    "    data_root / 'stimuli',\n",
    "    data_root / 'annotations',\n",
    "]\n",
    "\n",
    "candidate_roots = [path for path in candidate_roots if path.exists()]\n",
    "print(f'Data root: {data_root}')\n",
    "print('TextGrid roots to inspect:')\n",
    "for root in candidate_roots:\n",
    "    print('  -', root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce76d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Controls --------------------------------------------------------------------\n",
    "TARGET_STORIES: Sequence[str] = DEFAULT_TARGET_STORIES  # set to [] to process everything found\n",
    "OUTPUT_ORIGINAL_DIR = project_root / 'misc' / 'textgrids_original'\n",
    "OUTPUT_DIR = project_root / 'misc' / 'textgrids'\n",
    "DRY_RUN = False  # True -> skip writing sanitized files\n",
    "EPSILON = 1e-8  # tolerance for clamping neighbouring interval boundaries\n",
    "\n",
    "print(f'Saving sanitized transcripts under: {OUTPUT_DIR}')\n",
    "print(f'Backing up original transcripts under: {OUTPUT_ORIGINAL_DIR}')\n",
    "if TARGET_STORIES:\n",
    "    print('Restricting to stories:', ', '.join(sorted(TARGET_STORIES)))\n",
    "else:\n",
    "    print('Processing every TextGrid discovered in the search roots.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import shutil\n",
    "\n",
    "if not candidate_roots:\n",
    "    raise FileNotFoundError('No TextGrid directories found. Check data_root or candidate paths.')\n",
    "\n",
    "SOURCE_TEXTGRID_ROOT = candidate_roots[0]\n",
    "print(f'Using source TextGrid directory: {SOURCE_TEXTGRID_ROOT}')\n",
    "\n",
    "OUTPUT_ORIGINAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not any(OUTPUT_ORIGINAL_DIR.iterdir()):\n",
    "    print('Creating pristine backup at textgrids_original ...')\n",
    "    shutil.copytree(SOURCE_TEXTGRID_ROOT, OUTPUT_ORIGINAL_DIR, symlinks=False, dirs_exist_ok=True)\n",
    "else:\n",
    "    print('Backup directory already populated; leaving as-is.')\n",
    "\n",
    "print('Refreshing working copy at textgrids ...')\n",
    "shutil.copytree(SOURCE_TEXTGRID_ROOT, OUTPUT_DIR, symlinks=False, dirs_exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33946ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import re\n",
    "import shlex\n",
    "import tempfile\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "HEADER_LINES = [\n",
    "    'File type = \"ooTextFile\"\\n',\n",
    "    'Object class = \"TextGrid\"\\n',\n",
    "    '\\n',\n",
    "]\n",
    "\n",
    "FLOAT_LINE = re.compile(r'^(?P<prefix>\\s*(?:xmin|xmax)\\s*=\\s*)(?P<value>[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?)?(?P<suffix>\\s*)$')\n",
    "\n",
    "def _format_number(original: str, value: float) -> str:\n",
    "    if any(ch in original for ch in 'eE'):\n",
    "        return f'{value:.12g}'\n",
    "    if '.' in original:\n",
    "        decimals = len(original.split('.')[-1])\n",
    "        return f'{value:.{decimals}f}'\n",
    "    return str(int(round(value)))\n",
    "\n",
    "def _update_float_line(line: str, new_value: float) -> Tuple[str, bool]:\n",
    "    base = line[:-1] if line.endswith('\\n') else line\n",
    "    match = FLOAT_LINE.match(base)\n",
    "    if not match:\n",
    "        return line, False\n",
    "    original_value = match.group('value')\n",
    "    if original_value is None:\n",
    "        return line, False\n",
    "    formatted = _format_number(original_value, new_value)\n",
    "    updated = f\"{match.group('prefix')}{formatted}{match.group('suffix')}\"\n",
    "    if line.endswith('\\n'):\n",
    "        updated += '\\n'\n",
    "    return updated, formatted != original_value\n",
    "\n",
    "def ensure_header(lines: List[str]) -> Tuple[List[str], bool]:\n",
    "    for ln in lines[:5]:\n",
    "        if ln.strip().startswith('File type ='):\n",
    "            return lines, False\n",
    "    return HEADER_LINES + lines, True\n",
    "\n",
    "def fix_overlaps(lines: List[str], tolerance: float = EPSILON) -> Tuple[List[str], int]:\n",
    "    last_xmax_by_tier: Dict[str, float] = {}\n",
    "    current_tier: Optional[str] = None\n",
    "    current_xmin: Optional[float] = None\n",
    "    inside_intervals = False\n",
    "    fixes = 0\n",
    "\n",
    "    for idx, line in enumerate(lines):\n",
    "        stripped = line.strip()\n",
    "        if stripped.startswith('name ='):\n",
    "            parts = stripped.split('\"')\n",
    "            current_tier = parts[1] if len(parts) > 1 else None\n",
    "            if current_tier and current_tier not in last_xmax_by_tier:\n",
    "                last_xmax_by_tier[current_tier] = float('-inf')\n",
    "            current_xmin = None\n",
    "            inside_intervals = False\n",
    "        elif stripped.startswith('intervals ['):\n",
    "            inside_intervals = True\n",
    "            current_xmin = None\n",
    "        elif stripped.startswith('points ['):\n",
    "            inside_intervals = False\n",
    "            current_xmin = None\n",
    "        elif inside_intervals and stripped.startswith('xmin ='):\n",
    "            parts = stripped.split('=')\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            try:\n",
    "                value = float(parts[1].split()[0])\n",
    "            except Exception:\n",
    "                continue\n",
    "            last_xmax = None\n",
    "            if current_tier is not None:\n",
    "                last = last_xmax_by_tier.get(current_tier)\n",
    "                if last is not None and last != float('-inf'):\n",
    "                    last_xmax = last\n",
    "            if last_xmax is not None and value < last_xmax - tolerance:\n",
    "                new_line, changed = _update_float_line(line, last_xmax)\n",
    "                if changed:\n",
    "                    lines[idx] = new_line\n",
    "                    value = last_xmax\n",
    "                    fixes += 1\n",
    "            current_xmin = value\n",
    "        elif inside_intervals and stripped.startswith('xmax ='):\n",
    "            parts = stripped.split('=')\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            try:\n",
    "                value = float(parts[1].split()[0])\n",
    "            except Exception:\n",
    "                continue\n",
    "            if current_xmin is not None and value < current_xmin - tolerance:\n",
    "                new_line, changed = _update_float_line(line, current_xmin)\n",
    "                if changed:\n",
    "                    lines[idx] = new_line\n",
    "                    value = current_xmin\n",
    "                    fixes += 1\n",
    "            if current_tier is not None:\n",
    "                last_xmax_by_tier[current_tier] = value\n",
    "            current_xmin = None\n",
    "        elif stripped.startswith('item'):\n",
    "            inside_intervals = False\n",
    "            current_xmin = None\n",
    "    return lines, fixes\n",
    "\n",
    "def convert_chronological(lines: List[str]) -> Tuple[List[str], bool]:\n",
    "    if not lines or not lines[0].strip().startswith('\"Praat chronological TextGrid text file\"'):\n",
    "        return lines, False\n",
    "    domain_tokens = shlex.split(lines[1])\n",
    "    if len(domain_tokens) < 2:\n",
    "        return lines, False\n",
    "    xmin_token, xmax_token = domain_tokens[0], domain_tokens[1]\n",
    "    tier_line = shlex.split(lines[2])\n",
    "    if not tier_line:\n",
    "        return lines, False\n",
    "    tier_count = int(tier_line[0])\n",
    "    cursor = 3\n",
    "    tier_defs: List[Dict[str, str]] = []\n",
    "    for _ in range(tier_count):\n",
    "        if cursor >= len(lines):\n",
    "            break\n",
    "        tokens = shlex.split(lines[cursor])\n",
    "        cursor += 1\n",
    "        if len(tokens) < 4:\n",
    "            continue\n",
    "        tier_defs.append({\n",
    "            'class': tokens[0],\n",
    "            'name': tokens[1],\n",
    "            'xmin': tokens[2],\n",
    "            'xmax': tokens[3],\n",
    "        })\n",
    "    tier_defs = tier_defs[:tier_count]\n",
    "    intervals_by_tier: List[List[Tuple[str, str, str]]] = [[] for _ in range(len(tier_defs))]\n",
    "    while cursor < len(lines):\n",
    "        row = lines[cursor].strip()\n",
    "        cursor += 1\n",
    "        if not row:\n",
    "            continue\n",
    "        tokens = shlex.split(row)\n",
    "        if len(tokens) < 3:\n",
    "            continue\n",
    "        tier_idx = int(tokens[0])\n",
    "        start_token, end_token = tokens[1], tokens[2]\n",
    "        label = ''\n",
    "        if cursor < len(lines):\n",
    "            label_line = lines[cursor].strip()\n",
    "            cursor += 1\n",
    "            if label_line.startswith('\"') and label_line.endswith('\"'):\n",
    "                label = label_line[1:-1]\n",
    "            else:\n",
    "                label = label_line\n",
    "        if 1 <= tier_idx <= len(intervals_by_tier):\n",
    "            intervals_by_tier[tier_idx - 1].append((start_token, end_token, label))\n",
    "    out: List[str] = []\n",
    "    out.extend(HEADER_LINES)\n",
    "    out.append(f'xmin = {xmin_token} \\n')\n",
    "    out.append(f'xmax = {xmax_token} \\n')\n",
    "    out.append('tiers? <exists> \\n')\n",
    "    out.append(f'size = {len(tier_defs)} \\n')\n",
    "    out.append('item []: \\n')\n",
    "    for idx, tier in enumerate(tier_defs, start=1):\n",
    "        intervals = intervals_by_tier[idx - 1]\n",
    "        out.append(f'    item [{idx}]:\\n')\n",
    "        out.append(f'        class = \"{tier[\"class\"]}\" \\n')\n",
    "        out.append(f'        name = \"{tier[\"name\"]}\" \\n')\n",
    "        out.append(f'        xmin = {tier[\"xmin\"]} \\n')\n",
    "        out.append(f'        xmax = {tier[\"xmax\"]} \\n')\n",
    "        out.append(f'        intervals: size = {len(intervals)} \\n')\n",
    "        for jdx, (start, end, label) in enumerate(intervals, start=1):\n",
    "            safe_label = label.replace('\"', '\"\"')\n",
    "            out.append(f'        intervals [{jdx}]:\\n')\n",
    "            out.append(f'            xmin = {start} \\n')\n",
    "            out.append(f'            xmax = {end} \\n')\n",
    "            out.append(f'            text = \"{safe_label}\" \\n')\n",
    "    return out, True\n",
    "\n",
    "def sanitize_textgrid_text(text: str, tolerance: float = EPSILON) -> Tuple[str, Dict[str, int]]:\n",
    "    lines = text.splitlines(keepends=True)\n",
    "    lines, converted = convert_chronological(lines)\n",
    "    lines, header_added = ensure_header(lines)\n",
    "    lines, overlap_fixes = fix_overlaps(lines, tolerance=tolerance)\n",
    "    content = ''.join(lines)\n",
    "    metrics = {\n",
    "        'converted': int(converted),\n",
    "        'header_added': int(header_added),\n",
    "        'overlap_fixes': overlap_fixes,\n",
    "    }\n",
    "    return content, metrics\n",
    "\n",
    "def validate_textgrid(content: str) -> Tuple[bool, Optional[str]]:\n",
    "    with tempfile.NamedTemporaryFile('w', suffix='.TextGrid', delete=False) as tmp:\n",
    "        tmp.write(content)\n",
    "        tmp_path = Path(tmp.name)\n",
    "    try:\n",
    "        textgrid.TextGrid.fromFile(str(tmp_path))\n",
    "        return True, None\n",
    "    except Exception as exc:\n",
    "        return False, str(exc)\n",
    "    finally:\n",
    "        tmp_path.unlink(missing_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06a1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "targets = {story.lower() for story in TARGET_STORIES} if TARGET_STORIES else None\n",
    "processed: Dict[str, Dict[str, Any]] = {}\n",
    "records: List[Dict[str, Any]] = []\n",
    "\n",
    "source_root = SOURCE_TEXTGRID_ROOT\n",
    "for path in sorted(source_root.glob('*.TextGrid')):\n",
    "    story = path.stem.lower()\n",
    "    if targets and story not in targets:\n",
    "        continue\n",
    "    try:\n",
    "        original_text = path.read_text(encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        original_text = path.read_text(encoding='utf-8', errors='ignore')\n",
    "    sanitized_text, metrics = sanitize_textgrid_text(original_text, tolerance=EPSILON)\n",
    "    parse_ok, parse_error = validate_textgrid(sanitized_text)\n",
    "    dest_path = OUTPUT_DIR / path.name\n",
    "    changed = sanitized_text != original_text\n",
    "    if not DRY_RUN:\n",
    "        dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if dest_path.exists():\n",
    "            try:\n",
    "                dest_path.unlink()\n",
    "            except PermissionError:\n",
    "                dest_path.chmod(0o666)\n",
    "                dest_path.unlink()\n",
    "        dest_path.write_text(sanitized_text)\n",
    "    record = {\n",
    "        'story': path.stem,\n",
    "        'source': str(path),\n",
    "        'destination': str(dest_path),\n",
    "        'converted': bool(metrics['converted']),\n",
    "        'header_added': bool(metrics['header_added']),\n",
    "        'overlap_fixes': metrics['overlap_fixes'],\n",
    "        'changed': changed,\n",
    "        'parse_ok': parse_ok,\n",
    "        'parse_error': parse_error,\n",
    "    }\n",
    "    records.append(record)\n",
    "    processed[story] = record\n",
    "\n",
    "results_df = pd.DataFrame(records).sort_values(['story', 'source']).reset_index(drop=True)\n",
    "print(f'Repaired {len(results_df)} transcript(s).')\n",
    "display(results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Check original TextGrids in-place to reproduce prior failures\n",
    "from src.decoding import load_transcript_words\n",
    "\n",
    "original_failures: List[Tuple[str, str, str]] = []\n",
    "original_successes: List[str] = []\n",
    "subjects_to_test = [f'UTS{idx:02d}' for idx in range(1, 10)]\n",
    "stories_to_test = [story for story in (TARGET_STORIES or processed.keys())]\n",
    "for sub in subjects_to_test:\n",
    "    for story in stories_to_test:\n",
    "        try:\n",
    "            events = load_transcript_words(paths, sub, story)\n",
    "        except Exception as exc:\n",
    "            original_failures.append((sub, story, str(exc)))\n",
    "        else:\n",
    "            original_successes.append(f'{sub}/{story} -> {len(events)} words')\n",
    "\n",
    "print('Original transcript load check (raw data root)')\n",
    "if original_successes:\n",
    "    for line in original_successes:\n",
    "        print('  OK:', line)\n",
    "if original_failures:\n",
    "    print('Failures:')\n",
    "    for sub, story, err in original_failures:\n",
    "        print(f'  {sub}/{story}: {err}')\n",
    "else:\n",
    "    print('No failures encountered using original files.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4566ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Verify a few transcripts round-trip with the sanitized copies in place\n",
    "from src.decoding import load_transcript_words\n",
    "\n",
    "paths_override = dict(paths)\n",
    "paths_override['transcripts'] = str(OUTPUT_DIR)\n",
    "pairs_to_check = [(sub, story) for sub in ['UTS01'] for story in (TARGET_STORIES or processed.keys())]\n",
    "successes: List[str] = []\n",
    "failures: List[Tuple[str, str, str]] = []\n",
    "for sub, story in pairs_to_check:\n",
    "    try:\n",
    "        events = load_transcript_words(paths_override, sub, story)\n",
    "    except Exception as exc:\n",
    "        failures.append((sub, story, str(exc)))\n",
    "    else:\n",
    "        successes.append(f'{sub}/{story} -> {len(events)} words')\n",
    "\n",
    "print(\"Verification using paths['transcripts'] = OUTPUT_DIR\")\n",
    "for line in successes:\n",
    "    print('  OK:', line)\n",
    "if failures:\n",
    "    print('Failures:')\n",
    "    for sub, story, err in failures:\n",
    "        print(f'  {sub}/{story}: {err}')\n",
    "else:\n",
    "    print('All requested transcripts parsed successfully.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}