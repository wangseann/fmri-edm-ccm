{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4615e242",
   "metadata": {},
   "source": [
    "# Day 21 â€“ Batch Category Export with Word2Vec Fallback\n",
    "\n",
    "This notebook extends Day 20 by aligning a Word2Vec fallback embedding space to\n",
    "English1000 so OOV tokens can be mapped automatically. It batches category\n",
    "feature generation for every subject/story pair, saves the outputs, and reports\n",
    "combined token coverage using both vocabularies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d65e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "pd.options.display.max_columns = 60\n",
    "\n",
    "project_root = Path('/flash/PaoU/seann/fmri-edm-ccm')\n",
    "project_root.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(project_root)\n",
    "\n",
    "sys.path.append(str(project_root))\n",
    "sys.path.append('/flash/PaoU/seann/pyEDM/src')\n",
    "sys.path.append('/flash/PaoU/seann/MDE-main/src')\n",
    "\n",
    "try:\n",
    "    from gensim.models import KeyedVectors\n",
    "except Exception:\n",
    "    KeyedVectors = None  # type: ignore\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    def display(obj):  # type: ignore\n",
    "        print(obj)\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt  # noqa: F401  # kept for parity with Day 19 helpers\n",
    "except Exception as exc:\n",
    "    plt = None\n",
    "    warnings.warn(f'Matplotlib unavailable: {exc}')\n",
    "\n",
    "from src.utils import load_yaml\n",
    "from src.decoding import load_transcript_words\n",
    "from src.edm_ccm import English1000Loader\n",
    "\n",
    "EPS = 1e-12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4065e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration -------------------------------------------------------------------\n",
    "cfg = load_yaml('configs/demo.yaml')\n",
    "categories_cfg = cfg.get('categories', {}) or {}\n",
    "cluster_csv_path = categories_cfg.get('cluster_csv_path', '')\n",
    "prototype_weight_power = float(categories_cfg.get('prototype_weight_power', 1.0))\n",
    "seconds_bin_width_default = float(categories_cfg.get('seconds_bin_width', 0.05))\n",
    "temporal_weighting_default = str(categories_cfg.get('temporal_weighting', 'proportional')).lower()\n",
    "\n",
    "paths = cfg.get('paths', {})\n",
    "TR = float(cfg.get('TR', 2.0))\n",
    "features_root = Path(paths.get('features', 'features'))\n",
    "features_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SUBJECT = cfg.get('subject') or 'UTS01'\n",
    "STORY = cfg.get('story') or 'wheretheressmoke'\n",
    "TEMPORAL_WEIGHTING = temporal_weighting_default  # {'proportional', 'none'}\n",
    "SECONDS_BIN_WIDTH = seconds_bin_width_default\n",
    "\n",
    "# canonical smoothing controls tuned for forecasting\n",
    "SMOOTHING_SECONDS = 0.75             # shorter window preserves fast dynamics for forecasting\n",
    "SMOOTHING_METHOD = 'gaussian'       # {'moving_average', 'gaussian'}\n",
    "GAUSSIAN_SIGMA_SECONDS = 0.5 * SMOOTHING_SECONDS  # tie sigma to window length for EDM\n",
    "SMOOTHING_PAD_MODE = 'reflect'      # {'edge', 'reflect'}\n",
    "\n",
    "SAVE_OUTPUTS = True  # toggle off to dry-run the generation loop\n",
    "BATCH_SUBJECTS: Sequence[str] = []  # optionally restrict to these subjects\n",
    "BATCH_STORIES: Sequence[str] = []   # optionally restrict to these stories\n",
    "REPORT_TOKEN_COVERAGE = True\n",
    "\n",
    "fallback_cfg = categories_cfg.get('fallback', {}) or {}\n",
    "FALLBACK_ENABLED = bool(fallback_cfg.get('enabled', True))\n",
    "FALLBACK_MODEL_PATH = fallback_cfg.get('model_path') or paths.get('fallback_embeddings')\n",
    "FALLBACK_BINARY = bool(fallback_cfg.get('binary', True))\n",
    "FALLBACK_TRANSFORM_PATH = fallback_cfg.get('transform_path', 'misc/fallback_to_english1000.npz')\n",
    "FALLBACK_LABEL = str(fallback_cfg.get('label', 'word2vec'))\n",
    "\n",
    "FALLBACK_MODEL = None\n",
    "FALLBACK_ALIGNED = None\n",
    "FALLBACK_INFO: Dict[str, Any] = {}\n",
    "\n",
    "if FALLBACK_ENABLED:\n",
    "    if KeyedVectors is None:\n",
    "        raise ImportError('gensim is required to load word2vec-format embeddings for fallback vocab.')\n",
    "    if not FALLBACK_MODEL_PATH:\n",
    "        raise ValueError('Fallback embeddings enabled but no model_path provided (see categories.fallback.model_path).')\n",
    "    fallback_model_path = Path(FALLBACK_MODEL_PATH)\n",
    "    if not fallback_model_path.exists():\n",
    "        raise FileNotFoundError(f'Fallback embedding model not found at {fallback_model_path}')\n",
    "    transform_path = Path(FALLBACK_TRANSFORM_PATH)\n",
    "    if not transform_path.exists():\n",
    "        raise FileNotFoundError(f'Fallback alignment transform not found at {transform_path}')\n",
    "\n",
    "    print(f'[INFO] Loading fallback embeddings from {fallback_model_path} ...')\n",
    "    FALLBACK_MODEL = KeyedVectors.load_word2vec_format(str(fallback_model_path), binary=FALLBACK_BINARY)\n",
    "    transform_data = np.load(transform_path)\n",
    "    rotation = np.asarray(transform_data['rotation'], dtype=np.float32)\n",
    "    fallback_mean = np.asarray(transform_data['fallback_mean'], dtype=np.float32)\n",
    "    english_mean = np.asarray(transform_data['english_mean'], dtype=np.float32)\n",
    "\n",
    "    class AlignedFallback:\n",
    "        def __init__(self, kv: 'KeyedVectors', rotation: np.ndarray, fallback_mean: np.ndarray, english_mean: np.ndarray):\n",
    "            self._kv = kv\n",
    "            self.rotation = rotation\n",
    "            self.fallback_mean = fallback_mean\n",
    "            self.english_mean = english_mean\n",
    "            self.key_to_index = kv.key_to_index\n",
    "            self.vector_size = rotation.shape[1]\n",
    "\n",
    "        def get_vector(self, key: str) -> np.ndarray:\n",
    "            vec = self._kv.get_vector(key)\n",
    "            return (vec - self.fallback_mean) @ self.rotation + self.english_mean\n",
    "\n",
    "        def __contains__(self, key: str) -> bool:\n",
    "            return key in self._kv.key_to_index\n",
    "\n",
    "        def __getitem__(self, key: str) -> np.ndarray:\n",
    "            return self.get_vector(key)\n",
    "\n",
    "    FALLBACK_ALIGNED = AlignedFallback(FALLBACK_MODEL, rotation, fallback_mean, english_mean)\n",
    "    overlap_tokens = int(transform_data['tokens_used'].shape[0]) if 'tokens_used' in transform_data else None\n",
    "    FALLBACK_INFO = {\n",
    "        'model_path': str(fallback_model_path),\n",
    "        'transform_path': str(transform_path),\n",
    "        'tokens': len(FALLBACK_MODEL),\n",
    "        'dim': FALLBACK_MODEL.vector_size,\n",
    "        'overlap_tokens': overlap_tokens,\n",
    "        'label': FALLBACK_LABEL,\n",
    "    }\n",
    "else:\n",
    "    print('[INFO] Fallback embeddings disabled; English1000 only.')\n",
    "\n",
    "print(f'Subject/story default: {SUBJECT} / {STORY}')\n",
    "print(f'Cluster CSV: {cluster_csv_path or \"<none>\"}')\n",
    "print(f'Temporal weighting: {TEMPORAL_WEIGHTING}')\n",
    "print(f'Seconds bin width: {SECONDS_BIN_WIDTH}')\n",
    "print(f'Smoothing: {SMOOTHING_METHOD} | window={SMOOTHING_SECONDS}s | sigma={GAUSSIAN_SIGMA_SECONDS}')\n",
    "if FALLBACK_ALIGNED is not None:\n",
    "    print(\n",
    "        f\"Fallback model: {FALLBACK_INFO['model_path']} | dim={FALLBACK_INFO['dim']} | tokens={FALLBACK_INFO['tokens']} \"\n",
    "        f\"| overlap={FALLBACK_INFO.get('overlap_tokens', 'n/a')}\"\n",
    "    )\n",
    "    print(f\"Fallback transform: {FALLBACK_INFO['transform_path']}\")\n",
    "else:\n",
    "    print('Fallback model: <disabled>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68098033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def load_story_words(paths: Dict, subject: str, story: str) -> List[Tuple[str, float, float]]:\n",
    "    events = load_transcript_words(paths, subject, story)\n",
    "    if not events:\n",
    "        raise ValueError(f'No transcript events found for {subject} {story}.')\n",
    "    return [(str(word).strip(), float(start), float(end)) for word, start, end in events]\n",
    "\n",
    "\n",
    "def load_clusters_from_csv(csv_path: str) -> Dict[str, Dict[str, List[Tuple[str, float]]]]:\n",
    "    from pathlib import Path\n",
    "    if not csv_path or not Path(csv_path).exists():\n",
    "        raise FileNotFoundError(f'Cluster CSV not found at {csv_path}')\n",
    "    df = pd.read_csv(csv_path)\n",
    "    cols = {c.lower().strip(): c for c in df.columns}\n",
    "    for needed in ('category', 'word'):\n",
    "        assert needed in cols, f\"CSV must contain '{needed}' column.\"\n",
    "    cat_col = cols['category']\n",
    "    word_col = cols['word']\n",
    "    weight_col = cols.get('weight')\n",
    "    if weight_col is None:\n",
    "        df['_weight'] = 1.0\n",
    "        weight_col = '_weight'\n",
    "    df = df[[cat_col, word_col, weight_col]].copy()\n",
    "    df[word_col] = df[word_col].astype(str).str.strip().str.lower()\n",
    "    df[cat_col] = df[cat_col].astype(str).str.strip().str.lower()\n",
    "    df[weight_col] = pd.to_numeric(df[weight_col], errors='coerce').fillna(1.0).clip(lower=0.0)\n",
    "    clusters: Dict[str, Dict[str, List[Tuple[str, float]]]] = {}\n",
    "    for cat, sub in df.groupby(cat_col):\n",
    "        bucket: Dict[str, float] = {}\n",
    "        for w, wt in zip(sub[word_col].tolist(), sub[weight_col].tolist()):\n",
    "            if not w:\n",
    "                continue\n",
    "            bucket[w] = float(wt)\n",
    "        pairs = sorted(bucket.items())\n",
    "        if pairs:\n",
    "            clusters[cat] = {'words': pairs}\n",
    "    if not clusters:\n",
    "        raise ValueError('No clusters parsed from CSV.')\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def build_states_from_csv(\n",
    "    clusters: Dict[str, Dict[str, List[Tuple[str, float]]]],\n",
    "    primary_lookup: Dict[str, np.ndarray],\n",
    "    fallback=None,\n",
    "    weight_power: float = 1.0\n",
    ") -> Tuple[Dict[str, Dict], Dict[str, Dict]]:\n",
    "    category_states: Dict[str, Dict] = {}\n",
    "    category_definitions: Dict[str, Dict] = {}\n",
    "    oov_counts: Dict[str, int] = {}\n",
    "    for cat, spec in clusters.items():\n",
    "        pairs = spec.get('words', [])\n",
    "        vecs: List[np.ndarray] = []\n",
    "        weights: List[float] = []\n",
    "        found_words: List[str] = []\n",
    "        missing_words: List[str] = []\n",
    "        for word, wt in pairs:\n",
    "            vec = lookup_embedding(word, primary_lookup, fallback)\n",
    "            if vec is None:\n",
    "                missing_words.append(word)\n",
    "                continue\n",
    "            vecs.append(vec.astype(float))\n",
    "            weights.append(float(max(0.0, wt)) ** float(weight_power))\n",
    "            found_words.append(word)\n",
    "        if not vecs:\n",
    "            warnings.warn(f\"[{cat}] no usable representative embeddings; prototype will be None.\")\n",
    "            prototype = None\n",
    "            prototype_norm = None\n",
    "        else:\n",
    "            W = np.array(weights, dtype=float)\n",
    "            W = W / (W.sum() + 1e-12)\n",
    "            M = np.stack(vecs, axis=0)\n",
    "            prototype = (W[:, None] * M).sum(axis=0)\n",
    "            prototype_norm = float(np.linalg.norm(prototype))\n",
    "            if prototype_norm < EPS:\n",
    "                prototype = None\n",
    "                prototype_norm = None\n",
    "        rep_lex = {word: float(wt) for word, wt in pairs}\n",
    "        category_states[cat] = {\n",
    "            'name': cat,\n",
    "            'seeds': [],\n",
    "            'found_seeds': found_words,\n",
    "            'missing_seeds': missing_words,\n",
    "            'prototype': prototype,\n",
    "            'prototype_norm': prototype_norm,\n",
    "            'lexicon': rep_lex,\n",
    "            'expanded_count': 0,\n",
    "            'expansion_params': {'enabled': False, 'top_k': 0, 'min_sim': 0.0},\n",
    "        }\n",
    "        category_definitions[cat] = {\n",
    "            'from': 'csv',\n",
    "            'seeds': [],\n",
    "            'found_seeds': found_words,\n",
    "            'missing_seeds': missing_words,\n",
    "            'prototype_dim': int(prototype.shape[0]) if isinstance(prototype, np.ndarray) else 0,\n",
    "            'prototype_norm': prototype_norm,\n",
    "            'representative_words': rep_lex,\n",
    "            'lexicon': rep_lex,\n",
    "            'expanded_neighbors': {},\n",
    "        }\n",
    "        oov_counts[cat] = len(missing_words)\n",
    "    if any(oov_counts.values()):\n",
    "        warnings.warn(f\"OOV representative words: {oov_counts}\")\n",
    "    return category_states, category_definitions\n",
    "\n",
    "\n",
    "def build_tr_edges(word_events: Sequence[Tuple[str, float, float]], tr_s: float) -> np.ndarray:\n",
    "    if not word_events:\n",
    "        return np.arange(0, tr_s, tr_s)\n",
    "    max_end = max(end for _, _, end in word_events)\n",
    "    n_tr = max(1, int(math.ceil(max_end / tr_s)))\n",
    "    edges = np.arange(0.0, (n_tr + 1) * tr_s, tr_s, dtype=float)\n",
    "    if edges[-1] < max_end:\n",
    "        edges = np.append(edges, edges[-1] + tr_s)\n",
    "    if edges[-1] < max_end - 1e-9:\n",
    "        edges = np.append(edges, edges[-1] + tr_s)\n",
    "    return edges\n",
    "\n",
    "\n",
    "def lookup_embedding(\n",
    "    token: str,\n",
    "    primary_lookup: Dict[str, np.ndarray],\n",
    "    fallback=None,\n",
    "    *,\n",
    "    return_source: bool = False,\n",
    ") -> Optional[np.ndarray]:\n",
    "    key = token.lower().strip()\n",
    "    if not key:\n",
    "        return (None, None) if return_source else None\n",
    "    vec = primary_lookup.get(key) if primary_lookup else None\n",
    "    if vec is not None:\n",
    "        vec = np.asarray(vec, dtype=float)\n",
    "        return (vec, 'primary') if return_source else vec\n",
    "    fallback_vec = None\n",
    "    if fallback is not None:\n",
    "        try:\n",
    "            if hasattr(fallback, 'get_vector') and key in fallback:\n",
    "                fallback_vec = np.asarray(fallback.get_vector(key), dtype=float)\n",
    "            elif hasattr(fallback, '__contains__') and key in fallback:\n",
    "                fallback_vec = np.asarray(fallback[key], dtype=float)\n",
    "        except Exception:\n",
    "            fallback_vec = None\n",
    "    if fallback_vec is not None:\n",
    "        return (fallback_vec, 'fallback') if return_source else fallback_vec\n",
    "    return (None, None) if return_source else None\n",
    "\n",
    "\n",
    "def make_category_prototype(seeds: Sequence[str], primary_lookup: Dict[str, np.ndarray], fallback=None, allow_single: bool = False) -> Tuple[Optional[np.ndarray], List[str], List[str]]:\n",
    "    found_vectors = []\n",
    "    found_words = []\n",
    "    missing_words = []\n",
    "    for seed in seeds:\n",
    "        vec = lookup_embedding(seed, primary_lookup, fallback)\n",
    "        if vec is None:\n",
    "            missing_words.append(seed)\n",
    "            continue\n",
    "        found_vectors.append(vec)\n",
    "        found_words.append(seed)\n",
    "    if not found_vectors:\n",
    "        return None, found_words, missing_words\n",
    "    if len(found_vectors) < 2 and not allow_single:\n",
    "        warnings.warn(f'Only {len(found_vectors)} usable seed(s); enable allow_single_seed to accept singleton prototypes.')\n",
    "        if not allow_single:\n",
    "            return None, found_words, missing_words\n",
    "    prototype = np.mean(found_vectors, axis=0)\n",
    "    return prototype, found_words, missing_words\n",
    "\n",
    "\n",
    "def expand_category(prototype: np.ndarray, vocab_embeddings: np.ndarray, vocab_words: Sequence[str], top_k: int, min_sim: float) -> Dict[str, float]:\n",
    "    if prototype is None or vocab_embeddings is None or vocab_words is None:\n",
    "        return {}\n",
    "    proto = np.asarray(prototype, dtype=float)\n",
    "    proto_norm = np.linalg.norm(proto)\n",
    "    if proto_norm == 0:\n",
    "        return {}\n",
    "    proto_unit = proto / proto_norm\n",
    "    vocab_norms = np.linalg.norm(vocab_embeddings, axis=1)\n",
    "    valid_mask = vocab_norms > 0\n",
    "    sims = np.full(vocab_embeddings.shape[0], -1.0, dtype=float)\n",
    "    sims[valid_mask] = (vocab_embeddings[valid_mask] @ proto_unit) / vocab_norms[valid_mask]\n",
    "    top_k_eff = min(top_k, len(sims))\n",
    "    if top_k_eff <= 0:\n",
    "        return {}\n",
    "    candidate_idx = np.argpartition(-sims, top_k_eff - 1)[:top_k_eff]\n",
    "    out = {}\n",
    "    for idx in candidate_idx:\n",
    "        score = float(sims[idx])\n",
    "        if score < min_sim:\n",
    "            continue\n",
    "        out[vocab_words[idx]] = score\n",
    "    return out\n",
    "\n",
    "\n",
    "def tr_token_overlap(token_start: float, token_end: float, tr_start: float, tr_end: float, mode: str = 'proportional') -> float:\n",
    "    token_start = float(token_start)\n",
    "    token_end = float(token_end)\n",
    "    if token_end <= token_start:\n",
    "        token_end = token_start + 1e-3\n",
    "    if mode == 'midpoint':\n",
    "        midpoint = 0.5 * (token_start + token_end)\n",
    "        return 1.0 if tr_start <= midpoint < tr_end else 0.0\n",
    "    overlap = max(0.0, min(token_end, tr_end) - max(token_start, tr_start))\n",
    "    duration = token_end - token_start\n",
    "    if duration <= 0:\n",
    "        return 1.0 if overlap > 0 else 0.0\n",
    "    return max(0.0, min(1.0, overlap / duration))\n",
    "\n",
    "\n",
    "def score_tr(token_payload: Sequence[Dict], method: str, *, lexicon: Optional[Dict[str, float]] = None, prototype: Optional[np.ndarray] = None, prototype_norm: Optional[float] = None) -> float:\n",
    "    if not token_payload:\n",
    "        return float('nan')\n",
    "    method = method.lower()\n",
    "    if method == 'count':\n",
    "        if not lexicon:\n",
    "            return float('nan')\n",
    "        total = 0.0\n",
    "        for item in token_payload:\n",
    "            weight = lexicon.get(item['word'].lower())\n",
    "            if weight is None:\n",
    "                continue\n",
    "            total += weight * item['overlap']\n",
    "        return float(total)\n",
    "    if method == 'similarity':\n",
    "        if prototype is None or prototype_norm is None or prototype_norm < EPS:\n",
    "            return float('nan')\n",
    "        num = 0.0\n",
    "        denom = 0.0\n",
    "        for item in token_payload:\n",
    "            emb = item.get('embedding')\n",
    "            if emb is None:\n",
    "                continue\n",
    "            emb_norm = item.get('embedding_norm')\n",
    "            if emb_norm is None or emb_norm < EPS:\n",
    "                continue\n",
    "            sim = float(np.dot(emb, prototype) / (emb_norm * prototype_norm))\n",
    "            num += sim * item['overlap']\n",
    "            denom += item['overlap']\n",
    "        if denom == 0:\n",
    "            return float('nan')\n",
    "        value = num / denom\n",
    "        return float(np.clip(value, -1.0, 1.0))\n",
    "    raise ValueError(f'Unknown scoring method: {method}')\n",
    "\n",
    "\n",
    "def ensure_serializable(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, (np.floating, np.integer)):\n",
    "        return obj.item()\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: ensure_serializable(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [ensure_serializable(v) for v in obj]\n",
    "    return obj\n",
    "\n",
    "\n",
    "def build_token_buckets(edges: np.ndarray, event_records: Sequence[Dict], mode: str = 'proportional') -> List[List[Dict]]:\n",
    "    if edges.size < 2:\n",
    "        return []\n",
    "    buckets: List[List[Dict]] = [[] for _ in range(len(edges) - 1)]\n",
    "    for rec in event_records:\n",
    "        start = rec['start']\n",
    "        end = rec['end']\n",
    "        if end <= edges[0] or start >= edges[-1]:\n",
    "            continue\n",
    "        start_idx = max(0, int(np.searchsorted(edges, start, side='right')) - 1)\n",
    "        end_idx = max(0, int(np.searchsorted(edges, end, side='left')))\n",
    "        end_idx = min(end_idx, len(buckets) - 1)\n",
    "        for idx in range(start_idx, end_idx + 1):\n",
    "            bucket_start = edges[idx]\n",
    "            bucket_end = edges[idx + 1]\n",
    "            if mode == 'none':\n",
    "                overlap = 1.0 if not (end <= bucket_start or start >= bucket_end) else 0.0\n",
    "            else:\n",
    "                overlap = tr_token_overlap(start, end, bucket_start, bucket_end, 'proportional')\n",
    "            if overlap <= 0:\n",
    "                continue\n",
    "            buckets[idx].append({\n",
    "                'word': rec['word'],\n",
    "                'overlap': overlap,\n",
    "                'embedding': rec['embedding'],\n",
    "                'embedding_norm': rec['embedding_norm'],\n",
    "                'token_start': rec['start'],\n",
    "                'token_end': rec['end'],\n",
    "                'bucket_start': bucket_start,\n",
    "                'bucket_end': bucket_end,\n",
    "            })\n",
    "    return buckets\n",
    "\n",
    "\n",
    "def score_time_series(edges: np.ndarray, buckets: Sequence[Sequence[Dict]], category_states: Dict[str, Dict], category_names: Sequence[str], category_columns: Sequence[str], method: str, index_name: str) -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "    n_bins = len(buckets)\n",
    "    score_matrix = np.full((n_bins, len(category_names)), np.nan, dtype=float)\n",
    "    for col_idx, cat_name in enumerate(category_names):\n",
    "        state = category_states[cat_name]\n",
    "        lexicon = state.get('lexicon')\n",
    "        prototype = state.get('prototype')\n",
    "        prototype_norm = state.get('prototype_norm')\n",
    "        for bin_idx, bucket in enumerate(buckets):\n",
    "            score_matrix[bin_idx, col_idx] = score_tr(bucket, method, lexicon=lexicon, prototype=prototype, prototype_norm=prototype_norm)\n",
    "    data = {\n",
    "        index_name: np.arange(n_bins, dtype=int),\n",
    "        'start_sec': edges[:-1],\n",
    "        'end_sec': edges[1:],\n",
    "    }\n",
    "    for col_idx, col in enumerate(category_columns):\n",
    "        data[col] = score_matrix[:, col_idx]\n",
    "    df = pd.DataFrame(data)\n",
    "    return df, score_matrix\n",
    "\n",
    "\n",
    "def build_smoothing_kernel(seconds_bin_width: float, smoothing_seconds: float, *, method: str = 'moving_average', gaussian_sigma_seconds: Optional[float] = None) -> np.ndarray:\n",
    "    if smoothing_seconds <= 0:\n",
    "        return np.array([1.0], dtype=float)\n",
    "    method = str(method or 'moving_average').lower()\n",
    "    if method == 'moving_average':\n",
    "        window_samples = max(1, int(round(smoothing_seconds / seconds_bin_width)))\n",
    "        if window_samples % 2 == 0:\n",
    "            window_samples += 1\n",
    "        kernel = np.ones(window_samples, dtype=float)\n",
    "    elif method == 'gaussian':\n",
    "        sigma_seconds = float(gaussian_sigma_seconds) if gaussian_sigma_seconds not in (None, '') else max(smoothing_seconds / 2.0, seconds_bin_width)\n",
    "        sigma_samples = max(sigma_seconds / seconds_bin_width, 1e-6)\n",
    "        half_width = max(1, int(round(3.0 * sigma_samples)))\n",
    "        grid = np.arange(-half_width, half_width + 1, dtype=float)\n",
    "        kernel = np.exp(-0.5 * (grid / sigma_samples) ** 2)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown smoothing method: {method}\")\n",
    "    kernel_sum = float(kernel.sum())\n",
    "    if kernel_sum <= 0:\n",
    "        return np.array([1.0], dtype=float)\n",
    "    return kernel / kernel_sum\n",
    "\n",
    "\n",
    "def apply_smoothing_kernel(values: np.ndarray, kernel: np.ndarray, *, pad_mode: str = 'edge', eps: float = 1e-8) -> np.ndarray:\n",
    "    if values.size == 0 or kernel.size <= 1:\n",
    "        return values.copy()\n",
    "    pad_mode = pad_mode if pad_mode in {'edge', 'reflect'} else 'edge'\n",
    "    half = kernel.size // 2\n",
    "    padded = np.pad(values, ((half, half), (0, 0)), mode=pad_mode)\n",
    "    mask = np.isfinite(padded).astype(float)\n",
    "    filled = np.where(mask, padded, 0.0)\n",
    "    smoothed = np.empty((values.shape[0], values.shape[1]), dtype=float)\n",
    "    for col in range(values.shape[1]):\n",
    "        numerator = np.convolve(filled[:, col], kernel, mode='valid')\n",
    "        denominator = np.convolve(mask[:, col], kernel, mode='valid')\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            smoothed_col = numerator / np.maximum(denominator, eps)\n",
    "        smoothed_col[denominator < eps] = np.nan\n",
    "        smoothed[:, col] = smoothed_col\n",
    "    return smoothed\n",
    "\n",
    "\n",
    "def aggregate_seconds_to_edges(canonical_edges: np.ndarray, canonical_values: np.ndarray, target_edges: np.ndarray) -> np.ndarray:\n",
    "    if canonical_values.size == 0:\n",
    "        return np.empty((len(target_edges) - 1, 0), dtype=float)\n",
    "    midpoints = 0.5 * (canonical_edges[:-1] + canonical_edges[1:])\n",
    "    bin_ids = np.digitize(midpoints, target_edges) - 1\n",
    "    if bin_ids.size:\n",
    "        bin_ids = np.clip(bin_ids, 0, len(target_edges) - 2)\n",
    "    out = np.full((len(target_edges) - 1, canonical_values.shape[1]), np.nan, dtype=float)\n",
    "    for idx in range(out.shape[0]):\n",
    "        mask = bin_ids == idx\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        values = canonical_values[mask]\n",
    "        if values.ndim == 1:\n",
    "            values = values[:, None]\n",
    "        finite_any = np.isfinite(values).any(axis=0)\n",
    "        if not finite_any.any():\n",
    "            continue\n",
    "        col_means = np.full(values.shape[1], np.nan, dtype=float)\n",
    "        col_means[finite_any] = np.nanmean(values[:, finite_any], axis=0)\n",
    "        out[idx] = col_means\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b94a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_category_time_series(\n",
    "    subject: str,\n",
    "    story: str,\n",
    "    *,\n",
    "    cfg_base: Dict[str, Any],\n",
    "    categories_cfg_base: Dict[str, Any],\n",
    "    cluster_csv_path: str,\n",
    "    temporal_weighting: str,\n",
    "    prototype_weight_power: float,\n",
    "    smoothing_seconds: float,\n",
    "    smoothing_method: str,\n",
    "    gaussian_sigma_seconds: Optional[float],\n",
    "    smoothing_pad: str,\n",
    "    seconds_bin_width: float,\n",
    "    fallback_model: Optional[Any] = None,\n",
    "    record_coverage: bool = True,\n",
    "    save_outputs: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    if not subject or not story:\n",
    "        raise ValueError('Subject and story must be provided.')\n",
    "    print(f\"=== Day21 category build for {subject} / {story} ===\")\n",
    "\n",
    "    categories_cfg = json.loads(json.dumps(categories_cfg_base or {}))\n",
    "    categories_cfg['seconds_bin_width'] = float(seconds_bin_width)\n",
    "    category_sets = categories_cfg.get('sets', {})\n",
    "    available_sets = sorted(category_sets.keys())\n",
    "    category_set_name = categories_cfg.get('category_set') or (available_sets[0] if available_sets else None)\n",
    "    if cluster_csv_path:\n",
    "        if not category_set_name:\n",
    "            category_set_name = 'csv_clusters'\n",
    "        categories_cfg['category_set'] = category_set_name\n",
    "        categories_cfg['category_score_method'] = 'similarity'\n",
    "        categories_cfg['allow_single_seed'] = True\n",
    "        categories_cfg['expansion'] = {'enabled': False}\n",
    "    category_score_method = str(categories_cfg.get('category_score_method', 'similarity')).lower()\n",
    "    overlap_mode = str(categories_cfg.get('overlap_weighting', 'proportional')).lower()\n",
    "    expansion_cfg = categories_cfg.get('expansion', {})\n",
    "    allow_single = bool(categories_cfg.get('allow_single_seed', False))\n",
    "    exp_enabled = bool(expansion_cfg.get('enabled', True))\n",
    "    exp_top_k = int(expansion_cfg.get('top_k', 2000)) if exp_enabled else 0\n",
    "    exp_min_sim = float(expansion_cfg.get('min_sim', 0.35)) if exp_enabled else 0.0\n",
    "\n",
    "    selected_set_spec = category_sets.get(category_set_name, {}) if category_sets else {}\n",
    "\n",
    "    output_root = features_root / 'subjects' / subject / story\n",
    "    canonical_root = features_root / 'stories' / story\n",
    "    if save_outputs:\n",
    "        output_root.mkdir(parents=True, exist_ok=True)\n",
    "        canonical_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    story_events = load_story_words(paths, subject, story)\n",
    "    print(f'Loaded {len(story_events)} transcript events.')\n",
    "    tr_edges = build_tr_edges(story_events, TR)\n",
    "    n_tr = len(tr_edges) - 1\n",
    "    print(f'TR edges: {len(tr_edges)} (n_tr={n_tr}) spanning {tr_edges[-1]:.2f} seconds.')\n",
    "\n",
    "    embedding_source = str(categories_cfg.get('embedding_source', 'english1000')).lower()\n",
    "    english_loader = None\n",
    "    english_lookup: Dict[str, np.ndarray] = {}\n",
    "    english_vocab: List[str] = []\n",
    "    english_matrix = None\n",
    "    if embedding_source in {'english1000', 'both'}:\n",
    "        english1000_path = Path(paths.get('data_root', '')) / 'derivative' / 'english1000sm.hf5'\n",
    "        if english1000_path.exists():\n",
    "            english_loader = English1000Loader(english1000_path)\n",
    "            english_lookup = english_loader.lookup\n",
    "            english_vocab = english_loader.vocab\n",
    "            english_matrix = english_loader.embeddings\n",
    "            print(f'Loaded English1000 embeddings from {english1000_path} (vocab={len(english_vocab)}).')\n",
    "        else:\n",
    "            raise FileNotFoundError(f'English1000 embeddings not found at {english1000_path}')\n",
    "    else:\n",
    "        print('English1000 disabled by configuration.')\n",
    "\n",
    "    word2vec_model = fallback_model\n",
    "    if word2vec_model is None and embedding_source in {'word2vec', 'both'}:\n",
    "        w2v_path = categories_cfg.get('word2vec_path')\n",
    "        if w2v_path:\n",
    "            w2v_path = Path(w2v_path)\n",
    "            if w2v_path.exists():\n",
    "                try:\n",
    "                    if KeyedVectors is None:\n",
    "                        raise ImportError('gensim is required for word2vec fallback loading.')\n",
    "                    binary = w2v_path.suffix.lower() in {'.bin', '.gz'}\n",
    "                    word2vec_model = KeyedVectors.load_word2vec_format(w2v_path, binary=binary)\n",
    "                    print(f'Loaded Word2Vec fallback from {w2v_path}.')\n",
    "                except Exception as exc:\n",
    "                    warnings.warn(f'Failed to load Word2Vec fallback: {exc}')\n",
    "            else:\n",
    "                warnings.warn(f'Word2Vec path does not exist: {w2v_path}')\n",
    "        else:\n",
    "            warnings.warn('Word2Vec fallback requested but no path provided.')\n",
    "    elif word2vec_model is not None:\n",
    "        print('Using pre-loaded fallback embedding model (aligned).')\n",
    "    else:\n",
    "        print('Word2Vec fallback disabled.')\n",
    "\n",
    "    if cluster_csv_path:\n",
    "        csv_clusters = load_clusters_from_csv(cluster_csv_path)\n",
    "        category_states, category_definitions = build_states_from_csv(\n",
    "            csv_clusters,\n",
    "            english_lookup,\n",
    "            word2vec_model,\n",
    "            weight_power=prototype_weight_power,\n",
    "        )\n",
    "        category_names = sorted(category_states.keys())\n",
    "        category_columns = [f'cat_{name}' for name in category_names]\n",
    "        print(f\"Loaded {len(category_names)} CSV-driven categories from {cluster_csv_path}: {category_names}\")\n",
    "        zero_norm = [k for k, v in category_states.items() if v.get('prototype') is not None and (v.get('prototype_norm') or 0.0) < EPS]\n",
    "        if zero_norm:\n",
    "            warnings.warn(f\"Zero-norm prototypes (check OOV/weights): {zero_norm}\")\n",
    "    else:\n",
    "        category_states = {}\n",
    "        category_definitions = {}\n",
    "        seed_oov_counter = Counter()\n",
    "        for cat_name, cat_spec in selected_set_spec.items():\n",
    "            seeds = cat_spec.get('seeds', [])\n",
    "            explicit_words = cat_spec.get('words', [])\n",
    "            prototype = None\n",
    "            found_seeds: List[str] = []\n",
    "            missing_seeds: List[str] = []\n",
    "            if seeds:\n",
    "                prototype, found_seeds, missing_seeds = make_category_prototype(seeds, english_lookup, word2vec_model, allow_single)\n",
    "                seed_oov_counter[cat_name] = len(missing_seeds)\n",
    "                if prototype is None and category_score_method == 'similarity':\n",
    "                    warnings.warn(f\"Category '{cat_name}' has no usable prototype; TR scores will be NaN.\")\n",
    "            elif category_score_method == 'similarity':\n",
    "                warnings.warn(f'Category {cat_name} has no seeds; similarity method will yield NaNs.')\n",
    "            lexicon = {word.lower(): 1.0 for word in explicit_words}\n",
    "            for seed in found_seeds:\n",
    "                lexicon.setdefault(seed.lower(), 1.0)\n",
    "            prototype_norm = None\n",
    "            expanded_words = {}\n",
    "            if prototype is not None:\n",
    "                prototype_norm = float(np.linalg.norm(prototype))\n",
    "                if exp_enabled and english_matrix is not None:\n",
    "                    expanded_words = expand_category(prototype, english_matrix, english_vocab, exp_top_k, exp_min_sim)\n",
    "                    for word, weight in expanded_words.items():\n",
    "                        lexicon.setdefault(word.lower(), float(weight))\n",
    "            if not lexicon and category_score_method == 'count':\n",
    "                warnings.warn(f'Category {cat_name} lexicon is empty; counts will be NaN.')\n",
    "            category_states[cat_name] = {\n",
    "                'name': cat_name,\n",
    "                'seeds': seeds,\n",
    "                'found_seeds': found_seeds,\n",
    "                'missing_seeds': missing_seeds,\n",
    "                'prototype': prototype,\n",
    "                'prototype_norm': prototype_norm,\n",
    "                'lexicon': lexicon,\n",
    "                'expanded_count': len(expanded_words),\n",
    "                'expansion_params': {\n",
    "                    'enabled': exp_enabled,\n",
    "                    'top_k': exp_top_k,\n",
    "                    'min_sim': exp_min_sim,\n",
    "                },\n",
    "            }\n",
    "            category_definitions[cat_name] = {\n",
    "                'seeds': seeds,\n",
    "                'found_seeds': found_seeds,\n",
    "                'missing_seeds': missing_seeds,\n",
    "                'prototype_dim': int(prototype.shape[0]) if isinstance(prototype, np.ndarray) else 0,\n",
    "                'prototype_norm': prototype_norm,\n",
    "                'expanded_neighbors': ensure_serializable(expanded_words),\n",
    "                'lexicon': {word: float(weight) for word, weight in sorted(category_states[cat_name]['lexicon'].items())},\n",
    "            }\n",
    "        print('Category seeds missing counts:', dict(seed_oov_counter))\n",
    "        category_names = sorted(category_states.keys())\n",
    "        category_columns = [f'cat_{name}' for name in category_names]\n",
    "        print(f'Prepared {len(category_names)} categories: {category_names}')\n",
    "\n",
    "    tw_mode = str(temporal_weighting or 'proportional').lower()\n",
    "    if tw_mode not in {'proportional', 'none', 'midpoint'}:\n",
    "        raise ValueError(f'Unsupported temporal weighting: {tw_mode}')\n",
    "\n",
    "    seconds_bin_width = float(seconds_bin_width)\n",
    "    if seconds_bin_width <= 0:\n",
    "        raise ValueError('seconds_bin_width must be positive.')\n",
    "    smoothing_method = str(smoothing_method or 'moving_average').lower()\n",
    "    gaussian_sigma_seconds = gaussian_sigma_seconds if gaussian_sigma_seconds not in (None, '') else None\n",
    "    smoothing_pad = str(smoothing_pad or 'edge').lower()\n",
    "    if smoothing_pad not in {'edge', 'reflect'}:\n",
    "        smoothing_pad = 'edge'\n",
    "\n",
    "    embedding_cache: Dict[str, Tuple[Optional[np.ndarray], Optional[str]]] = {}\n",
    "    event_records: List[Dict] = []\n",
    "    tokens_with_embeddings = 0\n",
    "    tokens_primary_hits = 0\n",
    "    tokens_fallback_hits = 0\n",
    "    unique_all: set[str] = set()\n",
    "    unique_primary: set[str] = set()\n",
    "    unique_fallback: set[str] = set()\n",
    "    for word, onset, offset in story_events:\n",
    "        token = word.strip()\n",
    "        if not token:\n",
    "            continue\n",
    "        key = token.lower()\n",
    "        unique_all.add(key)\n",
    "        if key not in embedding_cache:\n",
    "            emb, source = lookup_embedding(token, english_lookup, word2vec_model, return_source=True)\n",
    "            embedding_cache[key] = (emb, source)\n",
    "        else:\n",
    "            emb, source = embedding_cache[key]\n",
    "        emb_norm = float(np.linalg.norm(emb)) if emb is not None else None\n",
    "        if emb is not None:\n",
    "            tokens_with_embeddings += 1\n",
    "            if source == 'primary':\n",
    "                tokens_primary_hits += 1\n",
    "                unique_primary.add(key)\n",
    "            elif source == 'fallback':\n",
    "                tokens_fallback_hits += 1\n",
    "                unique_fallback.add(key)\n",
    "        event_records.append({\n",
    "            'word': token,\n",
    "            'start': float(onset),\n",
    "            'end': float(offset),\n",
    "            'embedding': emb,\n",
    "            'embedding_norm': emb_norm,\n",
    "        })\n",
    "\n",
    "    total_tokens = len(event_records)\n",
    "    combined_hits = tokens_primary_hits + tokens_fallback_hits\n",
    "    primary_pct = (100.0 * tokens_primary_hits / total_tokens) if total_tokens else 0.0\n",
    "    fallback_pct = (100.0 * tokens_fallback_hits / total_tokens) if total_tokens else 0.0\n",
    "    combined_pct = (100.0 * combined_hits / total_tokens) if total_tokens else 0.0\n",
    "    unique_total = len(unique_all)\n",
    "    unique_combined = len(unique_primary | unique_fallback)\n",
    "    tokens_oov = total_tokens - combined_hits\n",
    "    print(f'Tokens with embeddings: {combined_hits}/{total_tokens} (OOV rate={tokens_oov / max(total_tokens, 1):.2%}).')\n",
    "    if record_coverage:\n",
    "        print(\n",
    "            f'Token coverage (primary={primary_pct:.2f}%, fallback={fallback_pct:.2f}%, combined={combined_pct:.2f}%)'\n",
    "        )\n",
    "\n",
    "    coverage = None\n",
    "    if record_coverage:\n",
    "        coverage = {\n",
    "            'tokens_total': total_tokens,\n",
    "            'tokens_primary': tokens_primary_hits,\n",
    "            'tokens_fallback': tokens_fallback_hits,\n",
    "            'tokens_combined': combined_hits,\n",
    "            'tokens_oov': tokens_oov,\n",
    "            'pct_tokens_primary': primary_pct,\n",
    "            'pct_tokens_fallback': fallback_pct,\n",
    "            'pct_tokens_combined': combined_pct,\n",
    "            'unique_total': unique_total,\n",
    "            'unique_primary': len(unique_primary),\n",
    "            'unique_fallback': len(unique_fallback),\n",
    "            'unique_combined': unique_combined,\n",
    "            'pct_unique_primary': (100.0 * len(unique_primary) / unique_total) if unique_total else 0.0,\n",
    "            'pct_unique_fallback': (100.0 * len(unique_fallback) / unique_total) if unique_total else 0.0,\n",
    "            'pct_unique_combined': (100.0 * unique_combined / unique_total) if unique_total else 0.0,\n",
    "        }\n",
    "    if not event_records:\n",
    "        raise ValueError('No token events available for category featurization.')\n",
    "\n",
    "    max_end_time = max(rec['end'] for rec in event_records)\n",
    "    canonical_edges = np.arange(0.0, max_end_time + seconds_bin_width, seconds_bin_width, dtype=float)\n",
    "    if canonical_edges[-1] < max_end_time:\n",
    "        canonical_edges = np.append(canonical_edges, canonical_edges[-1] + seconds_bin_width)\n",
    "    if canonical_edges[-1] < max_end_time - 1e-9:\n",
    "        canonical_edges = np.append(canonical_edges, canonical_edges[-1] + seconds_bin_width)\n",
    "    assert np.all(np.diff(canonical_edges) > 0), 'Non-monotone canonical edges.'\n",
    "\n",
    "    canonical_buckets = build_token_buckets(canonical_edges, event_records, tw_mode)\n",
    "    empty_canonical = sum(1 for bucket in canonical_buckets if not bucket)\n",
    "    print(f'Canonical bins without tokens: {empty_canonical}/{len(canonical_buckets)}')\n",
    "\n",
    "    canonical_df_raw, canonical_matrix = score_time_series(\n",
    "        canonical_edges,\n",
    "        canonical_buckets,\n",
    "        category_states,\n",
    "        category_names,\n",
    "        category_columns,\n",
    "        category_score_method,\n",
    "        index_name='bin_index',\n",
    "    )\n",
    "    canonical_values_raw = canonical_matrix.copy()\n",
    "    smoothing_kernel = build_smoothing_kernel(\n",
    "        seconds_bin_width,\n",
    "        smoothing_seconds,\n",
    "        method=smoothing_method,\n",
    "        gaussian_sigma_seconds=gaussian_sigma_seconds,\n",
    "    )\n",
    "    smoothing_applied = smoothing_kernel.size > 1\n",
    "    if canonical_values_raw.size and smoothing_applied:\n",
    "        canonical_values_smoothed = apply_smoothing_kernel(canonical_values_raw, smoothing_kernel, pad_mode=smoothing_pad)\n",
    "    else:\n",
    "        canonical_values_smoothed = canonical_values_raw.copy()\n",
    "\n",
    "    canonical_df_smoothed = canonical_df_raw.copy()\n",
    "    if category_columns:\n",
    "        canonical_df_smoothed.loc[:, category_columns] = canonical_values_smoothed\n",
    "    canonical_df_selected = canonical_df_smoothed if smoothing_applied else canonical_df_raw\n",
    "\n",
    "    if save_outputs:\n",
    "        canonical_root.mkdir(parents=True, exist_ok=True)\n",
    "        canonical_csv_path = canonical_root / 'category_timeseries_seconds.csv'\n",
    "        canonical_df_selected.to_csv(canonical_csv_path, index=False)\n",
    "        if smoothing_applied:\n",
    "            canonical_df_raw.to_csv(canonical_root / 'category_timeseries_seconds_raw.csv', index=False)\n",
    "        canonical_definition_path = canonical_root / 'category_definition.json'\n",
    "        with canonical_definition_path.open('w') as fh:\n",
    "            json.dump(ensure_serializable(category_definitions), fh, indent=2)\n",
    "        print(f'Saved canonical story series to {canonical_csv_path}')\n",
    "\n",
    "    tr_buckets = build_token_buckets(tr_edges, event_records, tw_mode)\n",
    "    empty_tr = sum(1 for bucket in tr_buckets if not bucket)\n",
    "    print(f'TRs without tokens: {empty_tr}/{len(tr_buckets)}')\n",
    "\n",
    "    if category_columns:\n",
    "        tr_values_raw = aggregate_seconds_to_edges(canonical_edges, canonical_values_raw, tr_edges)\n",
    "        tr_values_smoothed = aggregate_seconds_to_edges(canonical_edges, canonical_values_smoothed, tr_edges)\n",
    "    else:\n",
    "        tr_values_raw = np.empty((len(tr_edges) - 1, 0), dtype=float)\n",
    "        tr_values_smoothed = tr_values_raw\n",
    "\n",
    "    base_index = np.arange(len(tr_edges) - 1, dtype=int)\n",
    "    base_df = pd.DataFrame({'tr_index': base_index, 'start_sec': tr_edges[:-1], 'end_sec': tr_edges[1:]})\n",
    "    category_df_raw = base_df.copy()\n",
    "    category_df_smoothed = base_df.copy()\n",
    "    if category_columns:\n",
    "        category_df_raw.loc[:, category_columns] = tr_values_raw\n",
    "        category_df_smoothed.loc[:, category_columns] = tr_values_smoothed\n",
    "    category_df = category_df_smoothed if smoothing_applied else category_df_raw\n",
    "    print(category_df.head())\n",
    "\n",
    "    if category_score_method == 'similarity' and category_columns:\n",
    "        finite_vals = category_df[category_columns].to_numpy(dtype=float)\n",
    "        finite_vals = finite_vals[np.isfinite(finite_vals)]\n",
    "        if finite_vals.size:\n",
    "            assert np.nanmin(finite_vals) >= -1.0001 and np.nanmax(finite_vals) <= 1.0001, 'Similarity scores out of bounds.'\n",
    "    else:\n",
    "        if category_columns:\n",
    "            assert (category_df[category_columns].fillna(0.0) >= -1e-9).all().all(), 'Count scores must be non-negative.'\n",
    "\n",
    "    if save_outputs:\n",
    "        output_root.mkdir(parents=True, exist_ok=True)\n",
    "        category_csv_path = output_root / 'category_timeseries.csv'\n",
    "        category_df.to_csv(category_csv_path, index=False)\n",
    "        if smoothing_applied:\n",
    "            category_df_raw.to_csv(output_root / 'category_timeseries_raw.csv', index=False)\n",
    "        definition_path = output_root / 'category_definition.json'\n",
    "        with definition_path.open('w') as fh:\n",
    "            json.dump(ensure_serializable(category_definitions), fh, indent=2)\n",
    "        print(f'Saved category time series to {category_csv_path}')\n",
    "\n",
    "    trimmed_path = Path(paths.get('figs', 'figs')) / subject / story / 'day16_decoding' / 'semantic_pcs_trimmed.csv'\n",
    "    max_lag_primary = 0\n",
    "    trimmed_df = None\n",
    "    if trimmed_path.exists():\n",
    "        day16_trim = pd.read_csv(trimmed_path)\n",
    "        expected_len = len(day16_trim)\n",
    "        if len(day16_trim) > len(category_df):\n",
    "            raise ValueError('Day16 trimmed series longer than category series; regenerate Day16 or rerun Day17.')\n",
    "        max_lag_primary = max(0, len(category_df) - expected_len)\n",
    "        trimmed_df = category_df.iloc[max_lag_primary:].reset_index(drop=True)\n",
    "        if save_outputs:\n",
    "            trimmed_out = trimmed_df.copy()\n",
    "            trimmed_out.insert(0, 'trim_index', np.arange(len(trimmed_out), dtype=int))\n",
    "            trimmed_out.drop(columns=['tr_index'], inplace=True, errors='ignore')\n",
    "            trimmed_out.to_csv(output_root / 'category_timeseries_trimmed.csv', index=False)\n",
    "            print(f'Saved trimmed category series to {output_root / \"category_timeseries_trimmed.csv\"}')\n",
    "    else:\n",
    "        warnings.warn('Day16 trimmed PCs not found; skipping auto-alignment.')\n",
    "\n",
    "    smoothing_meta = {\n",
    "        'applied': bool(smoothing_applied),\n",
    "        'seconds': smoothing_seconds,\n",
    "        'method': smoothing_method,\n",
    "        'gaussian_sigma_seconds': float(gaussian_sigma_seconds) if gaussian_sigma_seconds is not None else None,\n",
    "        'kernel_size': int(smoothing_kernel.size),\n",
    "        'pad_mode': smoothing_pad,\n",
    "        'bin_width_seconds': seconds_bin_width,\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'subject': subject,\n",
    "        'story': story,\n",
    "        'temporal_weighting': tw_mode,\n",
    "        'category_columns': category_columns,\n",
    "        'category_states': category_states,\n",
    "        'category_definitions': category_definitions,\n",
    "        'category_score_method': category_score_method,\n",
    "        'event_records': event_records,\n",
    "        'canonical_buckets': canonical_buckets,\n",
    "        'tr_buckets': tr_buckets,\n",
    "        'canonical_df_raw': canonical_df_raw,\n",
    "        'canonical_df_smoothed': canonical_df_smoothed,\n",
    "        'canonical_df_selected': canonical_df_selected,\n",
    "        'category_df_raw': category_df_raw,\n",
    "        'category_df_smoothed': category_df_smoothed,\n",
    "        'category_df_selected': category_df,\n",
    "        'canonical_edges': canonical_edges,\n",
    "        'tr_edges': tr_edges,\n",
    "        'smoothing': smoothing_meta,\n",
    "        'output_root': output_root,\n",
    "        'canonical_root': canonical_root,\n",
    "        'trimmed_df': trimmed_df,\n",
    "        'coverage': coverage,\n",
    "        'max_lag_primary': max_lag_primary,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e94fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Subject/story discovery ---------------------------------------------------------\n",
    "\n",
    "def discover_subject_story_pairs(\n",
    "    paths: Dict[str, Any],\n",
    "    *,\n",
    "    default_subject: str,\n",
    "    default_story: str,\n",
    "    subject_overrides: Sequence[str] | None = None,\n",
    "    story_overrides: Sequence[str] | None = None,\n",
    ") -> List[Tuple[str, str]]:\n",
    "    combos = set()\n",
    "    failures: List[Dict[str, str]] = []\n",
    "\n",
    "    def _canon(name: str) -> str:\n",
    "        return ''.join(ch for ch in name.lower() if ch.isalnum())\n",
    "\n",
    "    transcript_variants: set[str] = set()\n",
    "\n",
    "    candidate_transcript_roots: List[Path] = []\n",
    "    transcripts_root_cfg = paths.get('transcripts')\n",
    "    if transcripts_root_cfg:\n",
    "        candidate_transcript_roots.append(Path(transcripts_root_cfg))\n",
    "    data_root_cfg = paths.get('data_root')\n",
    "    if data_root_cfg:\n",
    "        dr_cfg = Path(data_root_cfg)\n",
    "        candidate_transcript_roots.append(dr_cfg / 'derivative' / 'TextGrids')\n",
    "        candidate_transcript_roots.append(dr_cfg / 'derivatives' / 'TextGrids')\n",
    "\n",
    "    for root in candidate_transcript_roots:\n",
    "        if not root.exists():\n",
    "            continue\n",
    "        for path in root.glob('**/*'):\n",
    "            if path.is_file():\n",
    "                transcript_variants.add(_canon(path.stem))\n",
    "\n",
    "    def _norm(seq: Optional[Sequence[str]]) -> List[str]:\n",
    "        return [str(item).strip() for item in (seq or []) if str(item).strip()]\n",
    "\n",
    "    subject_overrides = _norm(subject_overrides)\n",
    "    story_overrides = _norm(story_overrides)\n",
    "\n",
    "    cache_root = Path(paths.get('cache', 'data_cache'))\n",
    "    if cache_root.exists():\n",
    "        for subj_dir in sorted(p for p in cache_root.iterdir() if p.is_dir()):\n",
    "            for story_dir in sorted(p for p in subj_dir.iterdir() if p.is_dir()):\n",
    "                combos.add((subj_dir.name, story_dir.name))\n",
    "\n",
    "    transcripts_root = paths.get('transcripts')\n",
    "    if transcripts_root:\n",
    "        tr_root = Path(transcripts_root)\n",
    "        if tr_root.exists():\n",
    "            for subj_dir in sorted(p for p in tr_root.iterdir() if p.is_dir()):\n",
    "                story_dirs = [d for d in subj_dir.iterdir() if d.is_dir()]\n",
    "                if story_dirs:\n",
    "                    for story_dir in story_dirs:\n",
    "                        combos.add((subj_dir.name, story_dir.name))\n",
    "                        transcript_variants.add(_canon(story_dir.name))\n",
    "                else:\n",
    "                    for file in subj_dir.glob('*.*'):\n",
    "                        if file.is_file():\n",
    "                            combos.add((subj_dir.name, file.stem))\n",
    "                            transcript_variants.add(_canon(file.stem))\n",
    "            for file in tr_root.glob('*.*'):\n",
    "                if file.is_file():\n",
    "                    transcript_variants.add(_canon(file.stem))\n",
    "                if file.is_file() and '_' in file.stem:\n",
    "                    sub_name, story_name = file.stem.split('_', 1)\n",
    "                    combos.add((sub_name, story_name))\n",
    "                    transcript_variants.add(_canon(story_name))\n",
    "\n",
    "    data_root = paths.get('data_root')\n",
    "    if data_root:\n",
    "        dr_root = Path(data_root)\n",
    "        if dr_root.exists():\n",
    "            for subj_dir in sorted(dr_root.glob('sub-*')):\n",
    "                subject_name = subj_dir.name.replace('sub-', '', 1)\n",
    "                for func_dir in sorted(subj_dir.glob('ses-*/func')):\n",
    "                    for bold_file in func_dir.glob('*task-*_bold.nii.gz'):\n",
    "                        task_part = bold_file.name.split('task-')[-1]\n",
    "                        task_part = task_part.split('_', 1)[0]\n",
    "                        if not task_part:\n",
    "                            continue\n",
    "                        canon = _canon(task_part)\n",
    "                        if transcript_variants and canon not in transcript_variants:\n",
    "                            continue\n",
    "                        combos.add((subject_name, task_part))\n",
    "\n",
    "    if subject_overrides and story_overrides:\n",
    "        combos.update((sub, story) for sub in subject_overrides for story in story_overrides)\n",
    "    elif subject_overrides:\n",
    "        fallback_stories = {story for _, story in combos} or {default_story}\n",
    "        combos.update((sub, story) for sub in subject_overrides for story in fallback_stories)\n",
    "    elif story_overrides:\n",
    "        fallback_subjects = {sub for sub, _ in combos} or {default_subject}\n",
    "        combos.update((sub, story) for sub in fallback_subjects for story in story_overrides)\n",
    "\n",
    "    combos.add((default_subject, default_story))\n",
    "\n",
    "    valid_pairs: List[Tuple[str, str]] = []\n",
    "    for subject, story in sorted(combos):\n",
    "        try:\n",
    "            load_story_words(paths, subject, story)\n",
    "        except FileNotFoundError:\n",
    "            warnings.warn(f'Skipping {subject} / {story}: transcript not found.')\n",
    "            failures.append({'subject': subject, 'story': story, 'error': 'transcript not found'})\n",
    "            continue\n",
    "        except Exception as exc:\n",
    "            warnings.warn(f'Skipping {subject} / {story}: failed to load transcript ({exc}).')\n",
    "            failures.append({'subject': subject, 'story': story, 'error': str(exc)})\n",
    "            continue\n",
    "        valid_pairs.append((subject, story))\n",
    "\n",
    "    if not valid_pairs:\n",
    "        raise RuntimeError('No valid subject/story pairs discovered; check transcript paths and overrides.')\n",
    "    return valid_pairs, failures\n",
    "\n",
    "\n",
    "subject_story_pairs, transcript_failures = discover_subject_story_pairs(\n",
    "    paths,\n",
    "    default_subject=SUBJECT,\n",
    "    default_story=STORY,\n",
    "    subject_overrides=BATCH_SUBJECTS,\n",
    "    story_overrides=BATCH_STORIES,\n",
    ")\n",
    "print(f'Discovered {len(subject_story_pairs)} subject/story pairs:')\n",
    "for sub, story in subject_story_pairs:\n",
    "    print(f'  - {sub} / {story}')\n",
    "if transcript_failures:\n",
    "    print(f\"{len(transcript_failures)} transcript(s) could not be parsed; see `transcript_failures` for details.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e2b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Transcript diagnostics ---------------------------------------------------\n",
    "if transcript_failures:\n",
    "    issues_df = pd.DataFrame(transcript_failures).sort_values(['subject', 'story']).reset_index(drop=True)\n",
    "    display(issues_df)\n",
    "else:\n",
    "    print('No transcript parsing issues detected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6fb680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: peek at the first few lines of problematic transcript files\n",
    "from src.decoding.text_align import _candidate_paths\n",
    "\n",
    "def preview_transcript_file(subject: str, story: str, *, max_lines: int = 8) -> None:\n",
    "    \"\"\"Print the first few lines of the first matching transcript file.\"\"\"\n",
    "    for cand in _candidate_paths(paths, subject, story):\n",
    "        if cand.exists():\n",
    "            print(f'Preview for {subject} / {story} -> {cand}')\n",
    "            with cand.open('r', encoding='utf-8', errors='replace') as fh:\n",
    "                for idx, line in enumerate(fh):\n",
    "                    if idx >= max_lines:\n",
    "                        break\n",
    "                    print(line.rstrip())\n",
    "            print('-' * 60)\n",
    "            return\n",
    "    print(f'No transcript file found for {subject} / {story}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc2c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Batch generation ---------------------------------------------------------------\n",
    "results_summary: List[Dict[str, Any]] = []\n",
    "coverage_rows: List[Dict[str, Any]] = []\n",
    "for subject, story in subject_story_pairs:\n",
    "    print(f\">>> Generating categories for {subject} / {story}\")\n",
    "    try:\n",
    "        result = generate_category_time_series(\n",
    "            subject,\n",
    "            story,\n",
    "            cfg_base=cfg,\n",
    "            categories_cfg_base=categories_cfg,\n",
    "            cluster_csv_path=cluster_csv_path,\n",
    "            temporal_weighting=TEMPORAL_WEIGHTING,\n",
    "            prototype_weight_power=prototype_weight_power,\n",
    "            smoothing_seconds=SMOOTHING_SECONDS,\n",
    "            smoothing_method=SMOOTHING_METHOD,\n",
    "            gaussian_sigma_seconds=GAUSSIAN_SIGMA_SECONDS,\n",
    "            smoothing_pad=SMOOTHING_PAD_MODE,\n",
    "            seconds_bin_width=SECONDS_BIN_WIDTH,\n",
    "            fallback_model=FALLBACK_ALIGNED,\n",
    "            record_coverage=REPORT_TOKEN_COVERAGE,\n",
    "            save_outputs=SAVE_OUTPUTS,\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        warnings.warn(f'FAILED: {subject} / {story} ({exc})')\n",
    "        results_summary.append({\n",
    "            'subject': subject,\n",
    "            'story': story,\n",
    "            'status': 'error',\n",
    "            'error': str(exc),\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    coverage = result.get('coverage')\n",
    "    if coverage:\n",
    "        row = {'subject': subject, 'story': story}\n",
    "        row.update(coverage)\n",
    "        coverage_rows.append(row)\n",
    "\n",
    "    results_summary.append({\n",
    "        'subject': subject,\n",
    "        'story': story,\n",
    "        'status': 'ok',\n",
    "        'tokens': len(result.get('event_records', [])),\n",
    "        'canonical_bins': len(result.get('canonical_edges', [])) - 1,\n",
    "        'trs': len(result.get('tr_edges', [])) - 1,\n",
    "        'categories': len(result.get('category_columns', [])),\n",
    "        'subject_features': str(result.get('output_root')),\n",
    "        'story_features': str(result.get('canonical_root')),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(results_summary)\n",
    "if not summary_df.empty:\n",
    "    summary_df = summary_df.sort_values(['status', 'subject', 'story']).reset_index(drop=True)\n",
    "display(summary_df)\n",
    "\n",
    "coverage_df = pd.DataFrame(coverage_rows)\n",
    "if not coverage_df.empty:\n",
    "    coverage_df = coverage_df.sort_values(['pct_tokens_combined', 'subject', 'story']).reset_index(drop=True)\n",
    "    combined_total = coverage_df['tokens_total'].sum()\n",
    "    combined_hits = coverage_df['tokens_combined'].sum()\n",
    "    combined_pct = 100.0 * combined_hits / combined_total if combined_total else 0.0\n",
    "    print(f\"Combined coverage across all processed stories: {combined_hits}/{combined_total} ({combined_pct:.2f}%)\")\n",
    "    print('Lowest combined coverage stories:')\n",
    "    display(coverage_df[['subject', 'story', 'tokens_total', 'tokens_oov', 'pct_tokens_combined', 'pct_unique_combined']].head(10))\n",
    "else:\n",
    "    print('No coverage data collected; ensure REPORT_TOKEN_COVERAGE is enabled.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cd536a",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Inspect `summary_df` to confirm every pair completed.\n",
    "- Review `coverage_df` to see combined token coverage and spot low-coverage stories.\n",
    "- The generated CSVs live under `features/subjects/<subject>/<story>/` and `features/stories/<story>/`.\n",
    "- Re-run this notebook whenever transcript corrections, embeddings, or smoothing settings change.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
