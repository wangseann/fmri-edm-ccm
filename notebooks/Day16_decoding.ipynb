{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2c3b54d",
   "metadata": {},
   "source": [
    "# Day 16 \u2013 Decoding & Evaluation\n",
    "\n",
    "Extends the ROI forecasting pipeline with text decoding, encoding-based reranking, and evaluation controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7c40a9",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nproject_root = Path('/flash/PaoU/seann/fmri-edm-ccm')\nos.chdir(project_root)\n\nsys.path.append(str(project_root))\nsys.path.append('/flash/PaoU/seann/pyEDM/src')\nsys.path.append('/flash/PaoU/seann/MDE-main/src')\n\nfrom pyEDM import Simplex, SMap, CCM, ComputeError\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom MDE import MDE\n\nfrom src.utils import load_yaml\nfrom src import features, roi\nfrom src.edm_ccm import English1000Loader\nfrom src.decoding import (\n    load_transcript_words,\n    make_tr_windows,\n    reference_text_windows,\n    PCProjector,\n    ROIEncoder,\n    pc_encoding_score,\n    roi_encoding_score,\n    eval_text_list,\n    identification_matrix,\n)\n\ntry:\n    from src.decoding import BeamDecoder\nexcept ImportError:\n    BeamDecoder = None\nfrom IPython.display import display\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547a95b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_yaml('configs/demo.yaml')\n",
    "dec_cfg = cfg.get('decoding', {})\n",
    "SUB, STORY = cfg['subject'], cfg['story']\n",
    "paths = cfg['paths']\n",
    "\n",
    "target_type = str(dec_cfg.get('target_type', 'pcs')).lower()\n",
    "target_basis_for_selection = str(dec_cfg.get('target_basis_for_selection', 'embedding')).lower()\n",
    "selection_val_windows = int(dec_cfg.get('selection_val_windows', 40))\n",
    "selection_metric = str(dec_cfg.get('selection_metric', 'cca_mean')).lower()\n",
    "selection_cca_components = int(dec_cfg.get('selection_cca_components', 3))\n",
    "selection_every_k_steps = max(1, int(dec_cfg.get('selection_every_k_steps', 1)))\n",
    "selection_timeout_s = dec_cfg.get('selection_timeout_s')\n",
    "if selection_timeout_s is not None:\n",
    "    selection_timeout_s = float(selection_timeout_s)\n",
    "use_topk_pcs_cfg = int(dec_cfg.get('use_topk_pcs', 5))\n",
    "\n",
    "weights_cfg = dec_cfg.get('weights', {}) or {}\n",
    "selection_weights = {\n",
    "    'embedding': float(weights_cfg.get('embedding', 1.0)),\n",
    "    'categories': float(weights_cfg.get('categories', 0.0)),\n",
    "    'roi': float(weights_cfg.get('roi', 0.0)),\n",
    "}\n",
    "\n",
    "print(f\"Subject/Story: {SUB} {STORY}\")\n",
    "print('Decoding config:', dec_cfg)\n",
    "\n",
    "X_full = features.load_english1000_TR(SUB, STORY, paths)\n",
    "semantic_dim = X_full.shape[1]\n",
    "\n",
    "env = pd.Series(features.load_envelope_TR(SUB, STORY, paths), name='env')\n",
    "wr = pd.Series(features.load_wordrate_TR(SUB, STORY, paths), name='wr')\n",
    "R = roi.load_schaefer_timeseries_TR(SUB, STORY, cfg['n_parcels'], paths)\n",
    "\n",
    "output_root = Path(paths['figs']) / SUB / STORY / 'day16_decoding'\n",
    "output_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "english1000_path = Path(paths['data_root']) / 'derivative' / 'english1000sm.hf5'\n",
    "if english1000_path.exists():\n",
    "    english_loader = English1000Loader(english1000_path)\n",
    "    print('Loaded English1000 embeddings from', english1000_path)\n",
    "else:\n",
    "    english_loader = None\n",
    "    print('WARNING: English1000 embeddings not found at', english1000_path)\n",
    "\n",
    "print('Semantic matrix:', X_full.shape)\n",
    "print('Drivers:', env.shape, wr.shape)\n",
    "print('ROI matrix:', R.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d795a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_grid = [1, 2]\n",
    "delta_options = [1, 2]\n",
    "delta_default = int(dec_cfg.get('delta_default', 1))\n",
    "E_univ_grid = [2, 3, 4]\n",
    "E_cap = 6\n",
    "theiler_min = max(int(cfg.get('theiler_min', 3)), 1)\n",
    "lib_sizes_primary = sorted(int(v) for v in cfg['lib_sizes'])\n",
    "\n",
    "roi_cols = [f'roi_{idx}' for idx in range(R.shape[1])]\n",
    "max_tau = max(tau_grid)\n",
    "max_lag_primary = max_tau * (E_cap - 1)\n",
    "if X_full.shape[0] <= max_lag_primary:\n",
    "    raise ValueError('Time series too short for requested lag configuration')\n",
    "\n",
    "N_total = X_full.shape[0]\n",
    "N_trim = N_total - max_lag_primary\n",
    "\n",
    "\n",
    "def make_splits(n_samples):\n",
    "    train_end = max(1, int(np.floor(n_samples * 0.5)))\n",
    "    val_span = max(1, int(np.floor(n_samples * 0.25)))\n",
    "    val_end = min(n_samples - 1, train_end + val_span)\n",
    "    if val_end <= train_end:\n",
    "        val_end = min(n_samples - 1, train_end + 1)\n",
    "    test_end = n_samples\n",
    "    if test_end <= val_end:\n",
    "        test_end = min(n_samples, val_end + 1)\n",
    "    return {\n",
    "        'train': (0, train_end),\n",
    "        'val': (train_end, val_end),\n",
    "        'test': (val_end, test_end),\n",
    "    }\n",
    "\n",
    "splits = make_splits(N_trim)\n",
    "print('Split indices:', splits)\n",
    "\n",
    "train_end_trim = splits['train'][1]\n",
    "train_end_global = max_lag_primary + train_end_trim\n",
    "Z_train, pca = features.pca_fit_transform(X_full[:train_end_global], cfg['pca_components'])\n",
    "Z = pca.transform(X_full)\n",
    "\n",
    "np.save(output_root / 'pca_components.npy', pca.components_)\n",
    "np.save(output_root / 'pca_mean.npy', pca.mean_)\n",
    "\n",
    "sem_pc1 = pd.Series(Z[:, 0], name='sem_pc1')\n",
    "base = pd.DataFrame({\n",
    "    'Time': np.arange(1, N_total + 1),\n",
    "    'sem_pc1': sem_pc1,\n",
    "    'env': env,\n",
    "    'wr': wr,\n",
    "})\n",
    "for idx, col in enumerate(roi_cols):\n",
    "    base[col] = R[:, idx]\n",
    "\n",
    "\n",
    "def make_lag_dict(series, max_lag):\n",
    "    return {\n",
    "        lag: series.shift(lag).iloc[max_lag:].reset_index(drop=True)\n",
    "        for lag in range(max_lag + 1)\n",
    "    }\n",
    "\n",
    "lag_store = {name: make_lag_dict(base[name], max_lag_primary)\n",
    "             for name in ['sem_pc1', 'env', 'wr'] + roi_cols}\n",
    "\n",
    "time_trim = base['Time'].iloc[max_lag_primary:].reset_index(drop=True)\n",
    "target_trim = lag_store['sem_pc1'][0]\n",
    "N = len(time_trim)\n",
    "print(f'Usable samples after lag trimming: {N}')\n",
    "\n",
    "sem_trim_all = Z[max_lag_primary:, :]\n",
    "roi_trim_all = R[max_lag_primary:, :]\n",
    "trim_indices = np.arange(max_lag_primary, N_total)\n",
    "\n",
    "embedding_dim_trim = sem_trim_all.shape[1]\n",
    "topk = max(1, min(use_topk_pcs_cfg, embedding_dim_trim))\n",
    "if topk < use_topk_pcs_cfg:\n",
    "    print(f'Adjusted top-{use_topk_pcs_cfg} PCs to available dimension {embedding_dim_trim}')\n",
    "trimmed_pc = sem_trim_all[:, :topk]\n",
    "pc_columns = [f'sem_pc{i+1}' for i in range(topk)]\n",
    "trimmed_pc_df = pd.DataFrame(trimmed_pc, columns=pc_columns)\n",
    "trimmed_pc_df.insert(0, 'trim_index', np.arange(len(trimmed_pc)))\n",
    "trimmed_pc_df.to_csv(output_root / 'semantic_pcs_trimmed.csv', index=False)\n",
    "\n",
    "\n",
    "def format_state(state_spec):\n",
    "    return ' | '.join(f'{var}:{lag}' for var, lag in state_spec)\n",
    "\n",
    "\n",
    "def evaluate_state(state_spec, phase='select', delta_step=1,\n",
    "                   data_store=lag_store, splits=splits,\n",
    "                   theiler=theiler_min, lib_sizes=lib_sizes_primary):\n",
    "    if not state_spec:\n",
    "        raise ValueError('state_spec must contain at least one coordinate')\n",
    "    df = pd.DataFrame({'Time': time_trim.values, 'target': target_trim.values})\n",
    "    max_lag_state = 0\n",
    "    columns = []\n",
    "    for var, lag in state_spec:\n",
    "        if var not in data_store:\n",
    "            raise KeyError(f'{var} not in data_store')\n",
    "        if lag not in data_store[var]:\n",
    "            raise KeyError(f'{var} lag {lag} unavailable')\n",
    "        col_name = f'{var}_lag{lag}'\n",
    "        df[col_name] = data_store[var][lag].values\n",
    "        columns.append(col_name)\n",
    "        max_lag_state = max(max_lag_state, lag)\n",
    "    train_slice = slice(*splits['train'])\n",
    "    stats = {}\n",
    "    for col in ['target'] + columns:\n",
    "        mu = df.loc[train_slice, col].mean()\n",
    "        sigma = df.loc[train_slice, col].std(ddof=0)\n",
    "        if sigma == 0 or np.isnan(sigma):\n",
    "            sigma = 1.0\n",
    "        df[col] = (df[col] - mu) / sigma\n",
    "        stats[col] = {'mean': float(mu), 'std': float(sigma)}\n",
    "    if phase == 'select':\n",
    "        lib_range = (splits['train'][0] + 1, splits['train'][1])\n",
    "        pred_range = (splits['val'][0] + 1, splits['val'][1])\n",
    "    elif phase == 'final':\n",
    "        lib_range = (splits['train'][0] + 1, splits['val'][1])\n",
    "        pred_range = (splits['test'][0] + 1, splits['test'][1])\n",
    "    else:\n",
    "        raise ValueError(f'Unknown phase {phase}')\n",
    "    if lib_range[1] <= lib_range[0] or pred_range[1] <= pred_range[0]:\n",
    "        return {'rho': np.nan, 'df': df, 'result': pd.DataFrame(), 'stats': stats,\n",
    "                'lib': lib_range, 'pred': pred_range, 'columns': columns,\n",
    "                'exclusion': max_lag_state + delta_step, 'phase': phase,\n",
    "                'delta': delta_step, 'state': state_spec}\n",
    "    train_len = splits['train'][1] - splits['train'][0]\n",
    "    knn = max(2, min(len(state_spec) + 1, max(2, train_len - 1)))\n",
    "    exclusion = max(max_lag_state + delta_step, theiler)\n",
    "    try:\n",
    "        simplex_df = Simplex(\n",
    "            dataFrame=df[['Time', 'target'] + columns],\n",
    "            columns=' '.join(columns),\n",
    "            target='target',\n",
    "            lib=f'{lib_range[0]} {lib_range[1]}',\n",
    "            pred=f'{pred_range[0]} {pred_range[1]}',\n",
    "            E=len(state_spec),\n",
    "            Tp=delta_step,\n",
    "            tau=0,\n",
    "            knn=knn,\n",
    "            exclusionRadius=exclusion,\n",
    "            embedded=True,\n",
    "            verbose=False\n",
    "        )\n",
    "        if simplex_df.empty:\n",
    "            rho = np.nan\n",
    "        else:\n",
    "            err = ComputeError(simplex_df['Observations'], simplex_df['Predictions'])\n",
    "            rho = float(err.get('rho', np.nan))\n",
    "    except Exception as exc:\n",
    "        print(f'Simplex failed for state {format_state(state_spec)} ({phase}): {exc}')\n",
    "        simplex_df = pd.DataFrame()\n",
    "        rho = np.nan\n",
    "    return {\n",
    "        'rho': rho,\n",
    "        'df': df[['Time', 'target'] + columns],\n",
    "        'result': simplex_df,\n",
    "        'stats': stats,\n",
    "        'lib': lib_range,\n",
    "        'pred': pred_range,\n",
    "        'columns': columns,\n",
    "        'exclusion': exclusion,\n",
    "        'phase': phase,\n",
    "        'delta': delta_step,\n",
    "        'state': state_spec,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d817ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_roi_cols = {}\n",
    "for roi_name in roi_cols:\n",
    "    for lag in range(0, max_lag_primary + 1):\n",
    "        lagged_roi_cols[f'{roi_name}_lag{lag}'] = lag_store[roi_name][lag].values\n",
    "\n",
    "mde_df = pd.DataFrame({'sem_pc1': target_trim.values})\n",
    "for col_name, values in lagged_roi_cols.items():\n",
    "    mde_df[col_name] = values\n",
    "\n",
    "N_trim = len(mde_df)\n",
    "lib_span = [splits['train'][0] + 1, splits['train'][1]]\n",
    "pred_span = [splits['val'][0] + 1, splits['val'][1]]\n",
    "\n",
    "\n",
    "def clamp_span(span, max_len):\n",
    "    start, end = span\n",
    "    start = min(max(1, start), max_len)\n",
    "    end = min(max(1, end), max_len)\n",
    "    if end < start:\n",
    "        end = start\n",
    "    return [start, end]\n",
    "\n",
    "lib_span = clamp_span(lib_span, N_trim)\n",
    "pred_span = clamp_span(pred_span, N_trim)\n",
    "\n",
    "p_lib_sizes = sorted({max(1, min(99, int(round(size / N_trim * 100)))) for size in lib_sizes_primary if size < N_trim})\n",
    "if not p_lib_sizes:\n",
    "    p_lib_sizes = [10, 25, 50, 75]\n",
    "\n",
    "print('Trimmed samples for MDE:', N_trim)\n",
    "print('Library span:', lib_span, 'Prediction span:', pred_span)\n",
    "print('pLibSizes (%):', p_lib_sizes)\n",
    "\n",
    "mde = MDE(\n",
    "    dataFrame=mde_df,\n",
    "    target='sem_pc1',\n",
    "    removeColumns=['sem_pc1'],\n",
    "    D=E_cap,\n",
    "    lib=lib_span,\n",
    "    pred=pred_span,\n",
    "    Tp=delta_default,\n",
    "    tau=-1,\n",
    "    exclusionRadius=theiler_min,\n",
    "    sample=5,\n",
    "    pLibSizes=p_lib_sizes,\n",
    "    ccmSlope=0.0,\n",
    "    crossMapRhoMin=0.0,\n",
    "    embedDimRhoMin=0.0,\n",
    "    cores=1,\n",
    "    noTime=True,\n",
    "    verbose=False,\n",
    "    consoleOut=False\n",
    ")\n",
    "mde.Run()\n",
    "\n",
    "mde_output = mde.MDEOut.copy()\n",
    "mde_output.insert(0, 'step', range(1, len(mde_output) + 1))\n",
    "\n",
    "\n",
    "def base_roi_name(var_name):\n",
    "    return var_name.split('_lag')[0] if '_lag' in var_name else var_name\n",
    "\n",
    "mde_variables = mde_output['variables'].tolist()\n",
    "mde_roi_bases = [base_roi_name(var) for var in mde_variables if var.startswith('roi_')]\n",
    "mde_roi_labels = sorted({int(name.split('_')[1]) for name in mde_roi_bases})\n",
    "print('MDE-selected variables:', mde_variables)\n",
    "\n",
    "mde_rank_df = mde_output.rename(columns={'variables': 'variable', 'rho': 'rho_val'})\n",
    "mde_rank_df.to_csv(output_root / 'mde_rank.csv', index=False)\n",
    "\n",
    "# ROI univariate baseline\n",
    "roi_univ_records = []\n",
    "for roi_name in roi_cols:\n",
    "    for tau in tau_grid:\n",
    "        for E in E_univ_grid:\n",
    "            lags = [tau * k for k in range(E)]\n",
    "            if lags[-1] > max_lag_primary:\n",
    "                continue\n",
    "            state = [(roi_name, lag) for lag in lags]\n",
    "            res = evaluate_state(state, phase='select')\n",
    "            roi_univ_records.append({\n",
    "                'roi': roi_name,\n",
    "                'tau': tau,\n",
    "                'E': E,\n",
    "                'state': state,\n",
    "                'rho_val': res['rho'],\n",
    "            })\n",
    "\n",
    "roi_univ_df = pd.DataFrame(roi_univ_records).sort_values('rho_val', ascending=False).reset_index(drop=True)\n",
    "best_univ_row = roi_univ_df.iloc[0]\n",
    "roi_univ_state = best_univ_row['state']\n",
    "roi_univ_select = evaluate_state(roi_univ_state, phase='select')\n",
    "roi_univ_final = evaluate_state(roi_univ_state, phase='final')\n",
    "print('ROI-Univariate best state:', format_state(roi_univ_state))\n",
    "print('Validation rho:', roi_univ_select['rho'])\n",
    "print('Test rho:', roi_univ_final['rho'])\n",
    "\n",
    "# ROI multivariate candidates from MDE order\n",
    "roi_multi_candidates = []\n",
    "for var in mde_variables:\n",
    "    if not var.startswith('roi_'):\n",
    "        continue\n",
    "    if '_lag' in var:\n",
    "        base, lag = var.rsplit('_lag', 1)\n",
    "        roi_multi_candidates.append((base, int(lag)))\n",
    "    else:\n",
    "        roi_multi_candidates.append((var, 0))\n",
    "\n",
    "print('ROI-Multivariate candidate coordinate count:', len(roi_multi_candidates))\n",
    "if roi_multi_candidates:\n",
    "    print('First candidates:', format_state(roi_multi_candidates[:min(3, len(roi_multi_candidates))]))\n",
    "\n",
    "roi_multi_state = list(roi_multi_candidates)\n",
    "roi_multi_select = None\n",
    "roi_multi_final = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8543d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize placeholders for ROI decoding artifacts; populated after selection\n",
    "sem_pred_trim = None\n",
    "roi_state_matrix = None\n",
    "roi_state_columns = []\n",
    "state_scaler = None\n",
    "ridge_semantic = None\n",
    "mde_steps = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee4d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_s = float(dec_cfg.get('tr_seconds', cfg['TR']))\n",
    "window_len_tr = int(dec_cfg.get('window_len_tr', 6))\n",
    "stride_tr = int(dec_cfg.get('stride_tr', 3))\n",
    "hrf_shift = int(dec_cfg.get('hrf_shift_tr', 0))\n",
    "\n",
    "windows_tr = make_tr_windows(N, tr_s, window_len_tr, stride_tr, hrf_shift)\n",
    "window_records = []\n",
    "for idx, (start, end) in enumerate(windows_tr):\n",
    "    record = {\n",
    "        'window_index': idx,\n",
    "        'start_trim': start,\n",
    "        'end_trim': end,\n",
    "        'start_global_tr': max_lag_primary + start,\n",
    "        'end_global_tr': max_lag_primary + end,\n",
    "        'start_sec': (max_lag_primary + start) * tr_s,\n",
    "        'end_sec': (max_lag_primary + end) * tr_s,\n",
    "    }\n",
    "    window_records.append(record)\n",
    "\n",
    "windows_df = pd.DataFrame(window_records)\n",
    "windows_df.to_csv(output_root / 'tr_windows.csv', index=False)\n",
    "print(f'Generated {len(windows_df)} windows.')\n",
    "\n",
    "try:\n",
    "    word_events = load_transcript_words(paths, SUB, STORY)\n",
    "    print(f'Loaded {len(word_events)} transcript intervals.')\n",
    "except FileNotFoundError as exc:\n",
    "    print(f'Transcript unavailable: {exc}')\n",
    "    word_events = []\n",
    "\n",
    "reference_texts = []\n",
    "reference_map = {}\n",
    "if word_events:\n",
    "    windows_sec = [(row['start_sec'], row['end_sec']) for row in window_records]\n",
    "    reference_texts = reference_text_windows(word_events, windows_sec)\n",
    "    ref_df = pd.DataFrame({'window_index': windows_df['window_index'], 'reference_text': reference_texts})\n",
    "    ref_df.to_csv(output_root / 'reference_texts.csv', index=False)\n",
    "    reference_map = dict(zip(windows_df['window_index'], reference_texts))\n",
    "\n",
    "win_test_mask = (windows_df['start_trim'] >= splits['test'][0]) & (windows_df['end_trim'] <= splits['test'][1])\n",
    "test_windows_df = windows_df[win_test_mask].reset_index(drop=True)\n",
    "test_windows_df.to_csv(output_root / 'test_windows.csv', index=False)\n",
    "print(f'Test windows: {len(test_windows_df)}')\n",
    "\n",
    "if reference_map:\n",
    "    test_reference_texts = [reference_map.get(idx, '') for idx in test_windows_df['window_index']]\n",
    "else:\n",
    "    test_reference_texts = []\n",
    "\n",
    "val_mask = (windows_df['start_trim'] >= splits['val'][0]) & (windows_df['end_trim'] <= splits['val'][1])\n",
    "val_candidates = windows_df[val_mask].copy()\n",
    "if selection_val_windows > 0 and len(val_candidates) > selection_val_windows:\n",
    "    start_idx = max(0, (len(val_candidates) - selection_val_windows) // 2)\n",
    "    val_candidates = val_candidates.iloc[start_idx:start_idx + selection_val_windows].copy()\n",
    "\n",
    "if val_candidates.empty and selection_val_windows > 0:\n",
    "    non_test_mask = windows_df['end_trim'] <= splits['test'][0]\n",
    "    fallback_df = windows_df[non_test_mask].copy()\n",
    "    if fallback_df.empty:\n",
    "        fallback_df = windows_df.copy()\n",
    "    if len(fallback_df) > selection_val_windows:\n",
    "        start_idx = max(0, (len(fallback_df) - selection_val_windows) // 2)\n",
    "        fallback_df = fallback_df.iloc[start_idx:start_idx + selection_val_windows].copy()\n",
    "    val_candidates = fallback_df\n",
    "\n",
    "validation_windows_df = val_candidates.reset_index(drop=True)\n",
    "validation_windows_df.to_csv(output_root / 'validation_windows.csv', index=False)\n",
    "print('Validation windows:', len(validation_windows_df))\n",
    "\n",
    "if reference_map:\n",
    "    validation_reference_texts = [reference_map.get(idx, '') for idx in validation_windows_df['window_index']]\n",
    "else:\n",
    "    validation_reference_texts = ['' for _ in range(len(validation_windows_df))]\n",
    "\n",
    "validation_meta = []\n",
    "for row in validation_windows_df.itertuples():\n",
    "    start = int(row.start_trim)\n",
    "    end = int(row.end_trim)\n",
    "    n_tr = end - start\n",
    "    if n_tr <= 0:\n",
    "        continue\n",
    "    tr_edges = np.linspace(row.start_sec, row.end_sec, n_tr + 1)\n",
    "    validation_meta.append({\n",
    "        'window_index': int(row.window_index),\n",
    "        'trim_slice': (start, end),\n",
    "        'tr_edges': tr_edges,\n",
    "        'observed_embedding': sem_trim_all[start:end],\n",
    "        'observed_pc': trimmed_pc[start:end],\n",
    "    })\n",
    "print('Validation windows prepared for selection:', len(validation_meta))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d2bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_projector_full = PCProjector(pca.components_, pca.mean_)\n",
    "pc_projector = PCProjector(pca.components_[:topk], pca.mean_)\n",
    "selection_projector = embedding_projector_full if target_basis_for_selection == 'embedding' else pc_projector\n",
    "selection_embedding_method = 'cosine' if selection_metric == 'cosine_mean' else 'cca'\n",
    "print(f\"Selection basis: {target_basis_for_selection}; projector dims = {selection_projector.n_components}\")\n",
    "\n",
    "\n",
    "def _distribute_word_times(count: int, start: float, end: float):\n",
    "    if count <= 0:\n",
    "        return []\n",
    "    duration = max(end - start, 1e-6)\n",
    "    step = duration / count\n",
    "    return [(start + i * step, start + (i + 1) * step) for i in range(count)]\n",
    "\n",
    "\n",
    "def aggregate_candidate_series(\n",
    "    text: str,\n",
    "    tr_edges: np.ndarray,\n",
    "    projector: PCProjector,\n",
    "    max_components: int | None = None,\n",
    ") -> np.ndarray | None:\n",
    "    if english_loader is None:\n",
    "        return None\n",
    "    raw_tokens = [tok for tok in text.strip().split() if tok]\n",
    "    if not raw_tokens:\n",
    "        return None\n",
    "    lookup = getattr(english_loader, 'lookup', {})\n",
    "    vectors = [lookup.get(tok.lower()) for tok in raw_tokens]\n",
    "    vectors = [vec for vec in vectors if vec is not None]\n",
    "    if not vectors:\n",
    "        return None\n",
    "    word_matrix = np.vstack(vectors)\n",
    "    pc_words = projector.word_to_pc(word_matrix)\n",
    "    if max_components is not None and pc_words.shape[1] > max_components:\n",
    "        pc_words = pc_words[:, :max_components]\n",
    "    word_times = _distribute_word_times(len(vectors), float(tr_edges[0]), float(tr_edges[-1]))\n",
    "    return projector.aggregate_to_TR(pc_words, word_times, tr_edges)\n",
    "\n",
    "\n",
    "def compute_embedding_score(target: np.ndarray, candidate: np.ndarray, method: str, cca_components: int) -> float:\n",
    "    score = pc_encoding_score(target, candidate, method=method, cca_components=cca_components)\n",
    "    return float(score) if np.isfinite(score) else float('-inf')\n",
    "\n",
    "\n",
    "def compute_roi_score(target: np.ndarray, candidate: np.ndarray, method: str, cca_components: int) -> float:\n",
    "    score = roi_encoding_score(target, candidate, method=method, cca_components=cca_components)\n",
    "    return float(score) if np.isfinite(score) else float('-inf')\n",
    "\n",
    "\n",
    "def category_scorer(text: str, tr_edges: np.ndarray, category_target: np.ndarray | None) -> float:\n",
    "    _ = (text, tr_edges, category_target)\n",
    "    return float('-inf')\n",
    "\n",
    "\n",
    "def combined_score(\n",
    "    text: str,\n",
    "    tr_edges: np.ndarray,\n",
    "    *,\n",
    "    embedding_target: np.ndarray | None = None,\n",
    "    embedding_projector: PCProjector | None = None,\n",
    "    embedding_method: str = 'cca',\n",
    "    cca_components: int = 5,\n",
    "    category_target: np.ndarray | None = None,\n",
    "    roi_target: np.ndarray | None = None,\n",
    "    roi_encoder: ROIEncoder | None = None,\n",
    "    roi_method: str = 'mean',\n",
    "    roi_cca_components: int | None = None,\n",
    "    weights: dict | None = None,\n",
    "    cache: dict | None = None,\n",
    ") -> float:\n",
    "    weights = weights or selection_weights\n",
    "    score_sum = 0.0\n",
    "    contributed = False\n",
    "    edges_key = (len(tr_edges), float(tr_edges[-1] - tr_edges[0]))\n",
    "\n",
    "    if embedding_target is not None and embedding_projector is not None:\n",
    "        weight = float(weights.get('embedding', 0.0))\n",
    "        if weight > 0:\n",
    "            max_components = embedding_target.shape[1]\n",
    "            cache_key = ('embedding', text, edges_key, embedding_projector.n_components, max_components)\n",
    "            candidate = None\n",
    "            if cache is not None and cache_key in cache:\n",
    "                candidate = cache[cache_key]\n",
    "            else:\n",
    "                candidate = aggregate_candidate_series(text, tr_edges, embedding_projector, max_components=max_components)\n",
    "                if cache is not None:\n",
    "                    cache[cache_key] = candidate\n",
    "            if candidate is not None and candidate.shape[1] >= max_components:\n",
    "                candidate_use = candidate[:, :max_components]\n",
    "                emb_score = compute_embedding_score(embedding_target, candidate_use, embedding_method, cca_components)\n",
    "                if np.isfinite(emb_score):\n",
    "                    score_sum += weight * emb_score\n",
    "                    contributed = True\n",
    "\n",
    "    if category_target is not None:\n",
    "        weight = float(weights.get('categories', 0.0))\n",
    "        if weight > 0:\n",
    "            cat_score = category_scorer(text, tr_edges, category_target)\n",
    "            if np.isfinite(cat_score):\n",
    "                score_sum += weight * cat_score\n",
    "                contributed = True\n",
    "\n",
    "    if roi_target is not None and roi_encoder is not None:\n",
    "        weight = float(weights.get('roi', 0.0))\n",
    "        if weight > 0:\n",
    "            cache_key = ('roi', text, edges_key, pc_projector.n_components)\n",
    "            candidate = None\n",
    "            if cache is not None and cache_key in cache:\n",
    "                candidate = cache[cache_key]\n",
    "            else:\n",
    "                candidate = aggregate_candidate_series(text, tr_edges, pc_projector, max_components=pc_projector.n_components)\n",
    "                if cache is not None:\n",
    "                    cache[cache_key] = candidate\n",
    "            if candidate is not None and candidate.size > 0:\n",
    "                try:\n",
    "                    roi_pred = roi_encoder.predict(candidate)\n",
    "                except Exception:\n",
    "                    roi_pred = None\n",
    "                if roi_pred is not None:\n",
    "                    cca_rois = roi_cca_components if roi_cca_components is not None else min(roi_target.shape[1], cca_components)\n",
    "                    roi_score = compute_roi_score(roi_target, roi_pred, roi_method, cca_rois)\n",
    "                    if np.isfinite(roi_score):\n",
    "                        score_sum += weight * roi_score\n",
    "                        contributed = True\n",
    "\n",
    "    if not contributed:\n",
    "        return float('-inf')\n",
    "    return float(score_sum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "selection_records = []\n",
    "best_metric = float('-inf')\n",
    "best_state_metric = []\n",
    "best_rho_val = float('-inf')\n",
    "best_state_rho = list(roi_multi_state)\n",
    "timed_out_any = False\n",
    "\n",
    "candidate_pool = reference_texts if reference_texts else []\n",
    "if english_loader is None:\n",
    "    print('WARNING: English1000 unavailable; decoding-metric selection disabled. Using rho fallback.')\n",
    "if validation_meta and not candidate_pool:\n",
    "    print('Validation selection skipped: candidate text pool empty.')\n",
    "selection_candidate_cache = {}\n",
    "candidate_index_map = {text: idx for idx, text in enumerate(candidate_pool)}\n",
    "\n",
    "ridge_alpha = float(dec_cfg.get('pc_decoder_reg', 1.0))\n",
    "\n",
    "\n",
    "def evaluate_decoding_state(state):\n",
    "    if not validation_meta or not candidate_pool:\n",
    "        return float('nan'), False\n",
    "    start_time = perf_counter()\n",
    "    cols = [f'{var}_lag{lag}' for var, lag in state]\n",
    "    matrix = np.column_stack([lag_store[var][lag].values for var, lag in state])\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(matrix[slice(*splits['train'])])\n",
    "    X_all = scaler.transform(matrix)\n",
    "    target_array = sem_trim_all if target_basis_for_selection == 'embedding' else trimmed_pc\n",
    "    ridge = Ridge(alpha=ridge_alpha)\n",
    "    ridge.fit(X_train, target_array[slice(*splits['train'])])\n",
    "    pred_all = ridge.predict(X_all)\n",
    "\n",
    "    per_window_scores = []\n",
    "    score_matrix = []\n",
    "    for meta, ref_text in zip(validation_meta, validation_reference_texts):\n",
    "        if selection_timeout_s is not None and (perf_counter() - start_time) > selection_timeout_s:\n",
    "            return float('nan'), True\n",
    "        start, end = meta['trim_slice']\n",
    "        tr_edges = meta['tr_edges']\n",
    "        embedding_target = pred_all[start:end]\n",
    "        if embedding_target.ndim != 2 or embedding_target.shape[0] != len(tr_edges) - 1:\n",
    "            per_window_scores.append(float('-inf'))\n",
    "            score_matrix.append([float('-inf')] * len(candidate_pool))\n",
    "            continue\n",
    "        if embedding_target.size == 0:\n",
    "            per_window_scores.append(float('-inf'))\n",
    "            score_matrix.append([float('-inf')] * len(candidate_pool))\n",
    "            continue\n",
    "        if target_basis_for_selection == 'pcs' and embedding_target.shape[1] > topk:\n",
    "            embedding_target = embedding_target[:, :topk]\n",
    "        elif target_basis_for_selection == 'embedding':\n",
    "            observed_dim = meta['observed_embedding'].shape[1]\n",
    "            if embedding_target.shape[1] > observed_dim:\n",
    "                embedding_target = embedding_target[:, :observed_dim]\n",
    "        row_scores = []\n",
    "        best_window_score = float('-inf')\n",
    "        for cand_text in candidate_pool:\n",
    "            score = combined_score(\n",
    "                cand_text,\n",
    "                tr_edges,\n",
    "                embedding_target=embedding_target,\n",
    "                embedding_projector=selection_projector,\n",
    "                embedding_method=selection_embedding_method,\n",
    "                cca_components=selection_cca_components,\n",
    "                weights=selection_weights,\n",
    "                cache=selection_candidate_cache,\n",
    "            )\n",
    "            row_scores.append(score)\n",
    "            if score > best_window_score:\n",
    "                best_window_score = score\n",
    "        per_window_scores.append(best_window_score)\n",
    "        score_matrix.append(row_scores)\n",
    "\n",
    "    scores_arr = np.asarray(per_window_scores, dtype=float)\n",
    "    scores_arr[~np.isfinite(scores_arr)] = np.nan\n",
    "    if selection_metric in {'cca_mean', 'cosine_mean'}:\n",
    "        metric_value = float(np.nanmean(scores_arr)) if np.isfinite(scores_arr).any() else float('nan')\n",
    "    elif selection_metric == 'ident_diag_pct':\n",
    "        diag_percentiles = []\n",
    "        for row_scores, ref_text in zip(score_matrix, validation_reference_texts):\n",
    "            if not ref_text:\n",
    "                continue\n",
    "            col_idx = candidate_index_map.get(ref_text)\n",
    "            if col_idx is None or col_idx >= len(row_scores):\n",
    "                continue\n",
    "            row_arr = np.asarray(row_scores, dtype=float)\n",
    "            value = row_arr[col_idx]\n",
    "            finite = row_arr[np.isfinite(row_arr)]\n",
    "            if finite.size == 0 or not np.isfinite(value):\n",
    "                continue\n",
    "            diag_percentiles.append(float(np.mean(finite <= value)))\n",
    "        metric_value = float(np.mean(diag_percentiles)) if diag_percentiles else float('nan')\n",
    "    else:\n",
    "        metric_value = float(np.nanmean(scores_arr)) if np.isfinite(scores_arr).any() else float('nan')\n",
    "    return metric_value, False\n",
    "\n",
    "\n",
    "cumulative_state = []\n",
    "for step_idx, coord in enumerate(roi_multi_state, start=1):\n",
    "    cumulative_state = cumulative_state + [coord]\n",
    "    select_eval = evaluate_state(cumulative_state, phase='select')\n",
    "    final_eval = evaluate_state(cumulative_state, phase='final')\n",
    "    rho_val = float(select_eval['rho']) if select_eval else float('nan')\n",
    "    if np.isfinite(rho_val) and rho_val > best_rho_val:\n",
    "        best_rho_val = rho_val\n",
    "        best_state_rho = list(cumulative_state)\n",
    "    delta_val = rho_val - (selection_records[-1]['rho_val'] if selection_records else 0.0)\n",
    "\n",
    "    metric_value = float('nan')\n",
    "    timed_out = False\n",
    "    should_eval = bool(validation_meta and candidate_pool and ((step_idx % selection_every_k_steps == 0) or (step_idx == len(roi_multi_state))))\n",
    "    if should_eval:\n",
    "        metric_value, timed_out = evaluate_decoding_state(cumulative_state)\n",
    "        if timed_out:\n",
    "            timed_out_any = True\n",
    "        if not timed_out and np.isfinite(metric_value) and (not np.isfinite(best_metric) or metric_value > best_metric):\n",
    "            best_metric = metric_value\n",
    "            best_state_metric = list(cumulative_state)\n",
    "\n",
    "    selection_records.append({\n",
    "        'step': step_idx,\n",
    "        'roi': coord[0],\n",
    "        'lag': coord[1],\n",
    "        'rho_val': rho_val,\n",
    "        'rho_test': float(final_eval['rho']) if final_eval else float('nan'),\n",
    "        'delta_rho_val': delta_val,\n",
    "        'state': format_state(cumulative_state),\n",
    "        'selection_metric': metric_value,\n",
    "        'selection_metric_name': selection_metric,\n",
    "        'selection_timed_out': timed_out,\n",
    "        'evaluated': should_eval,\n",
    "    })\n",
    "\n",
    "if roi_multi_state and timed_out_any:\n",
    "    print('Decoding selection timed out; falling back to rho-based criterion.')\n",
    "\n",
    "metric_selected = roi_multi_state and not timed_out_any and np.isfinite(best_metric)\n",
    "if metric_selected and best_state_metric:\n",
    "    roi_multi_state = best_state_metric\n",
    "    print('Selected ROI-Multivariate state by decoding metric:', format_state(roi_multi_state))\n",
    "    print(f\"Selection metric ({selection_metric}):\", best_metric)\n",
    "else:\n",
    "    roi_multi_state = best_state_rho if best_state_rho else list(roi_multi_state)\n",
    "    metric_selected = False\n",
    "    print('Selected ROI-Multivariate state by rho-based fallback:', format_state(roi_multi_state))\n",
    "\n",
    "roi_multi_select = evaluate_state(roi_multi_state, phase='select') if roi_multi_state else {'rho': np.nan}\n",
    "roi_multi_final = evaluate_state(roi_multi_state, phase='final') if roi_multi_state else {'rho': np.nan}\n",
    "\n",
    "final_eval = evaluate_state(roi_multi_state, phase='final') if roi_multi_state else None\n",
    "if final_eval and not final_eval['result'].empty:\n",
    "    pred_df = final_eval['result'].copy()\n",
    "    pred_df.rename(columns={'Predictions': 'sem_pc1_pred', 'Observations': 'sem_pc1_obs'}, inplace=True)\n",
    "    pred_df.to_csv(output_root / 'sem_pc1_predictions.csv', index=False)\n",
    "\n",
    "roi_state_matrix = None\n",
    "roi_state_columns = []\n",
    "state_scaler = None\n",
    "ridge_semantic = None\n",
    "sem_pred_trim = None\n",
    "\n",
    "if roi_multi_state:\n",
    "    roi_state_columns = [f'{var}_lag{lag}' for var, lag in roi_multi_state]\n",
    "    roi_state_matrix = np.column_stack([lag_store[var][lag].values for var, lag in roi_multi_state])\n",
    "\n",
    "    state_scaler = StandardScaler()\n",
    "    X_train = state_scaler.fit_transform(roi_state_matrix[slice(*splits['train'])])\n",
    "    X_all = state_scaler.transform(roi_state_matrix)\n",
    "\n",
    "    ridge_semantic = Ridge(alpha=ridge_alpha)\n",
    "    ridge_semantic.fit(X_train, trimmed_pc[slice(*splits['train'])])\n",
    "    sem_pred_trim = ridge_semantic.predict(X_all)\n",
    "\n",
    "    sem_pred_df = pd.DataFrame(sem_pred_trim, columns=pc_columns)\n",
    "    sem_pred_df.insert(0, 'trim_index', np.arange(len(sem_pred_trim)))\n",
    "    sem_pred_df.to_csv(output_root / 'semantic_pc_predictions.csv', index=False)\n",
    "\n",
    "    roi_state_df = pd.DataFrame(roi_state_matrix, columns=roi_state_columns)\n",
    "    roi_state_df.insert(0, 'trim_index', np.arange(len(roi_state_df)))\n",
    "    roi_state_df.to_csv(output_root / 'roi_state_series.csv', index=False)\n",
    "else:\n",
    "    print('No ROI multivariate state selected; semantic predictions unavailable.')\n",
    "\n",
    "mde_steps = selection_records\n",
    "mde_steps_df = pd.DataFrame(selection_records)\n",
    "mde_steps_df.to_csv(output_root / 'mde_path.csv', index=False)\n",
    "\n",
    "primary_rows = [\n",
    "    {\n",
    "        'label': 'ROI-Univariate',\n",
    "        'rho_test': roi_univ_final['rho'],\n",
    "        'rho_val': roi_univ_select['rho'],\n",
    "        'E': len(roi_univ_state),\n",
    "        'tau_sel': best_univ_row['tau'],\n",
    "        'Tp': delta_default,\n",
    "        'state': format_state(roi_univ_state),\n",
    "        'selection_metric': float('nan'),\n",
    "        'selection_basis': 'univariate',\n",
    "    }\n",
    "]\n",
    "primary_rows.append({\n",
    "    'label': 'ROI-Multivariate (MDE)',\n",
    "    'rho_test': roi_multi_final['rho'] if roi_multi_final else np.nan,\n",
    "    'rho_val': roi_multi_select['rho'] if roi_multi_select else np.nan,\n",
    "    'E': len(roi_multi_state),\n",
    "    'tau_sel': 'MDE-derived',\n",
    "    'Tp': delta_default,\n",
    "    'state': format_state(roi_multi_state),\n",
    "    'selection_metric': best_metric if metric_selected else float('nan'),\n",
    "    'selection_basis': target_basis_for_selection,\n",
    "})\n",
    "primary_results_df = pd.DataFrame(primary_rows)\n",
    "primary_results_df.to_csv(output_root / 'primary_results.csv', index=False)\n",
    "primary_results_df\n",
    "\n",
    "# Build test targets now that semantic predictions are available\n",
    "test_targets = []\n",
    "for row in test_windows_df.itertuples():\n",
    "    start = int(row.start_trim)\n",
    "    end = int(row.end_trim)\n",
    "    n_tr = end - start\n",
    "    if n_tr <= 0:\n",
    "        continue\n",
    "    tr_edges = np.linspace(row.start_sec, row.end_sec, n_tr + 1)\n",
    "    pc_target = sem_pred_trim[start:end] if sem_pred_trim is not None else trimmed_pc[start:end]\n",
    "    roi_target = roi_state_matrix[start:end] if roi_state_matrix is not None else None\n",
    "    test_targets.append({\n",
    "        'window_index': int(row.window_index),\n",
    "        'trim_slice': (start, end),\n",
    "        'tr_edges': tr_edges,\n",
    "        'pc_target': pc_target,\n",
    "        'roi_target': roi_target,\n",
    "    })\n",
    "print('Prepared targets for', len(test_targets), 'test windows.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pc_score_method = str(dec_cfg.get('pc_score_method', 'mean')).lower()\n",
    "roi_score_method = str(dec_cfg.get('roi_score_method', 'mean')).lower()\n",
    "cca_components = int(dec_cfg.get('cca_components', min(topk, 5)))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "alpha_values = dec_cfg.get('alpha_sweep', [dec_cfg.get('alpha_encoding', 0.6)])\n",
    "if not isinstance(alpha_values, (list, tuple)):\n",
    "    alpha_values = [alpha_values]\n",
    "alpha_values = [float(val) for val in alpha_values]\n",
    "\n",
    "print(f'PC projector prepared with top-{topk} components.')\n",
    "\n",
    "roi_encoder = None\n",
    "if roi_state_matrix is not None:\n",
    "    roi_encoder = ROIEncoder(alpha=float(dec_cfg.get('roi_encoding_reg', 1.0)))\n",
    "    roi_encoder.fit(trimmed_pc[slice(*splits['train'])], roi_state_matrix[slice(*splits['train'])])\n",
    "    print('ROI encoder fitted on training split.')\n",
    "else:\n",
    "    print('ROI encoder unavailable (no ROI multivariate state).')\n",
    "\n",
    "if reference_texts:\n",
    "    print('Reference texts available for', len(reference_texts), 'windows.')\n",
    "else:\n",
    "    print('Reference texts unavailable; nearest-neighbor decoding will be skipped if needed.')\n",
    "\n",
    "if test_targets:\n",
    "    print('Test targets ready:', len(test_targets))\n",
    "else:\n",
    "    print('No test targets prepared; decoding will be skipped.')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0cbb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_outputs = {}\n",
    "alpha_summary = []\n",
    "decode_cache = {}\n",
    "\n",
    "beam_available = False\n",
    "if BeamDecoder is not None and english_loader is not None and test_targets:\n",
    "    lm_name = dec_cfg.get('lm_name', 'gpt2')\n",
    "    beam_size = int(dec_cfg.get('beam_size', 5))\n",
    "    topk_next = int(dec_cfg.get('topk_next', 10))\n",
    "    max_tokens = int(dec_cfg.get('max_tokens_per_window', 25))\n",
    "    prompt_text = dec_cfg.get('prompt_text', '')\n",
    "    for alpha in alpha_values:\n",
    "        try:\n",
    "            decoder = BeamDecoder(\n",
    "                lm_name=lm_name,\n",
    "                beam_size=beam_size,\n",
    "                topk_next=topk_next,\n",
    "                alpha_encoding=float(alpha),\n",
    "                max_tokens_per_window=max_tokens,\n",
    "            )\n",
    "        except ImportError as exc:\n",
    "            print(f'Beam decoder unavailable for alpha {alpha}:', exc)\n",
    "            decoded_outputs = {}\n",
    "            beam_available = False\n",
    "            break\n",
    "        beam_available = True\n",
    "        decoded_texts = []\n",
    "        for target in test_targets:\n",
    "            embedding_target = target['pc_target']\n",
    "            roi_target = target['roi_target']\n",
    "            tr_edges = target['tr_edges']\n",
    "            roi_cca = min(cca_components, roi_target.shape[1]) if roi_target is not None else None\n",
    "\n",
    "            def scorer(text, tr_edges_inner, _unused, embedding_target=embedding_target, roi_target=roi_target, roi_cca=roi_cca):\n",
    "                return combined_score(\n",
    "                    text,\n",
    "                    tr_edges_inner,\n",
    "                    embedding_target=embedding_target,\n",
    "                    embedding_projector=pc_projector,\n",
    "                    embedding_method=pc_score_method,\n",
    "                    cca_components=cca_components,\n",
    "                    roi_target=roi_target,\n",
    "                    roi_encoder=roi_encoder,\n",
    "                    roi_method=roi_score_method,\n",
    "                    roi_cca_components=roi_cca,\n",
    "                    weights=selection_weights,\n",
    "                    cache=decode_cache,\n",
    "                )\n",
    "\n",
    "            decoded = decoder.decode_window(prompt_text, scorer, tr_edges, embedding_target)\n",
    "            decoded_texts.append(decoded.strip())\n",
    "        key = f'beam_alpha_{alpha:.2f}'\n",
    "        decoded_outputs[key] = decoded_texts\n",
    "        alpha_summary.append({'method': 'beam', 'alpha': float(alpha), 'n_windows': len(decoded_texts)})\n",
    "if not beam_available:\n",
    "    print('Beam decoding unavailable; falling back to nearest-neighbor reference selection.')\n",
    "\n",
    "if (not decoded_outputs or not beam_available) and reference_texts:\n",
    "    candidate_pool = reference_texts\n",
    "    nn_texts = []\n",
    "    for target in test_targets:\n",
    "        tr_edges = target['tr_edges']\n",
    "        embedding_target = target['pc_target']\n",
    "        roi_target = target['roi_target']\n",
    "        roi_cca = min(cca_components, roi_target.shape[1]) if roi_target is not None else None\n",
    "        best_text = ''\n",
    "        best_score = float('-inf')\n",
    "        for cand in candidate_pool:\n",
    "            score = combined_score(\n",
    "                cand,\n",
    "                tr_edges,\n",
    "                embedding_target=embedding_target,\n",
    "                embedding_projector=pc_projector,\n",
    "                embedding_method=pc_score_method,\n",
    "                cca_components=cca_components,\n",
    "                roi_target=roi_target,\n",
    "                roi_encoder=roi_encoder,\n",
    "                roi_method=roi_score_method,\n",
    "                roi_cca_components=roi_cca,\n",
    "                weights=selection_weights,\n",
    "                cache=decode_cache,\n",
    "            )\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_text = cand\n",
    "        nn_texts.append(best_text)\n",
    "    decoded_outputs['nearest_neighbor'] = nn_texts\n",
    "    alpha_summary.append({'method': 'nearest_neighbor', 'alpha': None, 'n_windows': len(nn_texts)})\n",
    "\n",
    "decoded_records = []\n",
    "for method, texts in decoded_outputs.items():\n",
    "    for window_idx, text in zip(test_windows_df['window_index'], texts):\n",
    "        decoded_records.append({'method': method, 'window_index': int(window_idx), 'decoded_text': text})\n",
    "\n",
    "decoded_df = pd.DataFrame(decoded_records)\n",
    "decoded_df.to_csv(output_root / 'decoded_texts.csv', index=False)\n",
    "pd.DataFrame(alpha_summary).to_csv(output_root / 'alpha_decode_summary.csv', index=False)\n",
    "print('Decoded outputs saved for methods:', list(decoded_outputs.keys()))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}