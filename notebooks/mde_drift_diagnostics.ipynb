{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b038173a",
   "metadata": {},
   "source": [
    "# Drift Diagnostics for MDE Category Predictions\n",
    "\n",
    "This notebook inspects multi-delay embedding (MDE) category predictions alongside the underlying Schaefer ROI time series. It helps you quantify low-frequency drift, relate it to dominant brain components, and try simple mitigation strategies such as projecting out drift-prone principal components (PCs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aa3439",
   "metadata": {},
   "source": [
    "## How to use this notebook\n",
    "\n",
    "1. Adjust the configuration cell (paths, subject/story, smoothing config, and categories).\n",
    "2. Run the data loading cell to gather predictions and compute principal components of the brain activity.\n",
    "3. Review the automatically generated summaries and plots to identify drift sources.\n",
    "4. Optionally project out selected PCs and (if desired) write corrected prediction CSVs back to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77137b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 120,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "})\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.float_format\", lambda v: f\"{v:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749246a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== configuration ====\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "SUBJECT = \"UTS01\"\n",
    "STORY = \"wheretheressmoke\"\n",
    "\n",
    "# Day26 smoothing directory that holds per-category folders\n",
    "PREDICTION_ROOT = PROJECT_ROOT / \"figs\" / SUBJECT / STORY / \"day26_smoothing_cli_MDE50step\"\n",
    "\n",
    "# Brain activity cache (Schaefer ROI time series)\n",
    "BRAIN_TS_PATH = PROJECT_ROOT / \"data_cache\" / SUBJECT / STORY / \"schaefer_400.npy\"\n",
    "CACHE_META_PATH = PROJECT_ROOT / \"data_cache\" / SUBJECT / STORY / \"cache_meta.json\"\n",
    "\n",
    "# Category subset and smoothing configuration\n",
    "SMOOTHING_DIR = \"gauss_1p00\"  # e.g. \"gauss_1p00\", \"movavg_1p00\", or \"none_0p00\"\n",
    "CATEGORY_NAMES: Optional[Sequence[str]] = None  # e.g. [\"cat_temporal_\", \"cat_locational_\"]\n",
    "\n",
    "# PCA / plotting parameters\n",
    "MAX_PCS = 6\n",
    "PLOT_PC_LIMIT = 6\n",
    "\n",
    "# Mitigation settings\n",
    "DRIFT_PC_COUNT = 1  # number of leading PCs to project out when correcting\n",
    "PC_CORRECTION_TARGETS: Optional[Sequence[str]] = None  # None -> apply correction to all loaded categories\n",
    "SAVE_CORRECTED = False\n",
    "CORRECTED_SUFFIX = \"driftcorrected\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52449908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== helper functions ====\n",
    "def discover_categories(prediction_root: Path, explicit: Optional[Sequence[str]] = None) -> List[str]:\n",
    "    if explicit:\n",
    "        return [str(name) for name in explicit]\n",
    "    if not prediction_root.exists():\n",
    "        raise FileNotFoundError(f\"Prediction root not found: {prediction_root}\")\n",
    "    return sorted(\n",
    "        entry.name\n",
    "        for entry in prediction_root.iterdir()\n",
    "        if entry.is_dir() and entry.name.startswith(\"cat_\")\n",
    "    )\n",
    "\n",
    "\n",
    "def load_prediction(\n",
    "    prediction_root: Path,\n",
    "    category: str,\n",
    "    smoothing_dir: str,\n",
    "    subject: str,\n",
    "    story: str,\n",
    ") -> Tuple[pd.DataFrame, Path]:\n",
    "    cat_dir = prediction_root / category\n",
    "    if not cat_dir.exists():\n",
    "        raise FileNotFoundError(f\"Missing category folder: {cat_dir}\")\n",
    "    config_dir = cat_dir / smoothing_dir if smoothing_dir else cat_dir\n",
    "    if not config_dir.exists():\n",
    "        raise FileNotFoundError(f\"Missing smoothing folder for {category}: {config_dir}\")\n",
    "\n",
    "    safe_name = category.rstrip(\"_\")\n",
    "    candidate = (\n",
    "        config_dir\n",
    "        / subject\n",
    "        / story\n",
    "        / \"day22_category_mde\"\n",
    "        / f\"mde_{safe_name}_best_prediction.csv\"\n",
    "    )\n",
    "    if not candidate.exists():\n",
    "        matches = list(config_dir.glob(f\"**/mde_{safe_name}_best_prediction.csv\"))\n",
    "        if not matches:\n",
    "            matches = list(config_dir.glob(\"**/mde_*_best_prediction.csv\"))\n",
    "        if not matches:\n",
    "            raise FileNotFoundError(\n",
    "                f\"No best-prediction CSV found for {category} under {config_dir}\"\n",
    "            )\n",
    "        candidate = sorted(matches)[0]\n",
    "\n",
    "    df = pd.read_csv(candidate)\n",
    "    df = df.copy()\n",
    "    for col in (\"trim_index\", \"start_sec\", \"time\", \"target\", \"prediction\"):\n",
    "        if col in df:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    df.sort_values(\"trim_index\", inplace=True, ignore_index=True)\n",
    "    return df, candidate\n",
    "\n",
    "\n",
    "def load_cache_meta(path: Path) -> Dict[str, Any]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Cache metadata not found: {path}\")\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "        return json.load(fh)\n",
    "\n",
    "\n",
    "def load_brain_timeseries(path: Path) -> np.ndarray:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Brain timeseries not found: {path}\")\n",
    "    arr = np.load(path)\n",
    "    if arr.ndim != 2:\n",
    "        raise ValueError(f\"Expected 2D array for brain timeseries, found shape {arr.shape}\")\n",
    "    mask = np.isfinite(arr)\n",
    "    if not np.all(mask):\n",
    "        # Replace NaNs/infs with column medians to keep PCA stable\n",
    "        col_median = np.nanmedian(arr, axis=0, keepdims=True)\n",
    "        arr = np.where(mask, arr, col_median)\n",
    "    return arr.astype(float)\n",
    "\n",
    "\n",
    "def compute_pca(matrix: np.ndarray, n_components: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    if matrix.size == 0:\n",
    "        raise ValueError(\"Cannot compute PCA on an empty matrix\")\n",
    "    X = np.asarray(matrix, dtype=float)\n",
    "    X = X - np.nanmean(X, axis=0, keepdims=True)\n",
    "    U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    if n_components <= 0:\n",
    "        n_components = Vt.shape[0]\n",
    "    keep = min(n_components, Vt.shape[0])\n",
    "    scores = U[:, :keep] * S[:keep]\n",
    "    components = Vt[:keep]\n",
    "    if X.shape[0] > 1:\n",
    "        explained_raw = (S ** 2) / (X.shape[0] - 1)\n",
    "        total = explained_raw.sum()\n",
    "        explained = explained_raw[:keep] / total if total > 0 else np.zeros(keep)\n",
    "    else:\n",
    "        explained = np.zeros(keep)\n",
    "    return scores, components, explained\n",
    "\n",
    "\n",
    "def build_pc_dataframe(scores: np.ndarray, start_sec: np.ndarray) -> pd.DataFrame:\n",
    "    data = {f\"PC{i+1}\": scores[:, i] for i in range(scores.shape[1])}\n",
    "    data[\"start_sec\"] = start_sec\n",
    "    data[\"tr_index\"] = np.arange(len(start_sec), dtype=int)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def compute_linear_trend(x: Iterable[float], y: Iterable[float]) -> Dict[str, float]:\n",
    "    x_arr = np.asarray(list(x), dtype=float)\n",
    "    y_arr = np.asarray(list(y), dtype=float)\n",
    "    mask = np.isfinite(x_arr) & np.isfinite(y_arr)\n",
    "    if mask.sum() < 3:\n",
    "        return {key: np.nan for key in (\"slope\", \"intercept\", \"delta\", \"r2\", \"slope_per_min\")}\n",
    "    x_valid = x_arr[mask]\n",
    "    y_valid = y_arr[mask]\n",
    "    slope, intercept = np.polyfit(x_valid, y_valid, 1)\n",
    "    fitted = slope * x_valid + intercept\n",
    "    resid = y_valid - fitted\n",
    "    var_y = np.var(y_valid)\n",
    "    var_resid = np.var(resid)\n",
    "    r2 = 1 - (var_resid / var_y) if var_y > 0 else np.nan\n",
    "    delta = slope * (x_valid.max() - x_valid.min())\n",
    "    slope_per_min = slope * 60.0\n",
    "    return {\n",
    "        \"slope\": slope,\n",
    "        \"intercept\": intercept,\n",
    "        \"delta\": delta,\n",
    "        \"r2\": r2,\n",
    "        \"slope_per_min\": slope_per_min,\n",
    "    }\n",
    "\n",
    "\n",
    "def safe_corr(a: Iterable[float], b: Iterable[float]) -> float:\n",
    "    a_arr = np.asarray(list(a), dtype=float)\n",
    "    b_arr = np.asarray(list(b), dtype=float)\n",
    "    mask = np.isfinite(a_arr) & np.isfinite(b_arr)\n",
    "    if mask.sum() < 3:\n",
    "        return float(\"nan\")\n",
    "    cov = np.corrcoef(a_arr[mask], b_arr[mask])\n",
    "    return float(cov[0, 1])\n",
    "\n",
    "\n",
    "def project_out_components(values: np.ndarray, basis: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    y = np.asarray(values, dtype=float)\n",
    "    X = np.asarray(basis, dtype=float)\n",
    "    if X.ndim == 1:\n",
    "        X = X[:, None]\n",
    "    mask = np.isfinite(y) & np.all(np.isfinite(X), axis=1)\n",
    "    correction = np.full(y.shape, np.nan, dtype=float)\n",
    "    coeffs = np.full(X.shape[1], np.nan, dtype=float)\n",
    "    if mask.sum() >= X.shape[1] and mask.any():\n",
    "        coeffs, *_ = np.linalg.lstsq(X[mask], y[mask], rcond=None)\n",
    "        correction_full = X @ coeffs\n",
    "        correction[mask] = correction_full[mask]\n",
    "    return correction, coeffs\n",
    "\n",
    "\n",
    "def build_category_summary(\n",
    "    category_store: Dict[str, Dict[str, Any]],\n",
    "    max_pc: int,\n",
    ") -> pd.DataFrame:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    for cat, payload in category_store.items():\n",
    "        merged = payload[\"merged\"]\n",
    "        trend_pred = compute_linear_trend(merged[\"start_sec\"], merged[\"prediction\"])\n",
    "        trend_target = compute_linear_trend(merged[\"start_sec\"], merged[\"target\"])\n",
    "        trend_resid = compute_linear_trend(merged[\"start_sec\"], merged[\"residual\"])\n",
    "        row: Dict[str, Any] = {\n",
    "            \"category\": cat,\n",
    "            \"n_samples\": merged.shape[0],\n",
    "            \"start_sec_min\": float(np.nanmin(merged[\"start_sec\"])),\n",
    "            \"start_sec_max\": float(np.nanmax(merged[\"start_sec\"])),\n",
    "            \"pred_slope\": trend_pred[\"slope\"],\n",
    "            \"pred_slope_per_min\": trend_pred[\"slope_per_min\"],\n",
    "            \"pred_delta\": trend_pred[\"delta\"],\n",
    "            \"pred_r2_trend\": trend_pred[\"r2\"],\n",
    "            \"target_slope\": trend_target[\"slope\"],\n",
    "            \"target_slope_per_min\": trend_target[\"slope_per_min\"],\n",
    "            \"target_delta\": trend_target[\"delta\"],\n",
    "            \"residual_slope\": trend_resid[\"slope\"],\n",
    "            \"residual_slope_per_min\": trend_resid[\"slope_per_min\"],\n",
    "            \"residual_r2_trend\": trend_resid[\"r2\"],\n",
    "        }\n",
    "        for idx in range(max_pc):\n",
    "            col = f\"PC{idx + 1}\"\n",
    "            if col in merged:\n",
    "                row[f\"corr_{col}\"] = safe_corr(merged[\"prediction\"], merged[col])\n",
    "        if \"merged_corrected\" in payload:\n",
    "            corrected = payload[\"merged_corrected\"]\n",
    "            trend_corr = compute_linear_trend(\n",
    "                corrected[\"start_sec\"], corrected[\"prediction_corrected\"]\n",
    "            )\n",
    "            row.update(\n",
    "                {\n",
    "                    \"pred_corrected_slope\": trend_corr[\"slope\"],\n",
    "                    \"pred_corrected_slope_per_min\": trend_corr[\"slope_per_min\"],\n",
    "                    \"pred_corrected_delta\": trend_corr[\"delta\"],\n",
    "                    \"pred_corrected_r2_trend\": trend_corr[\"r2\"],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            row.update(\n",
    "                {\n",
    "                    \"pred_corrected_slope\": np.nan,\n",
    "                    \"pred_corrected_slope_per_min\": np.nan,\n",
    "                    \"pred_corrected_delta\": np.nan,\n",
    "                    \"pred_corrected_r2_trend\": np.nan,\n",
    "                }\n",
    "            )\n",
    "        rows.append(row)\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    summary = pd.DataFrame(rows)\n",
    "    summary.set_index(\"category\", inplace=True)\n",
    "    if \"pred_slope_per_min\" in summary:\n",
    "        summary.sort_values(\n",
    "            by=\"pred_slope_per_min\",\n",
    "            key=lambda col: np.abs(col.fillna(0.0)),\n",
    "            ascending=False,\n",
    "            inplace=True,\n",
    "        )\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a92f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== load predictions and brain PCs ====\n",
    "          cache_meta = load_cache_meta(CACHE_META_PATH)\n",
    "          tr = float(cache_meta.get(\"tr\", 2.0))\n",
    "          print(f\"Loaded cache metadata from {CACHE_META_PATH} (TR = {tr}s)\")\n",
    "\n",
    "          brain_ts = load_brain_timeseries(BRAIN_TS_PATH)\n",
    "          time_axis = np.arange(brain_ts.shape[0], dtype=float) * tr\n",
    "          print(f\"Brain timeseries shape: {brain_ts.shape}\")\n",
    "\n",
    "          pc_scores, pc_components, pc_explained = compute_pca(brain_ts, MAX_PCS)\n",
    "          pc_df = build_pc_dataframe(pc_scores, time_axis)\n",
    "          pc_df[\"time_key\"] = (pc_df[\"start_sec\"] * 1000).round().astype(int)\n",
    "          print(f\"Computed {pc_scores.shape[1]} principal components (explained var shown below)\")\n",
    "\n",
    "          categories = discover_categories(PREDICTION_ROOT, CATEGORY_NAMES)\n",
    "          print(f\"Scanning categories (smoothing='{SMOOTHING_DIR}'):\n",
    "\" + \", \".join(categories))\n",
    "\n",
    "          category_store: Dict[str, Dict[str, Any]] = {}\n",
    "          for cat in categories:\n",
    "              try:\n",
    "                  pred_df, csv_path = load_prediction(PREDICTION_ROOT, cat, SMOOTHING_DIR, SUBJECT, STORY)\n",
    "              except FileNotFoundError as exc:\n",
    "                  print(f\"[warning] {exc}\")\n",
    "                  continue\n",
    "              pred_df[\"time_key\"] = (pred_df[\"start_sec\"] * 1000).round().astype(int)\n",
    "              merged = pred_df.merge(pc_df, how=\"left\", on=\"time_key\", suffixes=(\"\", \"_pc\"))\n",
    "              merged[\"residual\"] = merged[\"prediction\"] - merged[\"target\"]\n",
    "              category_store[cat] = {\n",
    "                  \"csv_path\": csv_path,\n",
    "                  \"raw\": pred_df,\n",
    "                  \"merged\": merged,\n",
    "              }\n",
    "          print(f\"Loaded {len(category_store)} categories with aligned PCs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42166439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== principal component diagnostics ====\n",
    "            pc_summary_rows = []\n",
    "            for idx in range(pc_scores.shape[1]):\n",
    "                col = f\"PC{idx + 1}\"\n",
    "                metrics = compute_linear_trend(pc_df[\"start_sec\"], pc_df[col])\n",
    "                metrics.update(\n",
    "                    {\n",
    "                        \"pc\": col,\n",
    "                        \"explained_var\": pc_explained[idx],\n",
    "                    }\n",
    "                )\n",
    "                pc_summary_rows.append(metrics)\n",
    "\n",
    "            pc_summary_df = pd.DataFrame(pc_summary_rows).set_index(\"pc\")\n",
    "            display(pc_summary_df)\n",
    "\n",
    "            limit = min(PLOT_PC_LIMIT, pc_scores.shape[1])\n",
    "            fig, axes = plt.subplots(limit, 1, figsize=(12, max(2.2 * limit, 3.5)), sharex=True)\n",
    "            if limit == 1:\n",
    "                axes = [axes]\n",
    "            for ax, idx in zip(axes, range(limit)):\n",
    "                col = f\"PC{idx + 1}\"\n",
    "                series = pc_df[col]\n",
    "                ax.plot(pc_df[\"start_sec\"], series, label=col)\n",
    "                trend = compute_linear_trend(pc_df[\"start_sec\"], series)\n",
    "                if np.isfinite(trend[\"slope\"]):\n",
    "                    fitted = trend[\"intercept\"] + trend[\"slope\"] * pc_df[\"start_sec\"]\n",
    "                    ax.plot(pc_df[\"start_sec\"], fitted, linestyle=\"--\", color=\"black\", linewidth=1.0)\n",
    "                    ax.text(\n",
    "                        0.01,\n",
    "                        0.95,\n",
    "                        f\"slope/min={trend['slope_per_min']:.4f}\n",
    "explained={pc_explained[idx]:.3f}\",\n",
    "                        transform=ax.transAxes,\n",
    "                        va=\"top\",\n",
    "                        ha=\"left\",\n",
    "                        fontsize=9,\n",
    "                        bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.7, edgecolor=\"none\"),\n",
    "                    )\n",
    "                ax.set_ylabel(col)\n",
    "                ax.grid(alpha=0.25)\n",
    "            axes[-1].set_xlabel(\"Time (s)\")\n",
    "            fig.suptitle(\"Schaefer ROI principal component time courses\", y=0.995)\n",
    "            fig.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61de1ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== category-level drift summary (pre/post correction) ====\n",
    "category_summary_df = build_category_summary(category_store, pc_scores.shape[1])\n",
    "if category_summary_df.empty:\n",
    "    print(\"No categories loaded. Check configuration paths.\")\n",
    "else:\n",
    "    display(category_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a633d08a",
   "metadata": {},
   "source": [
    "### Inspect a single category\n",
    "\n",
    "Update `category_to_plot` below to visualise the target, prediction, and (if present) drift-corrected series for one category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe024f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_to_plot = \"cat_temporal_\"\n",
    "\n",
    "payload = category_store.get(category_to_plot)\n",
    "if payload is None:\n",
    "    print(f\"Category '{category_to_plot}' not found. Available: {list(category_store)}\")\n",
    "else:\n",
    "    merged = payload.get(\"merged_corrected\", payload[\"merged\"])\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.plot(merged[\"start_sec\"], merged[\"target\"], label=\"Target\", linewidth=1.3)\n",
    "    ax.plot(merged[\"start_sec\"], merged[\"prediction\"], label=\"Prediction\", linewidth=1.1)\n",
    "    if \"prediction_corrected\" in merged:\n",
    "        ax.plot(\n",
    "            merged[\"start_sec\"],\n",
    "            merged[\"prediction_corrected\"],\n",
    "            label=\"Prediction (corrected)\",\n",
    "            linewidth=1.1,\n",
    "            linestyle=\"--\",\n",
    "        )\n",
    "    ax.set_title(f\"{category_to_plot} – predictions vs target\")\n",
    "    ax.set_xlabel(\"Start time (s)\")\n",
    "    ax.set_ylabel(\"Value\")\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4321e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== drift mitigation: project out leading PCs ====\n",
    "correction_reports: List[Dict[str, Any]] = []\n",
    "pc_cols = [f\"PC{i}\" for i in range(1, DRIFT_PC_COUNT + 1)]\n",
    "\n",
    "if DRIFT_PC_COUNT <= 0:\n",
    "    print(\"DRIFT_PC_COUNT set to 0; predictions will be copied without adjustment.\")\n",
    "\n",
    "target_categories = (\n",
    "    [str(cat) for cat in PC_CORRECTION_TARGETS]\n",
    "    if PC_CORRECTION_TARGETS\n",
    "    else list(category_store.keys())\n",
    ")\n",
    "\n",
    "if not target_categories:\n",
    "    print(\"No categories available for correction. Ensure category_store is populated.\")\n",
    "else:\n",
    "    for cat in sorted(target_categories):\n",
    "        payload = category_store.get(cat)\n",
    "        if payload is None:\n",
    "            print(f\"[skip] Category '{cat}' not loaded.\")\n",
    "            continue\n",
    "\n",
    "        merged = payload[\"merged\"].copy()\n",
    "        available_pc_cols = [col for col in pc_cols if col in merged]\n",
    "\n",
    "        correction_applied = False\n",
    "        coeffs_list: List[float] = []\n",
    "\n",
    "        if available_pc_cols and DRIFT_PC_COUNT > 0:\n",
    "            correction, coeffs = project_out_components(\n",
    "                merged[\"prediction\"].to_numpy(),\n",
    "                merged[available_pc_cols].to_numpy(),\n",
    "            )\n",
    "            coeffs_list = coeffs.tolist() if getattr(coeffs, \"size\", 0) else []\n",
    "            if np.isfinite(correction).any():\n",
    "                correction_applied = True\n",
    "            correction = np.where(np.isfinite(correction), correction, 0.0)\n",
    "        else:\n",
    "            correction = np.zeros(merged.shape[0], dtype=float)\n",
    "            coeffs_list = [0.0] * len(available_pc_cols)\n",
    "\n",
    "        merged[\"pc_drift_component\"] = correction\n",
    "        merged[\"prediction_corrected\"] = merged[\"prediction\"] - correction\n",
    "\n",
    "        payload[\"merged_corrected\"] = merged\n",
    "        payload[\"pc_coeffs\"] = coeffs_list\n",
    "        payload[\"correction_applied\"] = correction_applied\n",
    "        category_store[cat] = payload\n",
    "\n",
    "        trend_before = compute_linear_trend(merged[\"start_sec\"], merged[\"prediction\"])\n",
    "        trend_after = compute_linear_trend(merged[\"start_sec\"], merged[\"prediction_corrected\"])\n",
    "\n",
    "        correction_reports.append(\n",
    "            {\n",
    "                \"category\": cat,\n",
    "                \"pcs_used\": \",\".join(available_pc_cols) if available_pc_cols else \"(none)\",\n",
    "                \"coefficients\": coeffs_list,\n",
    "                \"correction_applied\": correction_applied,\n",
    "                \"slope_before_per_min\": trend_before[\"slope_per_min\"],\n",
    "                \"slope_after_per_min\": trend_after[\"slope_per_min\"],\n",
    "                \"delta_before\": trend_before[\"delta\"],\n",
    "                \"delta_after\": trend_after[\"delta\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if SAVE_CORRECTED:\n",
    "            out_df = payload[\"raw\"].copy()\n",
    "            out_df[\"prediction_corrected\"] = merged[\"prediction_corrected\"]\n",
    "            out_path = payload[\"csv_path\"].with_name(\n",
    "                payload[\"csv_path\"].stem + f\"_{CORRECTED_SUFFIX}.csv\"\n",
    "            )\n",
    "            out_df.to_csv(out_path, index=False)\n",
    "            payload[\"corrected_path\"] = out_path\n",
    "            category_store[cat] = payload\n",
    "            print(f\"Saved corrected predictions for '{cat}' → {out_path}\")\n",
    "\n",
    "    if correction_reports:\n",
    "        display(pd.DataFrame(correction_reports))\n",
    "    else:\n",
    "        print(\"No correction reports generated.\")\n",
    "\n",
    "category_summary_updated = build_category_summary(category_store, pc_scores.shape[1])\n",
    "if not category_summary_updated.empty:\n",
    "    display(category_summary_updated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddd1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== visualize all corrected categories ====\n",
    "corrected_entries = sorted(\n",
    "    [\n",
    "        (cat, payload)\n",
    "        for cat, payload in category_store.items()\n",
    "        if \"merged_corrected\" in payload and \"prediction_corrected\" in payload[\"merged_corrected\"]\n",
    "    ],\n",
    "    key=lambda item: item[0],\n",
    ")\n",
    "\n",
    "if not corrected_entries:\n",
    "    print(\"No corrected categories available. Run the correction cell first or adjust PC_CORRECTION_TARGETS.\")\n",
    "else:\n",
    "    n_panels = len(corrected_entries)\n",
    "    fig, axes = plt.subplots(n_panels, 1, figsize=(12, max(3.0 * n_panels, 3.5)), sharex=False)\n",
    "    if n_panels == 1:\n",
    "        axes = [axes]\n",
    "    for ax, (cat, payload) in zip(axes, corrected_entries):\n",
    "        merged = payload[\"merged_corrected\"]\n",
    "        ax.plot(merged[\"start_sec\"], merged[\"target\"], label=\"Target\", linewidth=1.2, color=\"#1f77b4\")\n",
    "        ax.plot(\n",
    "            merged[\"start_sec\"],\n",
    "            merged[\"prediction\"],\n",
    "            label=\"Prediction\",\n",
    "            linewidth=1.0,\n",
    "            color=\"#d62728\",\n",
    "            alpha=0.6,\n",
    "        )\n",
    "        ax.plot(\n",
    "            merged[\"start_sec\"],\n",
    "            merged[\"prediction_corrected\"],\n",
    "            label=\"Prediction (corrected)\",\n",
    "            linewidth=1.1,\n",
    "            linestyle=\"--\",\n",
    "            color=\"#2ca02c\",\n",
    "        )\n",
    "        slope_before = compute_linear_trend(merged[\"start_sec\"], merged[\"prediction\"])\n",
    "        slope_after = compute_linear_trend(merged[\"start_sec\"], merged[\"prediction_corrected\"])\n",
    "        status = \"applied\" if payload.get(\"correction_applied\", False) else \"copied\"\n",
    "        ax.set_title(\n",
    "            f\"{cat} – slope/min before={slope_before['slope_per_min']:.4f}, after={slope_after['slope_per_min']:.4f} ({status})\"\n",
    "        )\n",
    "        ax.set_xlabel(\"Start time (s)\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.legend(loc=\"upper left\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}