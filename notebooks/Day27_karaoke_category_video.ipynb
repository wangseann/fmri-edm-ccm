{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53bca0a5",
   "metadata": {},
   "source": [
    "# Day 27 - Karaoke Category Video\n",
    "\n",
    "This notebook builds smoothed category time series (based on Day 19) and renders a\n",
    "karaoke-style MP4 with transcript text on top and a 4x3 grid of category series below.\n",
    "\n",
    "Workflow:\n",
    "1. Update the configuration cell.\n",
    "2. Run the generation cell to create `result`.\n",
    "3. Run the token prep cell.\n",
    "4. Pick 12 categories for the grid.\n",
    "5. Run the video cell (requires ffmpeg).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dea71d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "pd.options.display.max_columns = 60\n",
    "\n",
    "project_root = Path('/flash/PaoU/seann/fmri-edm-ccm')\n",
    "project_root.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(project_root)\n",
    "\n",
    "sys.path.append(str(project_root))\n",
    "sys.path.append('/flash/PaoU/seann/pyEDM/src')\n",
    "sys.path.append('/flash/PaoU/seann/MDE-main/src')\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    widgets = None\n",
    "    def display(obj):\n",
    "        print(obj)\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except Exception as exc:\n",
    "    plt = None\n",
    "    warnings.warn(f'Matplotlib unavailable: {exc}')\n",
    "\n",
    "from src.utils import load_yaml\n",
    "from src.decoding import load_transcript_words\n",
    "from src.edm_ccm import English1000Loader\n",
    "\n",
    "EPS = 1e-12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e813215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from subprocess import run\n",
    "\n",
    "PROJECT_ROOT = Path('/flash/PaoU/seann/fmri-edm-ccm')\n",
    "CONFIG_PATH = PROJECT_ROOT / 'configs' / 'demo.yaml'\n",
    "\n",
    "\n",
    "def run_day19_for_subject(subject: str, stories=None, dry_run=False):\n",
    "    \"\"\"Batch regenerate Day19 category series via the CLI helper.\"\"\"\n",
    "    cmd = [\n",
    "        'python',\n",
    "        str(PROJECT_ROOT / 'scripts' / 'run_day19_batch.py'),\n",
    "        '--config',\n",
    "        str(CONFIG_PATH),\n",
    "        '--subjects',\n",
    "        subject,\n",
    "    ]\n",
    "    if stories:\n",
    "        cmd.extend(['--stories', *stories])\n",
    "    if dry_run:\n",
    "        cmd.append('--dry-run')\n",
    "    print('Running:', ' '.join(cmd))\n",
    "    run(cmd, check=True)\n",
    "\n",
    "\n",
    "# Example usage (uncomment to preview without executing):\n",
    "# run_day19_for_subject('UTS01', dry_run=True)\n",
    "\n",
    "# Load default story list if available\n",
    "stories = Path('misc/story_list.txt').read_text().splitlines() if Path('misc/story_list.txt').exists() else None\n",
    "# Example usage (uses all stories by default)\n",
    "# run_day19_for_subject('UTS01', stories=stories, dry_run=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1295436a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject/story: UTS01 / wheretheressmoke\n",
      "Cluster CSV: configs/cluster_words.csv\n",
      "Temporal weighting: proportional\n",
      "Seconds bin width: 0.05\n",
      "Smoothing: moving_average | window=1.0s | sigma=0.5\n",
      "Video output: featuresqaemb/videos/karaoke_UTS01_wheretheressmoke.mp4\n",
      "Video domain: tr | fps=1 | window=30.0s\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration -------------------------------------------------------------------\n",
    "cfg = load_yaml('configs/demo.yaml')\n",
    "categories_cfg = cfg.get('categories', {}) or {}\n",
    "cluster_csv_path = categories_cfg.get('cluster_csv_path', '')\n",
    "prototype_weight_power = float(categories_cfg.get('prototype_weight_power', 1.0))\n",
    "seconds_bin_width_default = float(categories_cfg.get('seconds_bin_width', 0.05))\n",
    "temporal_weighting_default = str(categories_cfg.get('temporal_weighting', 'proportional')).lower()\n",
    "\n",
    "paths = cfg.get('paths', {})\n",
    "TR = float(cfg.get('TR', 2.0))\n",
    "features_root = Path(paths.get('featurestest', 'featurestest'))\n",
    "features_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SUBJECT = cfg.get('subject') or 'UTS01'\n",
    "STORY = cfg.get('story') or 'wheretheressmoke'\n",
    "TEMPORAL_WEIGHTING = temporal_weighting_default  # {'proportional', 'none'}\n",
    "SECONDS_BIN_WIDTH = seconds_bin_width_default\n",
    "\n",
    "# canonical smoothing controls (edit to taste)\n",
    "SMOOTHING_SECONDS = 1.00            # shorter window preserves fast dynamics for forecasting\n",
    "SMOOTHING_METHOD = 'moving_average'       # {'moving_average', 'gaussian'}\n",
    "GAUSSIAN_SIGMA_SECONDS = 0.5 * SMOOTHING_SECONDS  # tie sigma to window length for EDM\n",
    "SMOOTHING_PAD_MODE = 'reflect'      # {'edge', 'reflect'}\n",
    "\n",
    "SAVE_OUTPUTS = True  # toggle off to skip writing CSVs\n",
    "\n",
    "# video settings\n",
    "VIDEO_DIR = features_root / 'videos'\n",
    "VIDEO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "KARAOKE_OUTPUT = str(VIDEO_DIR / f'karaoke_{SUBJECT}_{STORY}.mp4')\n",
    "KARAOKE_USE_DOMAIN = 'tr'  # 'tr' or 'canonical'\n",
    "KARAOKE_FPS = 15\n",
    "KARAOKE_WINDOW_SEC = 30.0\n",
    "KARAOKE_PAD_LEFT_SEC = 0.5\n",
    "KARAOKE_PAD_RIGHT_SEC = 2.0\n",
    "KARAOKE_BIN_WORDS_MAX = 30  # None to show all words in the bin\n",
    "KARAOKE_PLAYBACK_SPEED = 7.0  # 1.0 real-time; <1 slower; >1 faster\n",
    "KARAOKE_YLIM_PAD_FRAC = 0.05  # add 5% headroom to y-lims\n",
    "KARAOKE_ZSCORE = True  # z-score each category series before plotting\n",
    "KARAOKE_LOG_EVERY_FRAMES = None  # None -> log every ~5s of video time\n",
    "KARAOKE_CATEGORY_COUNT = 12  # must be 12 for the 4x3 grid\n",
    "KARAOKE_CATEGORY_COLUMNS = None  # set to a list of 12 column names if desired\n",
    "\n",
    "print(f'Subject/story: {SUBJECT} / {STORY}')\n",
    "print(f'Cluster CSV: {cluster_csv_path or \"<none>\"}')\n",
    "print(f'Temporal weighting: {TEMPORAL_WEIGHTING}')\n",
    "print(f'Seconds bin width: {SECONDS_BIN_WIDTH}')\n",
    "print(f'Smoothing: {SMOOTHING_METHOD} | window={SMOOTHING_SECONDS}s | sigma={GAUSSIAN_SIGMA_SECONDS}')\n",
    "print(f'Video output: {KARAOKE_OUTPUT}')\n",
    "print(f'Video domain: {KARAOKE_USE_DOMAIN} | fps={KARAOKE_FPS} | window={KARAOKE_WINDOW_SEC}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f48cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def load_story_words(paths: Dict, subject: str, story: str) -> List[Tuple[str, float, float]]:\n",
    "    events = load_transcript_words(paths, subject, story)\n",
    "    if not events:\n",
    "        raise ValueError(f'No transcript events found for {subject} {story}.')\n",
    "    return [(str(word).strip(), float(start), float(end)) for word, start, end in events]\n",
    "\n",
    "\n",
    "def load_clusters_from_csv(csv_path: str) -> Dict[str, Dict[str, List[Tuple[str, float]]]]:\n",
    "    from pathlib import Path\n",
    "    if not csv_path or not Path(csv_path).exists():\n",
    "        raise FileNotFoundError(f'Cluster CSV not found at {csv_path}')\n",
    "    df = pd.read_csv(csv_path)\n",
    "    cols = {c.lower().strip(): c for c in df.columns}\n",
    "    for needed in ('category', 'word'):\n",
    "        assert needed in cols, f\"CSV must contain '{needed}' column.\"\n",
    "    cat_col = cols['category']\n",
    "    word_col = cols['word']\n",
    "    weight_col = cols.get('weight')\n",
    "    if weight_col is None:\n",
    "        df['_weight'] = 1.0\n",
    "        weight_col = '_weight'\n",
    "    df = df[[cat_col, word_col, weight_col]].copy()\n",
    "    df[word_col] = df[word_col].astype(str).str.strip().str.lower()\n",
    "    df[cat_col] = df[cat_col].astype(str).str.strip().str.lower()\n",
    "    df[weight_col] = pd.to_numeric(df[weight_col], errors='coerce').fillna(1.0).clip(lower=0.0)\n",
    "    clusters: Dict[str, Dict[str, List[Tuple[str, float]]]] = {}\n",
    "    for cat, sub in df.groupby(cat_col):\n",
    "        bucket: Dict[str, float] = {}\n",
    "        for w, wt in zip(sub[word_col].tolist(), sub[weight_col].tolist()):\n",
    "            if not w:\n",
    "                continue\n",
    "            bucket[w] = float(wt)\n",
    "        pairs = sorted(bucket.items())\n",
    "        if pairs:\n",
    "            clusters[cat] = {'words': pairs}\n",
    "    if not clusters:\n",
    "        raise ValueError('No clusters parsed from CSV.')\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def build_states_from_csv(\n",
    "    clusters: Dict[str, Dict[str, List[Tuple[str, float]]]],\n",
    "    primary_lookup: Dict[str, np.ndarray],\n",
    "    fallback=None,\n",
    "    weight_power: float = 1.0\n",
    ") -> Tuple[Dict[str, Dict], Dict[str, Dict]]:\n",
    "    category_states: Dict[str, Dict] = {}\n",
    "    category_definitions: Dict[str, Dict] = {}\n",
    "    oov_counts: Dict[str, int] = {}\n",
    "    for cat, spec in clusters.items():\n",
    "        pairs = spec.get('words', [])\n",
    "        vecs: List[np.ndarray] = []\n",
    "        weights: List[float] = []\n",
    "        found_words: List[str] = []\n",
    "        missing_words: List[str] = []\n",
    "        for word, wt in pairs:\n",
    "            vec = lookup_embedding(word, primary_lookup, fallback)\n",
    "            if vec is None:\n",
    "                missing_words.append(word)\n",
    "                continue\n",
    "            vecs.append(vec.astype(float))\n",
    "            weights.append(float(max(0.0, wt)) ** float(weight_power))\n",
    "            found_words.append(word)\n",
    "        if not vecs:\n",
    "            warnings.warn(f\"[{cat}] no usable representative embeddings; prototype will be None.\")\n",
    "            prototype = None\n",
    "            prototype_norm = None\n",
    "        else:\n",
    "            W = np.array(weights, dtype=float)\n",
    "            W = W / (W.sum() + 1e-12)\n",
    "            M = np.stack(vecs, axis=0)\n",
    "            prototype = (W[:, None] * M).sum(axis=0)\n",
    "            prototype_norm = float(np.linalg.norm(prototype))\n",
    "            if prototype_norm < EPS:\n",
    "                prototype = None\n",
    "                prototype_norm = None\n",
    "        rep_lex = {word: float(wt) for word, wt in pairs}\n",
    "        category_states[cat] = {\n",
    "            'name': cat,\n",
    "            'seeds': [],\n",
    "            'found_seeds': found_words,\n",
    "            'missing_seeds': missing_words,\n",
    "            'prototype': prototype,\n",
    "            'prototype_norm': prototype_norm,\n",
    "            'lexicon': rep_lex,\n",
    "            'expanded_count': 0,\n",
    "            'expansion_params': {'enabled': False, 'top_k': 0, 'min_sim': 0.0},\n",
    "        }\n",
    "        category_definitions[cat] = {\n",
    "            'from': 'csv',\n",
    "            'seeds': [],\n",
    "            'found_seeds': found_words,\n",
    "            'missing_seeds': missing_words,\n",
    "            'prototype_dim': int(prototype.shape[0]) if isinstance(prototype, np.ndarray) else 0,\n",
    "            'prototype_norm': prototype_norm,\n",
    "            'representative_words': rep_lex,\n",
    "            'lexicon': rep_lex,\n",
    "            'expanded_neighbors': {},\n",
    "        }\n",
    "        oov_counts[cat] = len(missing_words)\n",
    "    if any(oov_counts.values()):\n",
    "        warnings.warn(f\"OOV representative words: {oov_counts}\")\n",
    "    return category_states, category_definitions\n",
    "\n",
    "\n",
    "def build_tr_edges(word_events: Sequence[Tuple[str, float, float]], tr_s: float) -> np.ndarray:\n",
    "    if not word_events:\n",
    "        return np.arange(0, tr_s, tr_s)\n",
    "    max_end = max(end for _, _, end in word_events)\n",
    "    n_tr = max(1, int(math.ceil(max_end / tr_s)))\n",
    "    edges = np.arange(0.0, (n_tr + 1) * tr_s, tr_s, dtype=float)\n",
    "    if edges[-1] < max_end:\n",
    "        edges = np.append(edges, edges[-1] + tr_s)\n",
    "    if edges[-1] < max_end - 1e-9:\n",
    "        edges = np.append(edges, edges[-1] + tr_s)\n",
    "    return edges\n",
    "\n",
    "\n",
    "def lookup_embedding(token: str, primary_lookup: Dict[str, np.ndarray], fallback=None) -> Optional[np.ndarray]:\n",
    "    key = token.lower().strip()\n",
    "    if not key:\n",
    "        return None\n",
    "    vec = primary_lookup.get(key) if primary_lookup else None\n",
    "    if vec is not None:\n",
    "        return np.asarray(vec, dtype=float)\n",
    "    if fallback is not None:\n",
    "        try:\n",
    "            if hasattr(fallback, 'get_vector') and key in fallback:\n",
    "                return np.asarray(fallback.get_vector(key), dtype=float)\n",
    "            if hasattr(fallback, '__contains__') and key in fallback:\n",
    "                return np.asarray(fallback[key], dtype=float)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def make_category_prototype(seeds: Sequence[str], primary_lookup: Dict[str, np.ndarray], fallback=None, allow_single: bool = False) -> Tuple[Optional[np.ndarray], List[str], List[str]]:\n",
    "    found_vectors = []\n",
    "    found_words = []\n",
    "    missing_words = []\n",
    "    for seed in seeds:\n",
    "        vec = lookup_embedding(seed, primary_lookup, fallback)\n",
    "        if vec is None:\n",
    "            missing_words.append(seed)\n",
    "            continue\n",
    "        found_vectors.append(vec)\n",
    "        found_words.append(seed)\n",
    "    if not found_vectors:\n",
    "        return None, found_words, missing_words\n",
    "    if len(found_vectors) < 2 and not allow_single:\n",
    "        warnings.warn(f'Only {len(found_vectors)} usable seed(s); enable allow_single_seed to accept singleton prototypes.')\n",
    "        if not allow_single:\n",
    "            return None, found_words, missing_words\n",
    "    prototype = np.mean(found_vectors, axis=0)\n",
    "    return prototype, found_words, missing_words\n",
    "\n",
    "\n",
    "def expand_category(prototype: np.ndarray, vocab_embeddings: np.ndarray, vocab_words: Sequence[str], top_k: int, min_sim: float) -> Dict[str, float]:\n",
    "    if prototype is None or vocab_embeddings is None or vocab_words is None:\n",
    "        return {}\n",
    "    proto = np.asarray(prototype, dtype=float)\n",
    "    proto_norm = np.linalg.norm(proto)\n",
    "    if proto_norm == 0:\n",
    "        return {}\n",
    "    proto_unit = proto / proto_norm\n",
    "    vocab_norms = np.linalg.norm(vocab_embeddings, axis=1)\n",
    "    valid_mask = vocab_norms > 0\n",
    "    sims = np.full(vocab_embeddings.shape[0], -1.0, dtype=float)\n",
    "    sims[valid_mask] = (vocab_embeddings[valid_mask] @ proto_unit) / vocab_norms[valid_mask]\n",
    "    top_k_eff = min(top_k, len(sims))\n",
    "    if top_k_eff <= 0:\n",
    "        return {}\n",
    "    candidate_idx = np.argpartition(-sims, top_k_eff - 1)[:top_k_eff]\n",
    "    out = {}\n",
    "    for idx in candidate_idx:\n",
    "        score = float(sims[idx])\n",
    "        if score < min_sim:\n",
    "            continue\n",
    "        out[vocab_words[idx]] = score\n",
    "    return out\n",
    "\n",
    "\n",
    "def tr_token_overlap(token_start: float, token_end: float, tr_start: float, tr_end: float, mode: str = 'proportional') -> float:\n",
    "    token_start = float(token_start)\n",
    "    token_end = float(token_end)\n",
    "    if token_end <= token_start:\n",
    "        token_end = token_start + 1e-3\n",
    "    if mode == 'midpoint':\n",
    "        midpoint = 0.5 * (token_start + token_end)\n",
    "        return 1.0 if tr_start <= midpoint < tr_end else 0.0\n",
    "    overlap = max(0.0, min(token_end, tr_end) - max(token_start, tr_start))\n",
    "    duration = token_end - token_start\n",
    "    if duration <= 0:\n",
    "        return 1.0 if overlap > 0 else 0.0\n",
    "    return max(0.0, min(1.0, overlap / duration))\n",
    "\n",
    "\n",
    "def score_tr(token_payload: Sequence[Dict], method: str, *, lexicon: Optional[Dict[str, float]] = None, prototype: Optional[np.ndarray] = None, prototype_norm: Optional[float] = None) -> float:\n",
    "    if not token_payload:\n",
    "        return float('nan')\n",
    "    method = method.lower()\n",
    "    if method == 'count':\n",
    "        if not lexicon:\n",
    "            return float('nan')\n",
    "        total = 0.0\n",
    "        for item in token_payload:\n",
    "            weight = lexicon.get(item['word'].lower())\n",
    "            if weight is None:\n",
    "                continue\n",
    "            total += weight * item['overlap']\n",
    "        return float(total)\n",
    "    if method == 'similarity':\n",
    "        if prototype is None or prototype_norm is None or prototype_norm < EPS:\n",
    "            return float('nan')\n",
    "        num = 0.0\n",
    "        denom = 0.0\n",
    "        for item in token_payload:\n",
    "            emb = item.get('embedding')\n",
    "            if emb is None:\n",
    "                continue\n",
    "            emb_norm = item.get('embedding_norm')\n",
    "            if emb_norm is None or emb_norm < EPS:\n",
    "                continue\n",
    "            sim = float(np.dot(emb, prototype) / (emb_norm * prototype_norm))\n",
    "            num += sim * item['overlap']\n",
    "            denom += item['overlap']\n",
    "        if denom == 0:\n",
    "            return float('nan')\n",
    "        value = num / denom\n",
    "        return float(np.clip(value, -1.0, 1.0))\n",
    "    raise ValueError(f'Unknown scoring method: {method}')\n",
    "\n",
    "\n",
    "def ensure_serializable(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, (np.floating, np.integer)):\n",
    "        return obj.item()\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: ensure_serializable(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [ensure_serializable(v) for v in obj]\n",
    "    return obj\n",
    "\n",
    "\n",
    "def build_token_buckets(edges: np.ndarray, event_records: Sequence[Dict], mode: str = 'proportional') -> List[List[Dict]]:\n",
    "    if edges.size < 2:\n",
    "        return []\n",
    "    buckets: List[List[Dict]] = [[] for _ in range(len(edges) - 1)]\n",
    "    for rec in event_records:\n",
    "        start = rec['start']\n",
    "        end = rec['end']\n",
    "        if end <= edges[0] or start >= edges[-1]:\n",
    "            continue\n",
    "        start_idx = max(0, int(np.searchsorted(edges, start, side='right')) - 1)\n",
    "        end_idx = max(0, int(np.searchsorted(edges, end, side='left')))\n",
    "        end_idx = min(end_idx, len(buckets) - 1)\n",
    "        for idx in range(start_idx, end_idx + 1):\n",
    "            bucket_start = edges[idx]\n",
    "            bucket_end = edges[idx + 1]\n",
    "            if mode == 'none':\n",
    "                overlap = 1.0 if not (end <= bucket_start or start >= bucket_end) else 0.0\n",
    "            else:\n",
    "                overlap = tr_token_overlap(start, end, bucket_start, bucket_end, 'proportional')\n",
    "            if overlap <= 0:\n",
    "                continue\n",
    "            buckets[idx].append({\n",
    "                'word': rec['word'],\n",
    "                'overlap': overlap,\n",
    "                'embedding': rec['embedding'],\n",
    "                'embedding_norm': rec['embedding_norm'],\n",
    "                'token_start': rec['start'],\n",
    "                'token_end': rec['end'],\n",
    "                'bucket_start': bucket_start,\n",
    "                'bucket_end': bucket_end,\n",
    "            })\n",
    "    return buckets\n",
    "\n",
    "\n",
    "def score_time_series(edges: np.ndarray, buckets: Sequence[Sequence[Dict]], category_states: Dict[str, Dict], category_names: Sequence[str], category_columns: Sequence[str], method: str, index_name: str) -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "    n_bins = len(buckets)\n",
    "    score_matrix = np.full((n_bins, len(category_names)), np.nan, dtype=float)\n",
    "    for col_idx, cat_name in enumerate(category_names):\n",
    "        state = category_states[cat_name]\n",
    "        lexicon = state.get('lexicon')\n",
    "        prototype = state.get('prototype')\n",
    "        prototype_norm = state.get('prototype_norm')\n",
    "        for bin_idx, bucket in enumerate(buckets):\n",
    "            score_matrix[bin_idx, col_idx] = score_tr(bucket, method, lexicon=lexicon, prototype=prototype, prototype_norm=prototype_norm)\n",
    "    data = {\n",
    "        index_name: np.arange(n_bins, dtype=int),\n",
    "        'start_sec': edges[:-1],\n",
    "        'end_sec': edges[1:],\n",
    "    }\n",
    "    for col_idx, col in enumerate(category_columns):\n",
    "        data[col] = score_matrix[:, col_idx]\n",
    "    df = pd.DataFrame(data)\n",
    "    return df, score_matrix\n",
    "\n",
    "\n",
    "def build_smoothing_kernel(seconds_bin_width: float, smoothing_seconds: float, *, method: str = 'moving_average', gaussian_sigma_seconds: Optional[float] = None) -> np.ndarray:\n",
    "    if smoothing_seconds <= 0:\n",
    "        return np.array([1.0], dtype=float)\n",
    "    method = str(method or 'moving_average').lower()\n",
    "    if method == 'moving_average':\n",
    "        window_samples = max(1, int(round(smoothing_seconds / seconds_bin_width)))\n",
    "        if window_samples % 2 == 0:\n",
    "            window_samples += 1\n",
    "        kernel = np.ones(window_samples, dtype=float)\n",
    "    elif method == 'gaussian':\n",
    "        sigma_seconds = float(gaussian_sigma_seconds) if gaussian_sigma_seconds not in (None, '') else max(smoothing_seconds / 2.0, seconds_bin_width)\n",
    "        sigma_samples = max(sigma_seconds / seconds_bin_width, 1e-6)\n",
    "        half_width = max(1, int(round(3.0 * sigma_samples)))\n",
    "        grid = np.arange(-half_width, half_width + 1, dtype=float)\n",
    "        kernel = np.exp(-0.5 * (grid / sigma_samples) ** 2)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown smoothing method: {method}\")\n",
    "    kernel_sum = float(kernel.sum())\n",
    "    if kernel_sum <= 0:\n",
    "        return np.array([1.0], dtype=float)\n",
    "    return kernel / kernel_sum\n",
    "\n",
    "\n",
    "def apply_smoothing_kernel(values: np.ndarray, kernel: np.ndarray, *, pad_mode: str = 'edge', eps: float = 1e-8) -> np.ndarray:\n",
    "    if values.size == 0 or kernel.size <= 1:\n",
    "        return values.copy()\n",
    "    pad_mode = pad_mode if pad_mode in {'edge', 'reflect'} else 'edge'\n",
    "    half = kernel.size // 2\n",
    "    padded = np.pad(values, ((half, half), (0, 0)), mode=pad_mode)\n",
    "    mask = np.isfinite(padded).astype(float)\n",
    "    filled = np.where(mask, padded, 0.0)\n",
    "    smoothed = np.empty((values.shape[0], values.shape[1]), dtype=float)\n",
    "    for col in range(values.shape[1]):\n",
    "        numerator = np.convolve(filled[:, col], kernel, mode='valid')\n",
    "        denominator = np.convolve(mask[:, col], kernel, mode='valid')\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            smoothed_col = numerator / np.maximum(denominator, eps)\n",
    "        smoothed_col[denominator < eps] = np.nan\n",
    "        smoothed[:, col] = smoothed_col\n",
    "    return smoothed\n",
    "\n",
    "\n",
    "def aggregate_seconds_to_edges(canonical_edges: np.ndarray, canonical_values: np.ndarray, target_edges: np.ndarray) -> np.ndarray:\n",
    "    if canonical_values.size == 0:\n",
    "        return np.empty((len(target_edges) - 1, 0), dtype=float)\n",
    "    midpoints = 0.5 * (canonical_edges[:-1] + canonical_edges[1:])\n",
    "    bin_ids = np.digitize(midpoints, target_edges) - 1\n",
    "    if bin_ids.size:\n",
    "        bin_ids = np.clip(bin_ids, 0, len(target_edges) - 2)\n",
    "    out = np.full((len(target_edges) - 1, canonical_values.shape[1]), np.nan, dtype=float)\n",
    "    for idx in range(out.shape[0]):\n",
    "        mask = bin_ids == idx\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        values = canonical_values[mask]\n",
    "        if values.ndim == 1:\n",
    "            values = values[:, None]\n",
    "        finite_any = np.isfinite(values).any(axis=0)\n",
    "        if not finite_any.any():\n",
    "            continue\n",
    "        col_means = np.full(values.shape[1], np.nan, dtype=float)\n",
    "        col_means[finite_any] = np.nanmean(values[:, finite_any], axis=0)\n",
    "        out[idx] = col_means\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fc0ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_category_time_series(\n",
    "    subject: str,\n",
    "    story: str,\n",
    "    *,\n",
    "    cfg_base: Dict[str, Any],\n",
    "    categories_cfg_base: Dict[str, Any],\n",
    "    cluster_csv_path: str,\n",
    "    temporal_weighting: str,\n",
    "    prototype_weight_power: float,\n",
    "    smoothing_seconds: float,\n",
    "    smoothing_method: str,\n",
    "    gaussian_sigma_seconds: Optional[float],\n",
    "    smoothing_pad: str,\n",
    "    seconds_bin_width: float,\n",
    "    save_outputs: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    if not subject or not story:\n",
    "        raise ValueError('Subject and story must be provided.')\n",
    "    print(f\"=== Day19 category build for {subject} / {story} ===\")\n",
    "\n",
    "    categories_cfg = json.loads(json.dumps(categories_cfg_base or {}))\n",
    "    categories_cfg['seconds_bin_width'] = float(seconds_bin_width)\n",
    "    category_sets = categories_cfg.get('sets', {})\n",
    "    available_sets = sorted(category_sets.keys())\n",
    "    category_set_name = categories_cfg.get('category_set') or (available_sets[0] if available_sets else None)\n",
    "    if cluster_csv_path:\n",
    "        if not category_set_name:\n",
    "            category_set_name = 'csv_clusters'\n",
    "        categories_cfg['category_set'] = category_set_name\n",
    "        categories_cfg['category_score_method'] = 'similarity'\n",
    "        categories_cfg['allow_single_seed'] = True\n",
    "        categories_cfg['expansion'] = {'enabled': False}\n",
    "    category_score_method = str(categories_cfg.get('category_score_method', 'similarity')).lower()\n",
    "    overlap_mode = str(categories_cfg.get('overlap_weighting', 'proportional')).lower()\n",
    "    expansion_cfg = categories_cfg.get('expansion', {})\n",
    "    allow_single = bool(categories_cfg.get('allow_single_seed', False))\n",
    "    exp_enabled = bool(expansion_cfg.get('enabled', True))\n",
    "    exp_top_k = int(expansion_cfg.get('top_k', 2000)) if exp_enabled else 0\n",
    "    exp_min_sim = float(expansion_cfg.get('min_sim', 0.35)) if exp_enabled else 0.0\n",
    "\n",
    "    selected_set_spec = category_sets.get(category_set_name, {}) if category_sets else {}\n",
    "\n",
    "    output_root = features_root / 'subjects' / subject / story\n",
    "    canonical_root = features_root / 'stories' / story\n",
    "    if save_outputs:\n",
    "        output_root.mkdir(parents=True, exist_ok=True)\n",
    "        canonical_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    story_events = load_story_words(paths, subject, story)\n",
    "    print(f'Loaded {len(story_events)} transcript events.')\n",
    "    tr_edges = build_tr_edges(story_events, TR)\n",
    "    n_tr = len(tr_edges) - 1\n",
    "    print(f'TR edges: {len(tr_edges)} (n_tr={n_tr}) spanning {tr_edges[-1]:.2f} seconds.')\n",
    "\n",
    "    embedding_source = str(categories_cfg.get('embedding_source', 'english1000')).lower()\n",
    "    english_loader = None\n",
    "    english_lookup: Dict[str, np.ndarray] = {}\n",
    "    english_vocab: List[str] = []\n",
    "    english_matrix = None\n",
    "    if embedding_source in {'english1000', 'both'}:\n",
    "        english1000_path = Path(paths.get('data_root', '')) / 'derivative' / 'english1000sm.hf5'\n",
    "        if english1000_path.exists():\n",
    "            english_loader = English1000Loader(english1000_path)\n",
    "            english_lookup = english_loader.lookup\n",
    "            english_vocab = english_loader.vocab\n",
    "            english_matrix = english_loader.embeddings\n",
    "            print(f'Loaded English1000 embeddings from {english1000_path} (vocab={len(english_vocab)}).')\n",
    "        else:\n",
    "            raise FileNotFoundError(f'English1000 embeddings not found at {english1000_path}')\n",
    "    else:\n",
    "        print('English1000 disabled by configuration.')\n",
    "\n",
    "    word2vec_model = None\n",
    "    if embedding_source in {'word2vec', 'both'}:\n",
    "        w2v_path = categories_cfg.get('word2vec_path')\n",
    "        if w2v_path:\n",
    "            w2v_path = Path(w2v_path)\n",
    "            if w2v_path.exists():\n",
    "                try:\n",
    "                    from gensim.models import KeyedVectors\n",
    "                    binary = w2v_path.suffix.lower() in {'.bin', '.gz'}\n",
    "                    word2vec_model = KeyedVectors.load_word2vec_format(w2v_path, binary=binary)\n",
    "                    print(f'Loaded Word2Vec fallback from {w2v_path}.')\n",
    "                except Exception as exc:\n",
    "                    warnings.warn(f'Failed to load Word2Vec fallback: {exc}')\n",
    "            else:\n",
    "                warnings.warn(f'Word2Vec path does not exist: {w2v_path}')\n",
    "        else:\n",
    "            warnings.warn('Word2Vec fallback requested but no path provided.')\n",
    "    else:\n",
    "        print('Word2Vec fallback disabled.')\n",
    "\n",
    "    if cluster_csv_path:\n",
    "        csv_clusters = load_clusters_from_csv(cluster_csv_path)\n",
    "        category_states, category_definitions = build_states_from_csv(\n",
    "            csv_clusters,\n",
    "            english_lookup,\n",
    "            word2vec_model,\n",
    "            weight_power=prototype_weight_power,\n",
    "        )\n",
    "        category_names = sorted(category_states.keys())\n",
    "        category_columns = [f'cat_{name}' for name in category_names]\n",
    "        print(f\"Loaded {len(category_names)} CSV-driven categories from {cluster_csv_path}: {category_names}\")\n",
    "        zero_norm = [k for k, v in category_states.items() if v.get('prototype') is not None and (v.get('prototype_norm') or 0.0) < EPS]\n",
    "        if zero_norm:\n",
    "            warnings.warn(f\"Zero-norm prototypes (check OOV/weights): {zero_norm}\")\n",
    "    else:\n",
    "        category_states = {}\n",
    "        category_definitions = {}\n",
    "        seed_oov_counter = Counter()\n",
    "        for cat_name, cat_spec in selected_set_spec.items():\n",
    "            seeds = cat_spec.get('seeds', [])\n",
    "            explicit_words = cat_spec.get('words', [])\n",
    "            prototype = None\n",
    "            found_seeds: List[str] = []\n",
    "            missing_seeds: List[str] = []\n",
    "            if seeds:\n",
    "                prototype, found_seeds, missing_seeds = make_category_prototype(seeds, english_lookup, word2vec_model, allow_single)\n",
    "                seed_oov_counter[cat_name] = len(missing_seeds)\n",
    "                if prototype is None and category_score_method == 'similarity':\n",
    "                    warnings.warn(f\"Category '{cat_name}' has no usable prototype; TR scores will be NaN.\")\n",
    "            elif category_score_method == 'similarity':\n",
    "                warnings.warn(f'Category {cat_name} has no seeds; similarity method will yield NaNs.')\n",
    "            lexicon = {word.lower(): 1.0 for word in explicit_words}\n",
    "            for seed in found_seeds:\n",
    "                lexicon.setdefault(seed.lower(), 1.0)\n",
    "            prototype_norm = None\n",
    "            expanded_words = {}\n",
    "            if prototype is not None:\n",
    "                prototype_norm = float(np.linalg.norm(prototype))\n",
    "                if exp_enabled and english_matrix is not None:\n",
    "                    expanded_words = expand_category(prototype, english_matrix, english_vocab, exp_top_k, exp_min_sim)\n",
    "                    for word, weight in expanded_words.items():\n",
    "                        lexicon.setdefault(word.lower(), float(weight))\n",
    "            if not lexicon and category_score_method == 'count':\n",
    "                warnings.warn(f'Category {cat_name} lexicon is empty; counts will be NaN.')\n",
    "            category_states[cat_name] = {\n",
    "                'name': cat_name,\n",
    "                'seeds': seeds,\n",
    "                'found_seeds': found_seeds,\n",
    "                'missing_seeds': missing_seeds,\n",
    "                'prototype': prototype,\n",
    "                'prototype_norm': prototype_norm,\n",
    "                'lexicon': lexicon,\n",
    "                'expanded_count': len(expanded_words),\n",
    "                'expansion_params': {\n",
    "                    'enabled': exp_enabled,\n",
    "                    'top_k': exp_top_k,\n",
    "                    'min_sim': exp_min_sim,\n",
    "                },\n",
    "            }\n",
    "            category_definitions[cat_name] = {\n",
    "                'seeds': seeds,\n",
    "                'found_seeds': found_seeds,\n",
    "                'missing_seeds': missing_seeds,\n",
    "                'prototype_dim': int(prototype.shape[0]) if isinstance(prototype, np.ndarray) else 0,\n",
    "                'prototype_norm': prototype_norm,\n",
    "                'expanded_neighbors': ensure_serializable(expanded_words),\n",
    "                'lexicon': {word: float(weight) for word, weight in sorted(category_states[cat_name]['lexicon'].items())},\n",
    "            }\n",
    "        print('Category seeds missing counts:', dict(seed_oov_counter))\n",
    "        category_names = sorted(category_states.keys())\n",
    "        category_columns = [f'cat_{name}' for name in category_names]\n",
    "        print(f'Prepared {len(category_names)} categories: {category_names}')\n",
    "\n",
    "    tw_mode = str(temporal_weighting or 'proportional').lower()\n",
    "    if tw_mode not in {'proportional', 'none', 'midpoint'}:\n",
    "        raise ValueError(f'Unsupported temporal weighting: {tw_mode}')\n",
    "\n",
    "    seconds_bin_width = float(seconds_bin_width)\n",
    "    if seconds_bin_width <= 0:\n",
    "        raise ValueError('seconds_bin_width must be positive.')\n",
    "    smoothing_method = str(smoothing_method or 'moving_average').lower()\n",
    "    gaussian_sigma_seconds = gaussian_sigma_seconds if gaussian_sigma_seconds not in (None, '') else None\n",
    "    smoothing_pad = str(smoothing_pad or 'edge').lower()\n",
    "    if smoothing_pad not in {'edge', 'reflect'}:\n",
    "        smoothing_pad = 'edge'\n",
    "\n",
    "    embedding_cache: Dict[str, Optional[np.ndarray]] = {}\n",
    "    event_records: List[Dict] = []\n",
    "    tokens_with_embeddings = 0\n",
    "    for word, onset, offset in story_events:\n",
    "        token = word.strip()\n",
    "        if not token:\n",
    "            continue\n",
    "        key = token.lower()\n",
    "        if key not in embedding_cache:\n",
    "            embedding_cache[key] = lookup_embedding(token, english_lookup, word2vec_model)\n",
    "        emb = embedding_cache[key]\n",
    "        emb_norm = float(np.linalg.norm(emb)) if emb is not None else None\n",
    "        if emb is not None:\n",
    "            tokens_with_embeddings += 1\n",
    "        event_records.append({\n",
    "            'word': token,\n",
    "            'start': float(onset),\n",
    "            'end': float(offset),\n",
    "            'embedding': emb,\n",
    "            'embedding_norm': emb_norm,\n",
    "        })\n",
    "\n",
    "    total_tokens = len(event_records)\n",
    "    print(f'Tokens with embeddings: {tokens_with_embeddings}/{total_tokens} (OOV rate={(total_tokens - tokens_with_embeddings) / max(total_tokens, 1):.2%}).')\n",
    "    if not event_records:\n",
    "        raise ValueError('No token events available for category featurization.')\n",
    "\n",
    "    max_end_time = max(rec['end'] for rec in event_records)\n",
    "    canonical_edges = np.arange(0.0, max_end_time + seconds_bin_width, seconds_bin_width, dtype=float)\n",
    "    if canonical_edges[-1] < max_end_time:\n",
    "        canonical_edges = np.append(canonical_edges, canonical_edges[-1] + seconds_bin_width)\n",
    "    if canonical_edges[-1] < max_end_time - 1e-9:\n",
    "        canonical_edges = np.append(canonical_edges, canonical_edges[-1] + seconds_bin_width)\n",
    "    assert np.all(np.diff(canonical_edges) > 0), 'Non-monotone canonical edges.'\n",
    "\n",
    "    canonical_buckets = build_token_buckets(canonical_edges, event_records, tw_mode)\n",
    "    empty_canonical = sum(1 for bucket in canonical_buckets if not bucket)\n",
    "    print(f'Canonical bins without tokens: {empty_canonical}/{len(canonical_buckets)}')\n",
    "\n",
    "    canonical_df_raw, canonical_matrix = score_time_series(\n",
    "        canonical_edges,\n",
    "        canonical_buckets,\n",
    "        category_states,\n",
    "        category_names,\n",
    "        category_columns,\n",
    "        category_score_method,\n",
    "        index_name='bin_index',\n",
    "    )\n",
    "    canonical_values_raw = canonical_matrix.copy()\n",
    "    smoothing_kernel = build_smoothing_kernel(\n",
    "        seconds_bin_width,\n",
    "        smoothing_seconds,\n",
    "        method=smoothing_method,\n",
    "        gaussian_sigma_seconds=gaussian_sigma_seconds,\n",
    "    )\n",
    "    smoothing_applied = smoothing_kernel.size > 1\n",
    "    if canonical_values_raw.size and smoothing_applied:\n",
    "        canonical_values_smoothed = apply_smoothing_kernel(canonical_values_raw, smoothing_kernel, pad_mode=smoothing_pad)\n",
    "    else:\n",
    "        canonical_values_smoothed = canonical_values_raw.copy()\n",
    "\n",
    "    canonical_df_smoothed = canonical_df_raw.copy()\n",
    "    if category_columns:\n",
    "        canonical_df_smoothed.loc[:, category_columns] = canonical_values_smoothed\n",
    "    canonical_df_selected = canonical_df_smoothed if smoothing_applied else canonical_df_raw\n",
    "\n",
    "    if save_outputs:\n",
    "        canonical_root.mkdir(parents=True, exist_ok=True)\n",
    "        canonical_csv_path = canonical_root / 'category_timeseries_seconds.csv'\n",
    "        canonical_df_selected.to_csv(canonical_csv_path, index=False)\n",
    "        if smoothing_applied:\n",
    "            canonical_df_raw.to_csv(canonical_root / 'category_timeseries_seconds_raw.csv', index=False)\n",
    "        canonical_definition_path = canonical_root / 'category_definition.json'\n",
    "        with canonical_definition_path.open('w') as fh:\n",
    "            json.dump(ensure_serializable(category_definitions), fh, indent=2)\n",
    "        print(f'Saved canonical story series to {canonical_csv_path}')\n",
    "\n",
    "    tr_buckets = build_token_buckets(tr_edges, event_records, tw_mode)\n",
    "    empty_tr = sum(1 for bucket in tr_buckets if not bucket)\n",
    "    print(f'TRs without tokens: {empty_tr}/{len(tr_buckets)}')\n",
    "\n",
    "    if category_columns:\n",
    "        tr_values_raw = aggregate_seconds_to_edges(canonical_edges, canonical_values_raw, tr_edges)\n",
    "        tr_values_smoothed = aggregate_seconds_to_edges(canonical_edges, canonical_values_smoothed, tr_edges)\n",
    "    else:\n",
    "        tr_values_raw = np.empty((len(tr_edges) - 1, 0), dtype=float)\n",
    "        tr_values_smoothed = tr_values_raw\n",
    "\n",
    "    base_index = np.arange(len(tr_edges) - 1, dtype=int)\n",
    "    base_df = pd.DataFrame({'tr_index': base_index, 'start_sec': tr_edges[:-1], 'end_sec': tr_edges[1:]})\n",
    "    category_df_raw = base_df.copy()\n",
    "    category_df_smoothed = base_df.copy()\n",
    "    if category_columns:\n",
    "        category_df_raw.loc[:, category_columns] = tr_values_raw\n",
    "        category_df_smoothed.loc[:, category_columns] = tr_values_smoothed\n",
    "    category_df = category_df_smoothed if smoothing_applied else category_df_raw\n",
    "    print(category_df.head())\n",
    "\n",
    "    if category_score_method == 'similarity' and category_columns:\n",
    "        finite_vals = category_df[category_columns].to_numpy(dtype=float)\n",
    "        finite_vals = finite_vals[np.isfinite(finite_vals)]\n",
    "        if finite_vals.size:\n",
    "            assert np.nanmin(finite_vals) >= -1.0001 and np.nanmax(finite_vals) <= 1.0001, 'Similarity scores out of bounds.'\n",
    "    else:\n",
    "        if category_columns:\n",
    "            assert (category_df[category_columns].fillna(0.0) >= -1e-9).all().all(), 'Count scores must be non-negative.'\n",
    "\n",
    "    if save_outputs:\n",
    "        output_root.mkdir(parents=True, exist_ok=True)\n",
    "        category_csv_path = output_root / 'category_timeseries.csv'\n",
    "        category_df.to_csv(category_csv_path, index=False)\n",
    "        if smoothing_applied:\n",
    "            category_df_raw.to_csv(output_root / 'category_timeseries_raw.csv', index=False)\n",
    "        definition_path = output_root / 'category_definition.json'\n",
    "        with definition_path.open('w') as fh:\n",
    "            json.dump(ensure_serializable(category_definitions), fh, indent=2)\n",
    "        print(f'Saved category time series to {category_csv_path}')\n",
    "\n",
    "    trimmed_path = Path(paths.get('figs', 'figs')) / subject / story / 'day16_decoding' / 'semantic_pcs_trimmed.csv'\n",
    "    max_lag_primary = 0\n",
    "    trimmed_df = None\n",
    "    if trimmed_path.exists():\n",
    "        day16_trim = pd.read_csv(trimmed_path)\n",
    "        expected_len = len(day16_trim)\n",
    "        if len(day16_trim) > len(category_df):\n",
    "            raise ValueError('Day16 trimmed series longer than category series; regenerate Day16 or rerun Day17.')\n",
    "        max_lag_primary = max(0, len(category_df) - expected_len)\n",
    "        trimmed_df = category_df.iloc[max_lag_primary:].reset_index(drop=True)\n",
    "        if save_outputs:\n",
    "            trimmed_out = trimmed_df.copy()\n",
    "            trimmed_out.insert(0, 'trim_index', np.arange(len(trimmed_out), dtype=int))\n",
    "            trimmed_out.drop(columns=['tr_index'], inplace=True, errors='ignore')\n",
    "            trimmed_out.to_csv(output_root / 'category_timeseries_trimmed.csv', index=False)\n",
    "            print(f'Saved trimmed category series to {output_root / \"category_timeseries_trimmed.csv\"}')\n",
    "    else:\n",
    "        warnings.warn('Day16 trimmed PCs not found; skipping auto-alignment.')\n",
    "\n",
    "    smoothing_meta = {\n",
    "        'applied': bool(smoothing_applied),\n",
    "        'seconds': smoothing_seconds,\n",
    "        'method': smoothing_method,\n",
    "        'gaussian_sigma_seconds': float(gaussian_sigma_seconds) if gaussian_sigma_seconds is not None else None,\n",
    "        'kernel_size': int(smoothing_kernel.size),\n",
    "        'pad_mode': smoothing_pad,\n",
    "        'bin_width_seconds': seconds_bin_width,\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'subject': subject,\n",
    "        'story': story,\n",
    "        'temporal_weighting': tw_mode,\n",
    "        'category_columns': category_columns,\n",
    "        'category_states': category_states,\n",
    "        'category_definitions': category_definitions,\n",
    "        'category_score_method': category_score_method,\n",
    "        'event_records': event_records,\n",
    "        'canonical_buckets': canonical_buckets,\n",
    "        'tr_buckets': tr_buckets,\n",
    "        'canonical_df_raw': canonical_df_raw,\n",
    "        'canonical_df_smoothed': canonical_df_smoothed,\n",
    "        'canonical_df_selected': canonical_df_selected,\n",
    "        'category_df_raw': category_df_raw,\n",
    "        'category_df_smoothed': category_df_smoothed,\n",
    "        'category_df_selected': category_df,\n",
    "        'canonical_edges': canonical_edges,\n",
    "        'tr_edges': tr_edges,\n",
    "        'smoothing': smoothing_meta,\n",
    "        'output_root': output_root,\n",
    "        'canonical_root': canonical_root,\n",
    "        'trimmed_df': trimmed_df,\n",
    "        'max_lag_primary': max_lag_primary,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee422a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Day19 category build for UTS01 / wheretheressmoke ===\n",
      "Loaded 2308 transcript events.\n",
      "TR edges: 302 (n_tr=301) spanning 602.00 seconds.\n",
      "Loaded English1000 embeddings from /bucket/PaoU/seann/openneuro/ds003020/derivative/english1000sm.hf5 (vocab=10470).\n",
      "Word2Vec fallback disabled.\n",
      "Loaded 12 CSV-driven categories from configs/cluster_words.csv: ['abstract', 'communal', 'emotional', 'locational', 'mental', 'numeric', 'professional', 'social', 'tactile', 'temporal', 'violent', 'visual']\n",
      "Tokens with embeddings: 1835/2308 (OOV rate=20.49%).\n",
      "Canonical bins without tokens: 0/12039\n",
      "Saved canonical story series to featuresqaemb/stories/wheretheressmoke/category_timeseries_seconds.csv\n",
      "TRs without tokens: 0/301\n",
      "   tr_index  start_sec  end_sec  cat_abstract  cat_communal  cat_emotional  \\\n",
      "0         0        0.0      2.0     -0.234710     -0.221588      -0.162214   \n",
      "1         1        2.0      4.0     -0.118607     -0.186303       0.095760   \n",
      "2         2        4.0      6.0     -0.212263     -0.350677      -0.237075   \n",
      "3         3        6.0      8.0     -0.088565     -0.428633      -0.301686   \n",
      "4         4        8.0     10.0     -0.394030     -0.368272      -0.342657   \n",
      "\n",
      "   cat_locational  cat_mental  cat_numeric  cat_professional  cat_social  \\\n",
      "0       -0.153373   -0.024500     0.133099         -0.027683   -0.016114   \n",
      "1       -0.297710    0.190081    -0.087680         -0.039687    0.105637   \n",
      "2       -0.210927   -0.067724     0.175485         -0.137288   -0.094446   \n",
      "3       -0.107320   -0.111763     0.217664         -0.226084   -0.266789   \n",
      "4       -0.076140   -0.030331     0.223652          0.057729   -0.032521   \n",
      "\n",
      "   cat_tactile  cat_temporal  cat_violent  cat_visual  \n",
      "0     0.052948      0.200849     0.077543    0.088356  \n",
      "1    -0.004579      0.141428     0.253060   -0.063777  \n",
      "2     0.186172      0.192468     0.178671    0.208267  \n",
      "3     0.355990      0.149258     0.138575    0.313628  \n",
      "4     0.109272      0.376442     0.144865    0.141703  \n",
      "Saved category time series to featuresqaemb/subjects/UTS01/wheretheressmoke/category_timeseries.csv\n",
      "\n",
      "Smoothing configuration: {'applied': True, 'seconds': 1.0, 'method': 'moving_average', 'gaussian_sigma_seconds': 0.5, 'kernel_size': 21, 'pad_mode': 'reflect', 'bin_width_seconds': 0.05}\n",
      "\n",
      "Canonical preview:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_3954650/3250063342.py:314: UserWarning: Day16 trimmed PCs not found; skipping auto-alignment.\n",
      "  warnings.warn('Day16 trimmed PCs not found; skipping auto-alignment.')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin_index</th>\n",
       "      <th>start_sec</th>\n",
       "      <th>end_sec</th>\n",
       "      <th>cat_abstract</th>\n",
       "      <th>cat_communal</th>\n",
       "      <th>cat_emotional</th>\n",
       "      <th>cat_locational</th>\n",
       "      <th>cat_mental</th>\n",
       "      <th>cat_numeric</th>\n",
       "      <th>cat_professional</th>\n",
       "      <th>cat_social</th>\n",
       "      <th>cat_tactile</th>\n",
       "      <th>cat_temporal</th>\n",
       "      <th>cat_violent</th>\n",
       "      <th>cat_visual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.216557</td>\n",
       "      <td>-0.250232</td>\n",
       "      <td>-0.327978</td>\n",
       "      <td>0.025273</td>\n",
       "      <td>-0.158207</td>\n",
       "      <td>0.312430</td>\n",
       "      <td>-0.049614</td>\n",
       "      <td>-0.146466</td>\n",
       "      <td>0.182867</td>\n",
       "      <td>0.235947</td>\n",
       "      <td>0.005606</td>\n",
       "      <td>0.152835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.216704</td>\n",
       "      <td>-0.250805</td>\n",
       "      <td>-0.327851</td>\n",
       "      <td>0.024511</td>\n",
       "      <td>-0.157807</td>\n",
       "      <td>0.312208</td>\n",
       "      <td>-0.049840</td>\n",
       "      <td>-0.146550</td>\n",
       "      <td>0.182932</td>\n",
       "      <td>0.236174</td>\n",
       "      <td>0.006072</td>\n",
       "      <td>0.152925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.220967</td>\n",
       "      <td>-0.267412</td>\n",
       "      <td>-0.324147</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>-0.146226</td>\n",
       "      <td>0.305769</td>\n",
       "      <td>-0.056401</td>\n",
       "      <td>-0.148991</td>\n",
       "      <td>0.184803</td>\n",
       "      <td>0.242755</td>\n",
       "      <td>0.019557</td>\n",
       "      <td>0.155537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.225229</td>\n",
       "      <td>-0.284018</td>\n",
       "      <td>-0.320444</td>\n",
       "      <td>-0.019644</td>\n",
       "      <td>-0.134644</td>\n",
       "      <td>0.299330</td>\n",
       "      <td>-0.062962</td>\n",
       "      <td>-0.151432</td>\n",
       "      <td>0.186674</td>\n",
       "      <td>0.249335</td>\n",
       "      <td>0.033043</td>\n",
       "      <td>0.158150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.229491</td>\n",
       "      <td>-0.300625</td>\n",
       "      <td>-0.316741</td>\n",
       "      <td>-0.041722</td>\n",
       "      <td>-0.123062</td>\n",
       "      <td>0.292892</td>\n",
       "      <td>-0.069524</td>\n",
       "      <td>-0.153874</td>\n",
       "      <td>0.188545</td>\n",
       "      <td>0.255916</td>\n",
       "      <td>0.046529</td>\n",
       "      <td>0.160762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bin_index  start_sec  end_sec  cat_abstract  cat_communal  cat_emotional  \\\n",
       "0          0       0.00     0.05     -0.216557     -0.250232      -0.327978   \n",
       "1          1       0.05     0.10     -0.216704     -0.250805      -0.327851   \n",
       "2          2       0.10     0.15     -0.220967     -0.267412      -0.324147   \n",
       "3          3       0.15     0.20     -0.225229     -0.284018      -0.320444   \n",
       "4          4       0.20     0.25     -0.229491     -0.300625      -0.316741   \n",
       "\n",
       "   cat_locational  cat_mental  cat_numeric  cat_professional  cat_social  \\\n",
       "0        0.025273   -0.158207     0.312430         -0.049614   -0.146466   \n",
       "1        0.024511   -0.157807     0.312208         -0.049840   -0.146550   \n",
       "2        0.002433   -0.146226     0.305769         -0.056401   -0.148991   \n",
       "3       -0.019644   -0.134644     0.299330         -0.062962   -0.151432   \n",
       "4       -0.041722   -0.123062     0.292892         -0.069524   -0.153874   \n",
       "\n",
       "   cat_tactile  cat_temporal  cat_violent  cat_visual  \n",
       "0     0.182867      0.235947     0.005606    0.152835  \n",
       "1     0.182932      0.236174     0.006072    0.152925  \n",
       "2     0.184803      0.242755     0.019557    0.155537  \n",
       "3     0.186674      0.249335     0.033043    0.158150  \n",
       "4     0.188545      0.255916     0.046529    0.160762  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TR-aligned preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tr_index</th>\n",
       "      <th>start_sec</th>\n",
       "      <th>end_sec</th>\n",
       "      <th>cat_abstract</th>\n",
       "      <th>cat_communal</th>\n",
       "      <th>cat_emotional</th>\n",
       "      <th>cat_locational</th>\n",
       "      <th>cat_mental</th>\n",
       "      <th>cat_numeric</th>\n",
       "      <th>cat_professional</th>\n",
       "      <th>cat_social</th>\n",
       "      <th>cat_tactile</th>\n",
       "      <th>cat_temporal</th>\n",
       "      <th>cat_violent</th>\n",
       "      <th>cat_visual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.234710</td>\n",
       "      <td>-0.221588</td>\n",
       "      <td>-0.162214</td>\n",
       "      <td>-0.153373</td>\n",
       "      <td>-0.024500</td>\n",
       "      <td>0.133099</td>\n",
       "      <td>-0.027683</td>\n",
       "      <td>-0.016114</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.200849</td>\n",
       "      <td>0.077543</td>\n",
       "      <td>0.088356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.118607</td>\n",
       "      <td>-0.186303</td>\n",
       "      <td>0.095760</td>\n",
       "      <td>-0.297710</td>\n",
       "      <td>0.190081</td>\n",
       "      <td>-0.087680</td>\n",
       "      <td>-0.039687</td>\n",
       "      <td>0.105637</td>\n",
       "      <td>-0.004579</td>\n",
       "      <td>0.141428</td>\n",
       "      <td>0.253060</td>\n",
       "      <td>-0.063777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.212263</td>\n",
       "      <td>-0.350677</td>\n",
       "      <td>-0.237075</td>\n",
       "      <td>-0.210927</td>\n",
       "      <td>-0.067724</td>\n",
       "      <td>0.175485</td>\n",
       "      <td>-0.137288</td>\n",
       "      <td>-0.094446</td>\n",
       "      <td>0.186172</td>\n",
       "      <td>0.192468</td>\n",
       "      <td>0.178671</td>\n",
       "      <td>0.208267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-0.088565</td>\n",
       "      <td>-0.428633</td>\n",
       "      <td>-0.301686</td>\n",
       "      <td>-0.107320</td>\n",
       "      <td>-0.111763</td>\n",
       "      <td>0.217664</td>\n",
       "      <td>-0.226084</td>\n",
       "      <td>-0.266789</td>\n",
       "      <td>0.355990</td>\n",
       "      <td>0.149258</td>\n",
       "      <td>0.138575</td>\n",
       "      <td>0.313628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.394030</td>\n",
       "      <td>-0.368272</td>\n",
       "      <td>-0.342657</td>\n",
       "      <td>-0.076140</td>\n",
       "      <td>-0.030331</td>\n",
       "      <td>0.223652</td>\n",
       "      <td>0.057729</td>\n",
       "      <td>-0.032521</td>\n",
       "      <td>0.109272</td>\n",
       "      <td>0.376442</td>\n",
       "      <td>0.144865</td>\n",
       "      <td>0.141703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tr_index  start_sec  end_sec  cat_abstract  cat_communal  cat_emotional  \\\n",
       "0         0        0.0      2.0     -0.234710     -0.221588      -0.162214   \n",
       "1         1        2.0      4.0     -0.118607     -0.186303       0.095760   \n",
       "2         2        4.0      6.0     -0.212263     -0.350677      -0.237075   \n",
       "3         3        6.0      8.0     -0.088565     -0.428633      -0.301686   \n",
       "4         4        8.0     10.0     -0.394030     -0.368272      -0.342657   \n",
       "\n",
       "   cat_locational  cat_mental  cat_numeric  cat_professional  cat_social  \\\n",
       "0       -0.153373   -0.024500     0.133099         -0.027683   -0.016114   \n",
       "1       -0.297710    0.190081    -0.087680         -0.039687    0.105637   \n",
       "2       -0.210927   -0.067724     0.175485         -0.137288   -0.094446   \n",
       "3       -0.107320   -0.111763     0.217664         -0.226084   -0.266789   \n",
       "4       -0.076140   -0.030331     0.223652          0.057729   -0.032521   \n",
       "\n",
       "   cat_tactile  cat_temporal  cat_violent  cat_visual  \n",
       "0     0.052948      0.200849     0.077543    0.088356  \n",
       "1    -0.004579      0.141428     0.253060   -0.063777  \n",
       "2     0.186172      0.192468     0.178671    0.208267  \n",
       "3     0.355990      0.149258     0.138575    0.313628  \n",
       "4     0.109272      0.376442     0.144865    0.141703  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = generate_category_time_series(\n",
    "    SUBJECT,\n",
    "    STORY,\n",
    "    cfg_base=cfg,\n",
    "    categories_cfg_base=categories_cfg,\n",
    "    cluster_csv_path=cluster_csv_path,\n",
    "    temporal_weighting=TEMPORAL_WEIGHTING,\n",
    "    prototype_weight_power=prototype_weight_power,\n",
    "    smoothing_seconds=SMOOTHING_SECONDS,\n",
    "    smoothing_method=SMOOTHING_METHOD,\n",
    "    gaussian_sigma_seconds=GAUSSIAN_SIGMA_SECONDS,\n",
    "    smoothing_pad=SMOOTHING_PAD_MODE,\n",
    "    seconds_bin_width=SECONDS_BIN_WIDTH,\n",
    "    save_outputs=SAVE_OUTPUTS,\n",
    ")\n",
    "\n",
    "canonical_df = result['canonical_df_selected']\n",
    "tr_df = result['category_df_selected']\n",
    "print()\n",
    "print('Smoothing configuration:', result['smoothing'])\n",
    "print()\n",
    "print('Canonical preview:')\n",
    "display(canonical_df.head())\n",
    "print()\n",
    "print('TR-aligned preview:')\n",
    "display(tr_df.head())\n",
    "if result['trimmed_df'] is not None:\n",
    "    print()\n",
    "    print(f\"Trimmed window length: {len(result['trimmed_df'])} (max_lag_primary={result['max_lag_primary']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fec8db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare transcript tokens and per-category scores\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if 'result' not in globals():\n",
    "    raise RuntimeError('Run the generation cell first to populate `result`.')\n",
    "\n",
    "_tokens_raw = result.get('event_records') or []\n",
    "if not _tokens_raw:\n",
    "    raise RuntimeError('No transcript events found - rerun upstream steps.')\n",
    "\n",
    "_category_states = result.get('category_states') or {}\n",
    "if not _category_states:\n",
    "    raise RuntimeError('Category states missing from result; rerun the generation cell.')\n",
    "\n",
    "_token_df = pd.DataFrame(_tokens_raw)\n",
    "_token_df = _token_df[['word', 'start', 'end', 'embedding', 'embedding_norm']].copy()\n",
    "_token_df['midpoint'] = 0.5 * (_token_df['start'] + _token_df['end'])\n",
    "_token_df['duration'] = _token_df['end'] - _token_df['start']\n",
    "_token_df['token_index'] = np.arange(len(_token_df))\n",
    "\n",
    "_score_method = str(result.get('category_score_method', 'similarity')).lower()\n",
    "_token_scores = {}\n",
    "_abs_max = 0.0\n",
    "for cat_name, state in _category_states.items():\n",
    "    scores = []\n",
    "    proto = state.get('prototype')\n",
    "    proto_norm = state.get('prototype_norm') or 0.0\n",
    "    lexicon = state.get('lexicon', {}) or {}\n",
    "    for rec in _tokens_raw:\n",
    "        word = rec['word']\n",
    "        if _score_method == 'count':\n",
    "            score = lexicon.get(word.lower(), np.nan)\n",
    "        else:\n",
    "            emb = rec.get('embedding')\n",
    "            emb_norm = rec.get('embedding_norm') or 0.0\n",
    "            if emb is None or proto is None or proto_norm <= 0 or emb_norm <= 0:\n",
    "                score = np.nan\n",
    "            else:\n",
    "                score = float(np.clip(np.dot(emb, proto) / (emb_norm * proto_norm), -1.0, 1.0))\n",
    "        scores.append(score)\n",
    "    arr = np.array(scores, dtype=float)\n",
    "    _token_scores[cat_name] = arr\n",
    "    finite = np.abs(arr[np.isfinite(arr)])\n",
    "    if finite.size:\n",
    "        _abs_max = max(_abs_max, float(finite.max()))\n",
    "\n",
    "TOKEN_BASE_DF = _token_df[['token_index', 'word', 'start', 'end', 'midpoint', 'duration']].copy()\n",
    "TOKEN_SCORE_CACHE = _token_scores\n",
    "TOKEN_SCORE_METHOD = _score_method\n",
    "TOKEN_SCORE_ABS_MAX = _abs_max if _abs_max > 0 else 1.0\n",
    "\n",
    "# Drop heavy objects from the temporary frame to free memory\n",
    "del _token_df\n",
    "del _tokens_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2b22d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected categories: ['cat_abstract', 'cat_communal', 'cat_emotional', 'cat_locational', 'cat_mental', 'cat_numeric', 'cat_professional', 'cat_social', 'cat_tactile', 'cat_temporal', 'cat_violent', 'cat_visual']\n"
     ]
    }
   ],
   "source": [
    "# Pick 12 categories for the 4x3 grid (edit as needed)\n",
    "if KARAOKE_CATEGORY_COLUMNS is None:\n",
    "    category_cols_12 = result['category_columns'][:KARAOKE_CATEGORY_COUNT]\n",
    "else:\n",
    "    category_cols_12 = list(KARAOKE_CATEGORY_COLUMNS)\n",
    "\n",
    "if len(category_cols_12) != 12:\n",
    "    raise ValueError(f'Expected 12 categories, got {len(category_cols_12)}.')\n",
    "\n",
    "print('Selected categories:', category_cols_12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18dd3bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.patches as mpatches\n",
    "import time\n",
    "\n",
    "if plt is None:\n",
    "    raise RuntimeError('Matplotlib unavailable in this environment.')\n",
    "\n",
    "\n",
    "def make_karaoke_category_video(\n",
    "    *,\n",
    "    result: dict,\n",
    "    token_df: pd.DataFrame,\n",
    "    category_cols: list,\n",
    "    out_mp4: str = \"karaoke_categories.mp4\",\n",
    "    use_domain: str = \"tr\",          # \"tr\" or \"canonical\"\n",
    "    fps: int = 30,\n",
    "    window_sec: float = 30.0,        # how many seconds visible at once\n",
    "    pad_left_sec: float = 0.5,\n",
    "    pad_right_sec: float = 2.0,\n",
    "    log_every_frames: int | None = None,\n",
    "    playback_speed: float = 1.0,\n",
    "    ypad_frac: float = 0.05,\n",
    "    bin_words_max: int | None = 30,\n",
    "    zscore: bool = True,\n",
    "):\n",
    "    '''\n",
    "    Creates an MP4 where the top shows words within the active bin and bottom is a 4x3 grid.\n",
    "    Assumes token_df has columns: word,start,end,midpoint (like your TOKEN_BASE_DF).\n",
    "    '''\n",
    "\n",
    "    assert use_domain in {\"tr\", \"canonical\"}\n",
    "    assert len(category_cols) == 12, \"Pass exactly 12 categories for a 4x3 grid.\"\n",
    "\n",
    "    if playback_speed <= 0:\n",
    "        raise ValueError(\"playback_speed must be > 0.\")\n",
    "    if ypad_frac < 0:\n",
    "        raise ValueError(\"ypad_frac must be >= 0.\")\n",
    "    if bin_words_max is not None and bin_words_max <= 0:\n",
    "        raise ValueError(\"bin_words_max must be > 0 or None.\")\n",
    "\n",
    "    if not isinstance(zscore, bool):\n",
    "        raise ValueError(\"zscore must be a boolean.\")\n",
    "\n",
    "    if log_every_frames is None:\n",
    "        log_every_frames = max(1, int(fps * 5))\n",
    "\n",
    "    # --- Choose time base and series ---\n",
    "    if use_domain == \"canonical\":\n",
    "        df = result[\"canonical_df_selected\"]\n",
    "        t = 0.5 * (result[\"canonical_edges\"][:-1] + result[\"canonical_edges\"][1:])\n",
    "        edges = result[\"canonical_edges\"]\n",
    "    else:\n",
    "        df = result[\"category_df_selected\"]\n",
    "        t = result[\"tr_edges\"][:-1]\n",
    "        edges = result[\"tr_edges\"]\n",
    "\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    edges = np.asarray(edges, dtype=float)\n",
    "\n",
    "    # Determine video duration from transcript\n",
    "    t_end = float(token_df[\"end\"].max())\n",
    "    transcript_duration = t_end + pad_right_sec\n",
    "    duration = transcript_duration / playback_speed\n",
    "    n_frames = int(np.ceil(duration * fps))\n",
    "\n",
    "    # Pre-extract series arrays for speed\n",
    "    Y = np.vstack([df[c].to_numpy(dtype=float) for c in category_cols])  # shape (12, T)\n",
    "\n",
    "    if zscore:\n",
    "        means = np.nanmean(Y, axis=1, keepdims=True)\n",
    "        stds = np.nanstd(Y, axis=1, keepdims=True)\n",
    "        stds = np.where(np.isfinite(stds) & (stds > 0), stds, 1.0)\n",
    "        Y = (Y - means) / stds\n",
    "\n",
    "    # Robust y-lims per subplot (full range with padding)\n",
    "    ylims = []\n",
    "    for i in range(12):\n",
    "        vals = Y[i]\n",
    "        finite = vals[np.isfinite(vals)]\n",
    "        if finite.size == 0:\n",
    "            ylims.append((-1, 1))\n",
    "        else:\n",
    "            lo = float(np.min(finite))\n",
    "            hi = float(np.max(finite))\n",
    "            if np.isclose(lo, hi):\n",
    "                pad = 1.0 if lo == 0 else abs(lo) * 0.1\n",
    "            else:\n",
    "                pad = (hi - lo) * ypad_frac\n",
    "            ylims.append((lo - pad, hi + pad))\n",
    "\n",
    "    # --- Figure layout: top karaoke + 4x3 plots ---\n",
    "    fig = plt.figure(figsize=(16, 9), dpi=150)\n",
    "    gs = fig.add_gridspec(5, 3, height_ratios=[0.65, 1, 1, 1, 1], hspace=0.35, wspace=0.25)\n",
    "\n",
    "    ax_text = fig.add_subplot(gs[0, :])\n",
    "    ax_text.axis(\"off\")\n",
    "\n",
    "    axes = []\n",
    "    for r in range(1, 5):\n",
    "        for c in range(3):\n",
    "            axes.append(fig.add_subplot(gs[r, c]))\n",
    "\n",
    "    # Initialize lines\n",
    "    lines = []\n",
    "    cursors = []\n",
    "    highlights = []\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.set_title(category_cols[i].replace(\"cat_\", \"\"), fontsize=10)\n",
    "        ax.set_xlim(0, window_sec)\n",
    "        ax.set_ylim(*ylims[i])\n",
    "        ax.grid(True, alpha=0.25)\n",
    "        (ln,) = ax.plot([], [], linewidth=1.6)\n",
    "        cursor = ax.axvline(0, linewidth=1.2, alpha=0.9)\n",
    "        highlight = mpatches.Rectangle(\n",
    "            (0, 0), 0, 1,\n",
    "            transform=ax.get_xaxis_transform(),\n",
    "            facecolor=\"#f4c542\",\n",
    "            alpha=0.25,\n",
    "            zorder=0,\n",
    "        )\n",
    "        ax.add_patch(highlight)\n",
    "        lines.append(ln)\n",
    "        cursors.append(cursor)\n",
    "        highlights.append(highlight)\n",
    "\n",
    "    # Text artists\n",
    "    txt_bin = ax_text.text(0.01, 0.72, \"\", fontsize=16, fontweight=\"bold\", va=\"center\", ha=\"left\")\n",
    "    txt_words = ax_text.text(0.01, 0.36, \"\", fontsize=14, va=\"center\", ha=\"left\")\n",
    "    txt_sub  = ax_text.text(0.01, 0.10, \"\", fontsize=11, va=\"center\", ha=\"left\", alpha=0.8)\n",
    "\n",
    "    words = token_df[\"word\"].astype(str).tolist()\n",
    "    starts = token_df[\"start\"].to_numpy(dtype=float)\n",
    "    ends = token_df[\"end\"].to_numpy(dtype=float)\n",
    "\n",
    "    start_time = None\n",
    "\n",
    "    def _bin_index(t_now: float) -> int:\n",
    "        idx = int(np.searchsorted(edges, t_now, side=\"right\") - 1)\n",
    "        return int(np.clip(idx, 0, len(edges) - 2))\n",
    "\n",
    "    def _format_bin_words(bin_words: list[str]) -> str:\n",
    "        if not bin_words:\n",
    "            return \"(no words in bin)\"\n",
    "        if bin_words_max is not None and len(bin_words) > bin_words_max:\n",
    "            shown = bin_words[:bin_words_max]\n",
    "            return \" \".join(shown) + \" ...\"\n",
    "        return \" \".join(bin_words)\n",
    "\n",
    "    def init():\n",
    "        for ln in lines:\n",
    "            ln.set_data([], [])\n",
    "        txt_bin.set_text(\"\")\n",
    "        txt_words.set_text(\"\")\n",
    "        txt_sub.set_text(\"\")\n",
    "        return lines + cursors + highlights + [txt_bin, txt_words, txt_sub]\n",
    "\n",
    "    def update(frame):\n",
    "        nonlocal start_time\n",
    "        if start_time is None:\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "        t_now = (frame / fps) * playback_speed\n",
    "\n",
    "        # sliding window\n",
    "        w0 = max(0.0, t_now - window_sec + pad_left_sec)\n",
    "        w1 = w0 + window_sec\n",
    "\n",
    "        # slice series in the window (by time array)\n",
    "        mask = (t >= w0) & (t <= w1)\n",
    "        if not np.any(mask):\n",
    "            return lines + cursors + highlights + [txt_bin, txt_words, txt_sub]\n",
    "\n",
    "        tw = t[mask] - w0  # shift to window coords\n",
    "\n",
    "        bin_idx = _bin_index(t_now)\n",
    "        bin_start = float(edges[bin_idx])\n",
    "        bin_end = float(edges[bin_idx + 1])\n",
    "\n",
    "        view_start = max(bin_start, w0)\n",
    "        view_end = min(bin_end, w1)\n",
    "        span_start = view_start - w0\n",
    "        span_width = max(0.0, view_end - view_start)\n",
    "\n",
    "        for i in range(12):\n",
    "            yw = Y[i, mask]\n",
    "            lines[i].set_data(tw, yw)\n",
    "            cursors[i].set_xdata([t_now - w0, t_now - w0])\n",
    "            axes[i].set_xlim(0, window_sec)\n",
    "            highlights[i].set_x(span_start)\n",
    "            highlights[i].set_width(span_width)\n",
    "\n",
    "        bin_mask = (starts < bin_end) & (ends > bin_start)\n",
    "        bin_words = [words[i] for i in np.where(bin_mask)[0]]\n",
    "        words_text = _format_bin_words(bin_words)\n",
    "\n",
    "        txt_bin.set_text(\n",
    "            f\"Bin {bin_start:6.2f}-{bin_end:6.2f}s | {len(bin_words)} words\"\n",
    "        )\n",
    "        txt_words.set_text(words_text)\n",
    "        txt_sub.set_text(\n",
    "            f\"t = {t_now:6.2f}s | bin {bin_idx + 1}/{len(edges) - 1} | domain: {use_domain}\"\n",
    "        )\n",
    "\n",
    "        if frame % log_every_frames == 0 or frame == n_frames - 1:\n",
    "            elapsed = time.perf_counter() - start_time\n",
    "            progress = (frame + 1) / n_frames\n",
    "            eta = elapsed / progress - elapsed if progress > 0 else float('inf')\n",
    "            print(\n",
    "                f\"Frame {frame + 1}/{n_frames} \"\n",
    "                f\"({progress * 100:.1f}%) | \"\n",
    "                f\"elapsed {elapsed / 60:.1f}m | ETA {eta / 60:.1f}m\"\n",
    "            )\n",
    "\n",
    "        return lines + cursors + highlights + [txt_bin, txt_words, txt_sub]\n",
    "\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update, init_func=init,\n",
    "        frames=n_frames, interval=1000/fps, blit=False\n",
    "    )\n",
    "\n",
    "    writer = animation.FFMpegWriter(\n",
    "        fps=fps,\n",
    "        codec=\"libx264\",\n",
    "        bitrate=5000,\n",
    "        extra_args=[\n",
    "            \"-pix_fmt\", \"yuv420p\",\n",
    "            \"-profile:v\", \"baseline\",\n",
    "            \"-level\", \"3.0\",\n",
    "            \"-movflags\", \"+faststart\"\n",
    "        ]\n",
    "    )\n",
    "    anim.save(out_mp4, writer=writer)\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved: {out_mp4}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c7ec95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 1/302 (0.3%) | elapsed 0.0m | ETA 0.0m\n",
      "Frame 6/302 (2.0%) | elapsed 0.1m | ETA 2.7m\n",
      "Frame 11/302 (3.6%) | elapsed 0.1m | ETA 2.5m\n",
      "Frame 16/302 (5.3%) | elapsed 0.1m | ETA 2.4m\n",
      "Frame 21/302 (7.0%) | elapsed 0.2m | ETA 2.5m\n",
      "Frame 26/302 (8.6%) | elapsed 0.2m | ETA 2.4m\n",
      "Frame 31/302 (10.3%) | elapsed 0.3m | ETA 2.5m\n",
      "Frame 36/302 (11.9%) | elapsed 0.3m | ETA 2.4m\n",
      "Frame 41/302 (13.6%) | elapsed 0.4m | ETA 2.3m\n",
      "Frame 46/302 (15.2%) | elapsed 0.4m | ETA 2.3m\n",
      "Frame 51/302 (16.9%) | elapsed 0.5m | ETA 2.2m\n",
      "Frame 56/302 (18.5%) | elapsed 0.5m | ETA 2.2m\n",
      "Frame 61/302 (20.2%) | elapsed 0.5m | ETA 2.2m\n",
      "Frame 66/302 (21.9%) | elapsed 0.6m | ETA 2.1m\n",
      "Frame 71/302 (23.5%) | elapsed 0.6m | ETA 2.1m\n",
      "Frame 76/302 (25.2%) | elapsed 0.7m | ETA 2.0m\n",
      "Frame 81/302 (26.8%) | elapsed 0.7m | ETA 2.0m\n",
      "Frame 86/302 (28.5%) | elapsed 0.8m | ETA 1.9m\n",
      "Frame 91/302 (30.1%) | elapsed 0.8m | ETA 1.9m\n",
      "Frame 96/302 (31.8%) | elapsed 0.9m | ETA 1.9m\n",
      "Frame 101/302 (33.4%) | elapsed 0.9m | ETA 1.8m\n",
      "Frame 106/302 (35.1%) | elapsed 1.0m | ETA 1.8m\n",
      "Frame 111/302 (36.8%) | elapsed 1.0m | ETA 1.7m\n",
      "Frame 116/302 (38.4%) | elapsed 1.1m | ETA 1.7m\n",
      "Frame 121/302 (40.1%) | elapsed 1.1m | ETA 1.6m\n",
      "Frame 126/302 (41.7%) | elapsed 1.1m | ETA 1.6m\n",
      "Frame 131/302 (43.4%) | elapsed 1.2m | ETA 1.6m\n",
      "Frame 136/302 (45.0%) | elapsed 1.2m | ETA 1.5m\n",
      "Frame 141/302 (46.7%) | elapsed 1.3m | ETA 1.5m\n",
      "Frame 146/302 (48.3%) | elapsed 1.3m | ETA 1.4m\n",
      "Frame 151/302 (50.0%) | elapsed 1.4m | ETA 1.4m\n",
      "Frame 156/302 (51.7%) | elapsed 1.4m | ETA 1.3m\n",
      "Frame 161/302 (53.3%) | elapsed 1.5m | ETA 1.3m\n",
      "Frame 166/302 (55.0%) | elapsed 1.5m | ETA 1.2m\n",
      "Frame 171/302 (56.6%) | elapsed 1.5m | ETA 1.2m\n",
      "Frame 176/302 (58.3%) | elapsed 1.6m | ETA 1.1m\n",
      "Frame 181/302 (59.9%) | elapsed 1.6m | ETA 1.1m\n",
      "Frame 186/302 (61.6%) | elapsed 1.7m | ETA 1.0m\n",
      "Frame 191/302 (63.2%) | elapsed 1.7m | ETA 1.0m\n",
      "Frame 196/302 (64.9%) | elapsed 1.8m | ETA 1.0m\n",
      "Frame 201/302 (66.6%) | elapsed 1.8m | ETA 0.9m\n",
      "Frame 206/302 (68.2%) | elapsed 1.8m | ETA 0.9m\n",
      "Frame 211/302 (69.9%) | elapsed 1.9m | ETA 0.8m\n",
      "Frame 216/302 (71.5%) | elapsed 1.9m | ETA 0.8m\n",
      "Frame 221/302 (73.2%) | elapsed 2.0m | ETA 0.7m\n",
      "Frame 226/302 (74.8%) | elapsed 2.0m | ETA 0.7m\n",
      "Frame 231/302 (76.5%) | elapsed 2.1m | ETA 0.6m\n",
      "Frame 236/302 (78.1%) | elapsed 2.1m | ETA 0.6m\n",
      "Frame 241/302 (79.8%) | elapsed 2.1m | ETA 0.5m\n",
      "Frame 246/302 (81.5%) | elapsed 2.2m | ETA 0.5m\n",
      "Frame 251/302 (83.1%) | elapsed 2.2m | ETA 0.5m\n",
      "Frame 256/302 (84.8%) | elapsed 2.3m | ETA 0.4m\n",
      "Frame 261/302 (86.4%) | elapsed 2.3m | ETA 0.4m\n",
      "Frame 266/302 (88.1%) | elapsed 2.4m | ETA 0.3m\n",
      "Frame 271/302 (89.7%) | elapsed 2.4m | ETA 0.3m\n",
      "Frame 276/302 (91.4%) | elapsed 2.5m | ETA 0.2m\n",
      "Frame 281/302 (93.0%) | elapsed 2.5m | ETA 0.2m\n",
      "Frame 286/302 (94.7%) | elapsed 2.5m | ETA 0.1m\n",
      "Frame 291/302 (96.4%) | elapsed 2.6m | ETA 0.1m\n",
      "Frame 296/302 (98.0%) | elapsed 2.6m | ETA 0.1m\n",
      "Frame 301/302 (99.7%) | elapsed 2.7m | ETA 0.0m\n",
      "Frame 302/302 (100.0%) | elapsed 2.7m | ETA 0.0m\n",
      "Saved: featuresqaemb/videos/karaoke_UTS01_wheretheressmoke.mp4\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "if plt is None:\n",
    "    raise RuntimeError('Matplotlib unavailable in this environment.')\n",
    "\n",
    "if shutil.which('ffmpeg') is None:\n",
    "    raise RuntimeError('ffmpeg not found on PATH. Install ffmpeg to write MP4 files.')\n",
    "\n",
    "make_karaoke_category_video(\n",
    "    result=result,\n",
    "    token_df=TOKEN_BASE_DF,\n",
    "    category_cols=category_cols_12,\n",
    "    out_mp4=KARAOKE_OUTPUT,\n",
    "    use_domain=KARAOKE_USE_DOMAIN,\n",
    "    fps=KARAOKE_FPS,\n",
    "    window_sec=KARAOKE_WINDOW_SEC,\n",
    "    pad_left_sec=KARAOKE_PAD_LEFT_SEC,\n",
    "    pad_right_sec=KARAOKE_PAD_RIGHT_SEC,\n",
    "        log_every_frames=KARAOKE_LOG_EVERY_FRAMES,\n",
    "    playback_speed=KARAOKE_PLAYBACK_SPEED,\n",
    "    ypad_frac=KARAOKE_YLIM_PAD_FRAC,\n",
    "    bin_words_max=KARAOKE_BIN_WORDS_MAX,\n",
    "    zscore=KARAOKE_ZSCORE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d4195f",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- If you want different categories, set `KARAOKE_CATEGORY_COLUMNS` to an explicit list of 12.\n",
    "- For faster preview, reduce `KARAOKE_FPS` or `KARAOKE_WINDOW_SEC`.\n",
    "- The output MP4 path is set by `KARAOKE_OUTPUT`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
