{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ebcf3d3",
   "metadata": {},
   "source": [
    "# Day 18 â€“ Single Story Category Debugger\n",
    "\n",
    "Run the Day17 semantic category featurization pipeline for a single subject/story pairing to debug cluster prototypes, temporal weighting, and output paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73cdd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple, Sequence, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "np.random.seed(42)\n",
    "pd.options.display.max_columns = 50\n",
    "\n",
    "project_root = Path('/flash/PaoU/seann/fmri-edm-ccm')\n",
    "project_root.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(project_root)\n",
    "\n",
    "sys.path.append(str(project_root))\n",
    "sys.path.append('/flash/PaoU/seann/pyEDM/src')\n",
    "sys.path.append('/flash/PaoU/seann/MDE-main/src')\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    widgets = None\n",
    "    def display(obj):\n",
    "        print(obj)\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except Exception as exc:\n",
    "    plt = None\n",
    "    warnings.warn(f'Matplotlib unavailable: {exc}')\n",
    "\n",
    "from src.utils import load_yaml\n",
    "from src.decoding import load_transcript_words\n",
    "from src.edm_ccm import English1000Loader\n",
    "\n",
    "EPS = 1e-12  # numeric guard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddf33c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from subprocess import run\n",
    "\n",
    "PROJECT_ROOT = Path('/flash/PaoU/seann/fmri-edm-ccm')\n",
    "CONFIG_PATH = PROJECT_ROOT / 'configs' / 'demo.yaml'\n",
    "\n",
    "\n",
    "def build_caches_for_subject(subject: str, stories=None, dry_run=False, semantic_components=None):\n",
    "    \"\"\"Batch regenerate Schaefer ROI caches via the story-cache helper.\"\"\"\n",
    "    cmd = [\n",
    "        'python',\n",
    "        str(PROJECT_ROOT / 'scripts' / 'run_story_cache_batch.py'),\n",
    "        '--config',\n",
    "        str(CONFIG_PATH),\n",
    "        '--subjects',\n",
    "        subject,\n",
    "    ]\n",
    "    if stories:\n",
    "        cmd.extend(['--stories', *stories])\n",
    "    if dry_run:\n",
    "        cmd.append('--dry-run')\n",
    "    if semantic_components is not None:\n",
    "        cmd.extend(['--semantic-components', str(int(semantic_components))])\n",
    "    print('Running:', ' '.join(cmd))\n",
    "    run(cmd, check=True)\n",
    "\n",
    "\n",
    "# Example usage (uncomment to preview without executing):\n",
    "# build_caches_for_subject('UTS01', dry_run=True)\n",
    "\n",
    "# Load default story list if available\n",
    "stories = Path('misc/story_list.txt').read_text().splitlines() if Path('misc/story_list.txt').exists() else None\n",
    "# Example usage (uses all stories by default)\n",
    "# build_caches_for_subject('UTS01', stories=stories, dry_run=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86081391",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_yaml('configs/demo.yaml')\n",
    "categories_cfg = cfg.get('categories', {}) or {}\n",
    "cluster_csv_path = categories_cfg.get('cluster_csv_path', '')\n",
    "temporal_weighting = str(categories_cfg.get('temporal_weighting', 'proportional')).lower()\n",
    "assert temporal_weighting in {'proportional', 'none'}, \"temporal_weighting must be 'proportional' or 'none'.\"\n",
    "prototype_weight_power = float(categories_cfg.get('prototype_weight_power', 1.0))\n",
    "canonical_smoothing_seconds = float(categories_cfg.get('canonical_smoothing_seconds', 0.0))\n",
    "canonical_smoothing_method = str(categories_cfg.get('canonical_smoothing_method', 'moving_average')).lower()\n",
    "if canonical_smoothing_method not in {'moving_average', 'gaussian'}:\n",
    "    warnings.warn(f\"Unsupported canonical_smoothing_method='{canonical_smoothing_method}', defaulting to 'moving_average'.\")\n",
    "    canonical_smoothing_method = 'moving_average'\n",
    "canonical_gaussian_sigma_seconds_raw = categories_cfg.get('canonical_gaussian_sigma_seconds')\n",
    "canonical_gaussian_sigma_seconds = None if canonical_gaussian_sigma_seconds_raw in (None, '') else float(canonical_gaussian_sigma_seconds_raw)\n",
    "\n",
    "SUBJECT_DEFAULT = cfg.get('subject')\n",
    "STORY_DEFAULT = cfg.get('story')\n",
    "paths = cfg.get('paths', {})\n",
    "TR = float(cfg.get('TR', 2.0))\n",
    "\n",
    "features_root = Path(paths.get('features', 'features'))\n",
    "features_root.mkdir(parents=True, exist_ok=True)\n",
    "if not paths.get('features') and Path('figs').exists():\n",
    "    warnings.warn(\"Using default 'features/' directory. Update configs/demo.yaml with paths.features.\")\n",
    "\n",
    "if widgets is not None:\n",
    "    subject_widget = widgets.Text(value=str(SUBJECT_DEFAULT or ''), description='Subject')\n",
    "    story_widget = widgets.Text(value=str(STORY_DEFAULT or ''), description='Story')\n",
    "    display(subject_widget, story_widget)\n",
    "    SUBJECT_SELECTED = subject_widget.value or SUBJECT_DEFAULT\n",
    "    STORY_SELECTED = story_widget.value or STORY_DEFAULT\n",
    "else:\n",
    "    SUBJECT_SELECTED = SUBJECT_DEFAULT\n",
    "    STORY_SELECTED = STORY_DEFAULT\n",
    "if SUBJECT_SELECTED is None or STORY_SELECTED is None:\n",
    "    raise ValueError(\"Set 'subject' and 'story' in configs/demo.yaml before running Day18.\")\n",
    "\n",
    "categories_cfg_base = json.loads(json.dumps(categories_cfg))\n",
    "if cluster_csv_path and not Path(cluster_csv_path).exists():\n",
    "    warnings.warn(f\"Cluster CSV not found at {cluster_csv_path}. Create it before full runs.\")\n",
    "print(f'Target subject/story: {SUBJECT_SELECTED} / {STORY_SELECTED}')\n",
    "print(f'Cluster CSV: {cluster_csv_path or \"<none>\"}')\n",
    "print(f'Temporal weighting: {temporal_weighting} | Prototype weight power: {prototype_weight_power}')\n",
    "print(f'Canonical smoothing (s): {canonical_smoothing_seconds}')\n",
    "print(f'Canonical smoothing method: {canonical_smoothing_method} | Gaussian sigma (s): {canonical_gaussian_sigma_seconds}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7966a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def load_story_words(paths: Dict, subject: str, story: str) -> List[Tuple[str, float, float]]:\n",
    "    \"\"\"Wrapper around load_transcript_words with sanity checks.\"\"\"\n",
    "    events = load_transcript_words(paths, subject, story)\n",
    "    if not events:\n",
    "        raise ValueError(f'No transcript events found for {subject} {story}.')\n",
    "    return [(str(word).strip(), float(start), float(end)) for word, start, end in events]\n",
    "\n",
    "\n",
    "# === NEW: load cluster words from CSV (category, word, [weight]) ===\n",
    "def load_clusters_from_csv(csv_path: str) -> Dict[str, Dict[str, List[Tuple[str, float]]]]:\n",
    "    from pathlib import Path  # local import\n",
    "    if not csv_path or not Path(csv_path).exists():\n",
    "        raise FileNotFoundError(f'Cluster CSV not found at {csv_path}')\n",
    "    df = pd.read_csv(csv_path)\n",
    "    cols = {c.lower().strip(): c for c in df.columns}\n",
    "    for needed in ('category', 'word'):\n",
    "        assert needed in cols, f\"CSV must contain '{needed}' column.\"\n",
    "    cat_col = cols['category']\n",
    "    word_col = cols['word']\n",
    "    weight_col = cols.get('weight')\n",
    "    if weight_col is None:\n",
    "        df['_weight'] = 1.0\n",
    "        weight_col = '_weight'\n",
    "    df = df[[cat_col, word_col, weight_col]].copy()\n",
    "    df[word_col] = df[word_col].astype(str).str.strip().str.lower()\n",
    "    df[cat_col] = df[cat_col].astype(str).str.strip().str.lower()\n",
    "    df[weight_col] = pd.to_numeric(df[weight_col], errors='coerce').fillna(1.0).clip(lower=0.0)\n",
    "    clusters: Dict[str, Dict[str, List[Tuple[str, float]]]] = {}\n",
    "    for cat, sub in df.groupby(cat_col):\n",
    "        bucket: Dict[str, float] = {}\n",
    "        for w, wt in zip(sub[word_col].tolist(), sub[weight_col].tolist()):\n",
    "            if not w:\n",
    "                continue\n",
    "            bucket[w] = float(wt)\n",
    "        pairs = sorted(bucket.items())\n",
    "        if pairs:\n",
    "            clusters[cat] = {'words': pairs}\n",
    "    if not clusters:\n",
    "        raise ValueError('No clusters parsed from CSV.')\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# === NEW: build category prototypes from representative words (CSV) ===\n",
    "def build_states_from_csv(\n",
    "    clusters: Dict[str, Dict[str, List[Tuple[str, float]]]],\n",
    "    primary_lookup: Dict[str, np.ndarray],\n",
    "    fallback=None,\n",
    "    weight_power: float = 1.0\n",
    ") -> Tuple[Dict[str, Dict], Dict[str, Dict]]:\n",
    "    category_states: Dict[str, Dict] = {}\n",
    "    category_definitions: Dict[str, Dict] = {}\n",
    "    oov_counts: Dict[str, int] = {}\n",
    "    for cat, spec in clusters.items():\n",
    "        pairs = spec.get('words', [])\n",
    "        vecs: List[np.ndarray] = []\n",
    "        weights: List[float] = []\n",
    "        found_words: List[str] = []\n",
    "        missing_words: List[str] = []\n",
    "        for word, wt in pairs:\n",
    "            vec = lookup_embedding(word, primary_lookup, fallback)\n",
    "            if vec is None:\n",
    "                missing_words.append(word)\n",
    "                continue\n",
    "            vecs.append(vec.astype(float))\n",
    "            weights.append(float(max(0.0, wt)) ** float(weight_power))\n",
    "            found_words.append(word)\n",
    "        if not vecs:\n",
    "            warnings.warn(f\"[{cat}] no usable representative embeddings; prototype will be None.\")\n",
    "            prototype = None\n",
    "            prototype_norm = None\n",
    "        else:\n",
    "            W = np.array(weights, dtype=float)\n",
    "            W = W / (W.sum() + 1e-12)\n",
    "            M = np.stack(vecs, axis=0)\n",
    "            prototype = (W[:, None] * M).sum(axis=0)\n",
    "            prototype_norm = float(np.linalg.norm(prototype))\n",
    "            if prototype_norm < EPS:\n",
    "                prototype = None\n",
    "                prototype_norm = None\n",
    "        rep_lex = {word: float(wt) for word, wt in pairs}\n",
    "        category_states[cat] = {\n",
    "            'name': cat,\n",
    "            'seeds': [],\n",
    "            'found_seeds': found_words,\n",
    "            'missing_seeds': missing_words,\n",
    "            'prototype': prototype,\n",
    "            'prototype_norm': prototype_norm,\n",
    "            'lexicon': rep_lex,\n",
    "            'expanded_count': 0,\n",
    "            'expansion_params': {'enabled': False, 'top_k': 0, 'min_sim': 0.0},\n",
    "        }\n",
    "        category_definitions[cat] = {\n",
    "            'from': 'csv',\n",
    "            'seeds': [],\n",
    "            'found_seeds': found_words,\n",
    "            'missing_seeds': missing_words,\n",
    "            'prototype_dim': int(prototype.shape[0]) if isinstance(prototype, np.ndarray) else 0,\n",
    "            'prototype_norm': prototype_norm,\n",
    "            'representative_words': rep_lex,\n",
    "            'lexicon': rep_lex,\n",
    "            'expanded_neighbors': {},\n",
    "        }\n",
    "        oov_counts[cat] = len(missing_words)\n",
    "    if any(oov_counts.values()):\n",
    "        warnings.warn(f\"OOV representative words: {oov_counts}\")\n",
    "    return category_states, category_definitions\n",
    "\n",
    "\n",
    "def build_tr_edges(word_events: Sequence[Tuple[str, float, float]], tr_s: float) -> np.ndarray:\n",
    "    if not word_events:\n",
    "        return np.arange(0, tr_s, tr_s)\n",
    "    max_end = max(end for _, _, end in word_events)\n",
    "    n_tr = max(1, int(math.ceil(max_end / tr_s)))\n",
    "    edges = np.arange(0.0, (n_tr + 1) * tr_s, tr_s, dtype=float)\n",
    "    if edges[-1] < max_end:\n",
    "        edges = np.append(edges, edges[-1] + tr_s)\n",
    "    if edges[-1] < max_end - 1e-9:\n",
    "        edges = np.append(edges, edges[-1] + tr_s)\n",
    "    return edges\n",
    "\n",
    "\n",
    "def lookup_embedding(token: str, primary_lookup: Dict[str, np.ndarray], fallback=None) -> Optional[np.ndarray]:\n",
    "    key = token.lower().strip()\n",
    "    if not key:\n",
    "        return None\n",
    "    vec = primary_lookup.get(key) if primary_lookup else None\n",
    "    if vec is not None:\n",
    "        return np.asarray(vec, dtype=float)\n",
    "    if fallback is not None:\n",
    "        try:\n",
    "            if hasattr(fallback, 'get_vector') and key in fallback:\n",
    "                return np.asarray(fallback.get_vector(key), dtype=float)\n",
    "            if hasattr(fallback, '__contains__') and key in fallback:\n",
    "                return np.asarray(fallback[key], dtype=float)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def make_category_prototype(seeds: Sequence[str], primary_lookup: Dict[str, np.ndarray], fallback=None, allow_single: bool = False) -> Tuple[Optional[np.ndarray], List[str], List[str]]:\n",
    "    found_vectors = []\n",
    "    found_words = []\n",
    "    missing_words = []\n",
    "    for seed in seeds:\n",
    "        vec = lookup_embedding(seed, primary_lookup, fallback)\n",
    "        if vec is None:\n",
    "            missing_words.append(seed)\n",
    "            continue\n",
    "        found_vectors.append(vec)\n",
    "        found_words.append(seed)\n",
    "    if not found_vectors:\n",
    "        return None, found_words, missing_words\n",
    "    if len(found_vectors) < 2 and not allow_single:\n",
    "        warnings.warn(f'Only {len(found_vectors)} usable seed(s); enable allow_single_seed to accept singleton prototypes.')\n",
    "        if not allow_single:\n",
    "            return None, found_words, missing_words\n",
    "    prototype = np.mean(found_vectors, axis=0)\n",
    "    return prototype, found_words, missing_words\n",
    "\n",
    "\n",
    "def expand_category(prototype: np.ndarray, vocab_embeddings: np.ndarray, vocab_words: Sequence[str], top_k: int, min_sim: float) -> Dict[str, float]:\n",
    "    if prototype is None or vocab_embeddings is None or vocab_words is None:\n",
    "        return {}\n",
    "    proto = np.asarray(prototype, dtype=float)\n",
    "    proto_norm = np.linalg.norm(proto)\n",
    "    if proto_norm == 0:\n",
    "        return {}\n",
    "    proto_unit = proto / proto_norm\n",
    "    vocab_norms = np.linalg.norm(vocab_embeddings, axis=1)\n",
    "    valid_mask = vocab_norms > 0\n",
    "    sims = np.full(vocab_embeddings.shape[0], -1.0, dtype=float)\n",
    "    sims[valid_mask] = (vocab_embeddings[valid_mask] @ proto_unit) / vocab_norms[valid_mask]\n",
    "    top_k_eff = min(top_k, len(sims))\n",
    "    if top_k_eff <= 0:\n",
    "        return {}\n",
    "    candidate_idx = np.argpartition(-sims, top_k_eff - 1)[:top_k_eff]\n",
    "    out = {}\n",
    "    for idx in candidate_idx:\n",
    "        score = float(sims[idx])\n",
    "        if score < min_sim:\n",
    "            continue\n",
    "        out[vocab_words[idx]] = score\n",
    "    return out\n",
    "\n",
    "\n",
    "def tr_token_overlap(token_start: float, token_end: float, tr_start: float, tr_end: float, mode: str = 'proportional') -> float:\n",
    "    token_start = float(token_start)\n",
    "    token_end = float(token_end)\n",
    "    if token_end <= token_start:\n",
    "        token_end = token_start + 1e-3\n",
    "    if mode == 'midpoint':\n",
    "        midpoint = 0.5 * (token_start + token_end)\n",
    "        return 1.0 if tr_start <= midpoint < tr_end else 0.0\n",
    "    overlap = max(0.0, min(token_end, tr_end) - max(token_start, tr_start))\n",
    "    duration = token_end - token_start\n",
    "    if duration <= 0:\n",
    "        return 1.0 if overlap > 0 else 0.0\n",
    "    return max(0.0, min(1.0, overlap / duration))\n",
    "\n",
    "\n",
    "def score_tr(token_payload: Sequence[Dict], method: str, *, lexicon: Optional[Dict[str, float]] = None, prototype: Optional[np.ndarray] = None, prototype_norm: Optional[float] = None) -> float:\n",
    "    if not token_payload:\n",
    "        return float('nan')\n",
    "    method = method.lower()\n",
    "    if method == 'count':\n",
    "        if not lexicon:\n",
    "            return float('nan')\n",
    "        total = 0.0\n",
    "        for item in token_payload:\n",
    "            weight = lexicon.get(item['word'].lower())\n",
    "            if weight is None:\n",
    "                continue\n",
    "            total += weight * item['overlap']\n",
    "        return float(total)\n",
    "    if method == 'similarity':\n",
    "        if prototype is None or prototype_norm is None or prototype_norm < EPS:\n",
    "            return float('nan')\n",
    "        num = 0.0\n",
    "        denom = 0.0\n",
    "        for item in token_payload:\n",
    "            emb = item.get('embedding')\n",
    "            if emb is None:\n",
    "                continue\n",
    "            emb_norm = item.get('embedding_norm')\n",
    "            if emb_norm is None or emb_norm < EPS:\n",
    "                continue\n",
    "            sim = float(np.dot(emb, prototype) / (emb_norm * prototype_norm))\n",
    "            num += sim * item['overlap']\n",
    "            denom += item['overlap']\n",
    "        if denom == 0:\n",
    "            return float('nan')\n",
    "        value = num / denom\n",
    "        return float(np.clip(value, -1.0, 1.0))\n",
    "    raise ValueError(f'Unknown scoring method: {method}')\n",
    "\n",
    "\n",
    "def ensure_serializable(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, (np.floating, np.integer)):\n",
    "        return obj.item()\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: ensure_serializable(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [ensure_serializable(v) for v in obj]\n",
    "    return obj\n",
    "\n",
    "\n",
    "def build_token_buckets(edges: np.ndarray, event_records: Sequence[Dict], mode: str = 'proportional') -> List[List[Dict]]:\n",
    "    if edges.size < 2:\n",
    "        return []\n",
    "    buckets: List[List[Dict]] = [[] for _ in range(len(edges) - 1)]\n",
    "    for rec in event_records:\n",
    "        start = rec['start']\n",
    "        end = rec['end']\n",
    "        if end <= edges[0] or start >= edges[-1]:\n",
    "            continue\n",
    "        start_idx = max(0, int(np.searchsorted(edges, start, side='right')) - 1)\n",
    "        end_idx = max(0, int(np.searchsorted(edges, end, side='left')))\n",
    "        end_idx = min(end_idx, len(buckets) - 1)\n",
    "        for idx in range(start_idx, end_idx + 1):\n",
    "            bucket_start = edges[idx]\n",
    "            bucket_end = edges[idx + 1]\n",
    "            if mode == 'none':\n",
    "                overlap = 1.0 if not (end <= bucket_start or start >= bucket_end) else 0.0\n",
    "            else:\n",
    "                overlap = tr_token_overlap(start, end, bucket_start, bucket_end, 'proportional')\n",
    "            if overlap <= 0:\n",
    "                continue\n",
    "            buckets[idx].append({\n",
    "                'word': rec['word'],\n",
    "                'overlap': overlap,\n",
    "                'embedding': rec['embedding'],\n",
    "                'embedding_norm': rec['embedding_norm'],\n",
    "                'token_start': rec['start'],\n",
    "                'token_end': rec['end'],\n",
    "                'bucket_start': bucket_start,\n",
    "                'bucket_end': bucket_end,\n",
    "            })\n",
    "    return buckets\n",
    "\n",
    "\n",
    "def score_time_series(edges: np.ndarray, buckets: Sequence[Sequence[Dict]], category_states: Dict[str, Dict], category_names: Sequence[str], category_columns: Sequence[str], method: str, index_name: str) -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "    n_bins = len(buckets)\n",
    "    score_matrix = np.full((n_bins, len(category_names)), np.nan, dtype=float)\n",
    "    for col_idx, cat_name in enumerate(category_names):\n",
    "        state = category_states[cat_name]\n",
    "        lexicon = state.get('lexicon')\n",
    "        prototype = state.get('prototype')\n",
    "        prototype_norm = state.get('prototype_norm')\n",
    "        for bin_idx, bucket in enumerate(buckets):\n",
    "            score_matrix[bin_idx, col_idx] = score_tr(bucket, method, lexicon=lexicon, prototype=prototype, prototype_norm=prototype_norm)\n",
    "    data = {\n",
    "        index_name: np.arange(n_bins, dtype=int),\n",
    "        'start_sec': edges[:-1],\n",
    "        'end_sec': edges[1:],\n",
    "    }\n",
    "    for col_idx, col in enumerate(category_columns):\n",
    "        data[col] = score_matrix[:, col_idx]\n",
    "    df = pd.DataFrame(data)\n",
    "    return df, score_matrix\n",
    "\n",
    "def build_smoothing_kernel(seconds_bin_width: float, smoothing_seconds: float, *, method: str = 'moving_average', gaussian_sigma_seconds: Optional[float] = None) -> np.ndarray:\n",
    "    if smoothing_seconds <= 0:\n",
    "        return np.array([1.0], dtype=float)\n",
    "    method = str(method or 'moving_average').lower()\n",
    "    if method == 'moving_average':\n",
    "        window_samples = max(1, int(round(smoothing_seconds / seconds_bin_width)))\n",
    "        if window_samples % 2 == 0:\n",
    "            window_samples += 1\n",
    "        kernel = np.ones(window_samples, dtype=float)\n",
    "    elif method == 'gaussian':\n",
    "        sigma_seconds = float(gaussian_sigma_seconds) if gaussian_sigma_seconds not in (None, '') else max(smoothing_seconds / 2.0, seconds_bin_width)\n",
    "        sigma_samples = max(sigma_seconds / seconds_bin_width, 1e-6)\n",
    "        half_width = max(1, int(round(3.0 * sigma_samples)))\n",
    "        grid = np.arange(-half_width, half_width + 1, dtype=float)\n",
    "        kernel = np.exp(-0.5 * (grid / sigma_samples) ** 2)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown smoothing method: {method}\")\n",
    "    kernel_sum = float(kernel.sum())\n",
    "    if kernel_sum <= 0:\n",
    "        return np.array([1.0], dtype=float)\n",
    "    return kernel / kernel_sum\n",
    "\n",
    "\n",
    "def apply_smoothing_kernel(values: np.ndarray, kernel: np.ndarray, *, pad_mode: str = 'edge') -> np.ndarray:\n",
    "    if values.size == 0 or kernel.size <= 1:\n",
    "        return values.copy()\n",
    "    pad_mode = pad_mode if pad_mode in {'edge', 'reflect'} else 'edge'\n",
    "    half = kernel.size // 2\n",
    "    padded = np.pad(values, ((half, half), (0, 0)), mode=pad_mode)\n",
    "    smoothed = np.apply_along_axis(lambda column: np.convolve(column, kernel, mode='valid'), 0, padded)\n",
    "    return smoothed\n",
    "\n",
    "\n",
    "def aggregate_seconds_to_edges(canonical_edges: np.ndarray, canonical_values: np.ndarray, target_edges: np.ndarray) -> np.ndarray:\n",
    "    if canonical_values.size == 0:\n",
    "        return np.empty((len(target_edges) - 1, 0), dtype=float)\n",
    "    midpoints = 0.5 * (canonical_edges[:-1] + canonical_edges[1:])\n",
    "    bin_ids = np.digitize(midpoints, target_edges) - 1\n",
    "    if bin_ids.size:\n",
    "        bin_ids = np.clip(bin_ids, 0, len(target_edges) - 2)\n",
    "    out = np.full((len(target_edges) - 1, canonical_values.shape[1]), np.nan, dtype=float)\n",
    "    for idx in range(out.shape[0]):\n",
    "        mask = bin_ids == idx\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        values = canonical_values[mask]\n",
    "        if values.ndim == 1:\n",
    "            values = values[:, None]\n",
    "        finite_any = np.isfinite(values).any(axis=0)\n",
    "        if not finite_any.any():\n",
    "            continue\n",
    "        col_means = np.full(values.shape[1], np.nan, dtype=float)\n",
    "        col_means[finite_any] = np.nanmean(values[:, finite_any], axis=0)\n",
    "        out[idx] = col_means\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f7758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_category_featurization(subject: str, story: str, *,\n",
    "                            cfg_base: Dict,\n",
    "                            categories_cfg_base: Dict,\n",
    "                            cluster_csv_path: str,\n",
    "                            temporal_weighting: str,\n",
    "                            prototype_weight_power: float,\n",
    "                            canonical_smoothing_seconds: float,\n",
    "                            canonical_smoothing_method: str,\n",
    "                            canonical_gaussian_sigma_seconds: Optional[float],\n",
    "                            temporal_weighting_override: Optional[str] = None) -> Dict[str, Any]:\n",
    "    if not subject or not story:\n",
    "        raise ValueError('Subject and story must be provided.')\n",
    "    print(f\"=== Running category featurization for {subject} / {story} ===\")\n",
    "\n",
    "    categories_cfg = json.loads(json.dumps(categories_cfg_base or {}))\n",
    "    category_sets = categories_cfg.get('sets', {})\n",
    "    available_sets = sorted(category_sets.keys())\n",
    "    category_set_name = categories_cfg.get('category_set') or (available_sets[0] if available_sets else None)\n",
    "    if cluster_csv_path:\n",
    "        if not category_set_name:\n",
    "            category_set_name = 'csv_clusters'\n",
    "        categories_cfg['category_set'] = category_set_name\n",
    "        categories_cfg['category_score_method'] = 'similarity'\n",
    "        categories_cfg['allow_single_seed'] = True\n",
    "        categories_cfg['expansion'] = {'enabled': False}\n",
    "    category_score_method = str(categories_cfg.get('category_score_method', 'similarity')).lower()\n",
    "    overlap_mode = str(categories_cfg.get('overlap_weighting', 'proportional')).lower()\n",
    "    expansion_cfg = categories_cfg.get('expansion', {})\n",
    "    allow_single = bool(categories_cfg.get('allow_single_seed', False))\n",
    "    exp_enabled = bool(expansion_cfg.get('enabled', True))\n",
    "    exp_top_k = int(expansion_cfg.get('top_k', 2000)) if exp_enabled else 0\n",
    "    exp_min_sim = float(expansion_cfg.get('min_sim', 0.35)) if exp_enabled else 0.0\n",
    "\n",
    "    selected_set_spec = category_sets.get(category_set_name, {}) if category_sets else {}\n",
    "\n",
    "    output_root = features_root / 'subjects' / subject / story / 'day17_categories'\n",
    "    output_root.mkdir(parents=True, exist_ok=True)\n",
    "    canonical_root = features_root / 'stories' / story / 'day17_categories'\n",
    "    canonical_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    story_events = load_story_words(paths, subject, story)\n",
    "    print(f'Loaded {len(story_events)} transcript events.')\n",
    "    tr_edges = build_tr_edges(story_events, TR)\n",
    "    n_tr = len(tr_edges) - 1\n",
    "    print(f'TR edges: {len(tr_edges)} (n_tr={n_tr}) spanning {tr_edges[-1]:.2f} seconds.')\n",
    "\n",
    "    embedding_source = str(categories_cfg.get('embedding_source', 'english1000')).lower()\n",
    "    english_loader = None\n",
    "    english_lookup: Dict[str, np.ndarray] = {}\n",
    "    english_vocab: List[str] = []\n",
    "    english_matrix = None\n",
    "    if embedding_source in {'english1000', 'both'}:\n",
    "        english1000_path = Path(paths.get('data_root', '')) / 'derivative' / 'english1000sm.hf5'\n",
    "        if english1000_path.exists():\n",
    "            english_loader = English1000Loader(english1000_path)\n",
    "            english_lookup = english_loader.lookup\n",
    "            english_vocab = english_loader.vocab\n",
    "            english_matrix = english_loader.embeddings\n",
    "            print(f'Loaded English1000 embeddings from {english1000_path} (vocab={len(english_vocab)}).')\n",
    "        else:\n",
    "            raise FileNotFoundError(f'English1000 embeddings not found at {english1000_path}')\n",
    "    else:\n",
    "        print('English1000 disabled by configuration.')\n",
    "\n",
    "    word2vec_model = None\n",
    "    if embedding_source in {'word2vec', 'both'}:\n",
    "        w2v_path = categories_cfg.get('word2vec_path')\n",
    "        if w2v_path:\n",
    "            w2v_path = Path(w2v_path)\n",
    "            if w2v_path.exists():\n",
    "                try:\n",
    "                    from gensim.models import KeyedVectors\n",
    "                    binary = w2v_path.suffix.lower() in {'.bin', '.gz'}\n",
    "                    word2vec_model = KeyedVectors.load_word2vec_format(w2v_path, binary=binary)\n",
    "                    print(f'Loaded Word2Vec fallback from {w2v_path}.')\n",
    "                except Exception as exc:\n",
    "                    warnings.warn(f'Failed to load Word2Vec fallback: {exc}')\n",
    "            else:\n",
    "                warnings.warn(f'Word2Vec path does not exist: {w2v_path}')\n",
    "        else:\n",
    "            warnings.warn('Word2Vec fallback requested but no path provided.')\n",
    "    else:\n",
    "        print('Word2Vec fallback disabled.')\n",
    "\n",
    "    if cluster_csv_path:\n",
    "        csv_clusters = load_clusters_from_csv(cluster_csv_path)\n",
    "        category_states, category_definitions = build_states_from_csv(\n",
    "            csv_clusters,\n",
    "            english_lookup,\n",
    "            word2vec_model,\n",
    "            weight_power=prototype_weight_power,\n",
    "        )\n",
    "        category_names = sorted(category_states.keys())\n",
    "        category_columns = [f'cat_{name}' for name in category_names]\n",
    "        print(f\"Loaded {len(category_names)} CSV-driven categories from {cluster_csv_path}: {category_names}\")\n",
    "        zero_norm = [k for k, v in category_states.items() if v.get('prototype') is not None and (v.get('prototype_norm') or 0.0) < EPS]\n",
    "        if zero_norm:\n",
    "            warnings.warn(f\"Zero-norm prototypes (check OOV/weights): {zero_norm}\")\n",
    "    else:\n",
    "        category_states = {}\n",
    "        category_definitions = {}\n",
    "        seed_oov_counter = Counter()\n",
    "        for cat_name, cat_spec in selected_set_spec.items():\n",
    "            seeds = cat_spec.get('seeds', [])\n",
    "            explicit_words = cat_spec.get('words', [])\n",
    "            prototype = None\n",
    "            found_seeds: List[str] = []\n",
    "            missing_seeds: List[str] = []\n",
    "            if seeds:\n",
    "                prototype, found_seeds, missing_seeds = make_category_prototype(seeds, english_lookup, word2vec_model, allow_single)\n",
    "                seed_oov_counter[cat_name] = len(missing_seeds)\n",
    "                if prototype is None and category_score_method == 'similarity':\n",
    "                    warnings.warn(f\"Category '{cat_name}' has no usable prototype; TR scores will be NaN.\")\n",
    "            elif category_score_method == 'similarity':\n",
    "                warnings.warn(f'Category {cat_name} has no seeds; similarity method will yield NaNs.')\n",
    "            lexicon = {word.lower(): 1.0 for word in explicit_words}\n",
    "            for seed in found_seeds:\n",
    "                lexicon.setdefault(seed.lower(), 1.0)\n",
    "            prototype_norm = None\n",
    "            expanded_words = {}\n",
    "            if prototype is not None:\n",
    "                prototype_norm = float(np.linalg.norm(prototype))\n",
    "                if exp_enabled and english_matrix is not None:\n",
    "                    expanded_words = expand_category(prototype, english_matrix, english_vocab, exp_top_k, exp_min_sim)\n",
    "                    for word, weight in expanded_words.items():\n",
    "                        lexicon.setdefault(word.lower(), float(weight))\n",
    "            if not lexicon and category_score_method == 'count':\n",
    "                warnings.warn(f'Category {cat_name} lexicon is empty; counts will be NaN.')\n",
    "            category_states[cat_name] = {\n",
    "                'name': cat_name,\n",
    "                'seeds': seeds,\n",
    "                'found_seeds': found_seeds,\n",
    "                'missing_seeds': missing_seeds,\n",
    "                'prototype': prototype,\n",
    "                'prototype_norm': prototype_norm,\n",
    "                'lexicon': lexicon,\n",
    "                'expanded_count': len(expanded_words),\n",
    "                'expansion_params': {\n",
    "                    'enabled': exp_enabled,\n",
    "                    'top_k': exp_top_k,\n",
    "                    'min_sim': exp_min_sim,\n",
    "                },\n",
    "            }\n",
    "            category_definitions[cat_name] = {\n",
    "                'seeds': seeds,\n",
    "                'found_seeds': found_seeds,\n",
    "                'missing_seeds': missing_seeds,\n",
    "                'prototype_dim': int(prototype.shape[0]) if isinstance(prototype, np.ndarray) else 0,\n",
    "                'prototype_norm': prototype_norm,\n",
    "                'expanded_neighbors': ensure_serializable(expanded_words),\n",
    "                'lexicon': {word: float(weight) for word, weight in sorted(category_states[cat_name]['lexicon'].items())},\n",
    "            }\n",
    "        print('Category seeds missing counts:', dict(seed_oov_counter))\n",
    "        category_names = sorted(category_states.keys())\n",
    "        category_columns = [f'cat_{name}' for name in category_names]\n",
    "        print(f'Prepared {len(category_names)} categories: {category_names}')\n",
    "\n",
    "    tw_mode = str(temporal_weighting_override or temporal_weighting or 'proportional').lower()\n",
    "\n",
    "    seconds_bin_width = float(categories_cfg.get('seconds_bin_width', 0.05))\n",
    "    if seconds_bin_width <= 0:\n",
    "        raise ValueError('seconds_bin_width must be positive.')\n",
    "    smoothing_method = str(canonical_smoothing_method or 'moving_average').lower()\n",
    "    gaussian_sigma_seconds = canonical_gaussian_sigma_seconds\n",
    "    smoothing_pad = str(categories_cfg.get('canonical_smoothing_pad', 'edge')).lower()\n",
    "    if smoothing_pad not in {'edge', 'reflect'}:\n",
    "        smoothing_pad = 'edge'\n",
    "\n",
    "    embedding_cache: Dict[str, Optional[np.ndarray]] = {}\n",
    "    event_records: List[Dict] = []\n",
    "    tokens_with_embeddings = 0\n",
    "    for word, onset, offset in story_events:\n",
    "        token = word.strip()\n",
    "        if not token:\n",
    "            continue\n",
    "        key = token.lower()\n",
    "        if key not in embedding_cache:\n",
    "            embedding_cache[key] = lookup_embedding(token, english_lookup, word2vec_model)\n",
    "        emb = embedding_cache[key]\n",
    "        emb_norm = float(np.linalg.norm(emb)) if emb is not None else None\n",
    "        if emb is not None:\n",
    "            tokens_with_embeddings += 1\n",
    "        event_records.append({\n",
    "            'word': token,\n",
    "            'start': float(onset),\n",
    "            'end': float(offset),\n",
    "            'embedding': emb,\n",
    "            'embedding_norm': emb_norm,\n",
    "        })\n",
    "\n",
    "    total_tokens = len(event_records)\n",
    "    print(f'Tokens with embeddings: {tokens_with_embeddings}/{total_tokens} (OOV rate={(total_tokens - tokens_with_embeddings) / max(total_tokens, 1):.2%}).')\n",
    "\n",
    "    if not event_records:\n",
    "        raise ValueError('No token events available for category featurization.')\n",
    "    max_end_time = max(rec['end'] for rec in event_records)\n",
    "    canonical_edges = np.arange(0.0, max_end_time + seconds_bin_width, seconds_bin_width, dtype=float)\n",
    "    if canonical_edges[-1] < max_end_time:\n",
    "        canonical_edges = np.append(canonical_edges, canonical_edges[-1] + seconds_bin_width)\n",
    "    if canonical_edges[-1] < max_end_time - 1e-9:\n",
    "        canonical_edges = np.append(canonical_edges, canonical_edges[-1] + seconds_bin_width)\n",
    "    assert np.all(np.diff(canonical_edges) > 0), 'Non-monotone canonical edges.'\n",
    "\n",
    "    canonical_buckets = build_token_buckets(canonical_edges, event_records, tw_mode)\n",
    "    empty_canonical = sum(1 for bucket in canonical_buckets if not bucket)\n",
    "    print(f'Canonical bins without tokens: {empty_canonical}/{len(canonical_buckets)}')\n",
    "\n",
    "    canonical_df_raw, canonical_matrix = score_time_series(\n",
    "        canonical_edges,\n",
    "        canonical_buckets,\n",
    "        category_states,\n",
    "        category_names,\n",
    "        category_columns,\n",
    "        category_score_method,\n",
    "        index_name='bin_index',\n",
    "    )\n",
    "    canonical_values_raw = canonical_matrix.copy()\n",
    "    canonical_df_raw = canonical_df_raw.copy()\n",
    "    smoothing_kernel = build_smoothing_kernel(\n",
    "        seconds_bin_width,\n",
    "        canonical_smoothing_seconds,\n",
    "        method=smoothing_method,\n",
    "        gaussian_sigma_seconds=gaussian_sigma_seconds,\n",
    "    )\n",
    "    smoothing_applied = smoothing_kernel.size > 1\n",
    "    if canonical_values_raw.size and smoothing_applied:\n",
    "        canonical_values_smoothed = apply_smoothing_kernel(canonical_values_raw, smoothing_kernel, pad_mode=smoothing_pad)\n",
    "    else:\n",
    "        canonical_values_smoothed = canonical_values_raw.copy()\n",
    "    canonical_df_smoothed = canonical_df_raw.copy()\n",
    "    if category_columns:\n",
    "        canonical_df_smoothed.loc[:, category_columns] = canonical_values_smoothed\n",
    "    canonical_df_selected = canonical_df_smoothed if smoothing_applied else canonical_df_raw\n",
    "\n",
    "    canonical_csv_path = canonical_root / 'category_timeseries_seconds.csv'\n",
    "    canonical_df_selected.to_csv(canonical_csv_path, index=False)\n",
    "    if smoothing_applied:\n",
    "        canonical_raw_path = canonical_root / 'category_timeseries_seconds_raw.csv'\n",
    "        canonical_df_raw.to_csv(canonical_raw_path, index=False)\n",
    "    canonical_definition_path = canonical_root / 'category_definition.json'\n",
    "    with canonical_definition_path.open('w') as fh:\n",
    "        json.dump(ensure_serializable(category_definitions), fh, indent=2)\n",
    "    print(f'Saved canonical story series to {canonical_csv_path}')\n",
    "\n",
    "    tr_buckets = build_token_buckets(tr_edges, event_records, tw_mode)\n",
    "    empty_tr = sum(1 for bucket in tr_buckets if not bucket)\n",
    "    print(f'TRs without tokens: {empty_tr}/{len(tr_buckets)}')\n",
    "\n",
    "    if category_columns:\n",
    "        tr_values_raw = aggregate_seconds_to_edges(canonical_edges, canonical_values_raw, tr_edges)\n",
    "        tr_values_smoothed = aggregate_seconds_to_edges(canonical_edges, canonical_values_smoothed, tr_edges)\n",
    "    else:\n",
    "        tr_values_raw = np.empty((len(tr_edges) - 1, 0), dtype=float)\n",
    "        tr_values_smoothed = tr_values_raw\n",
    "\n",
    "    base_index = np.arange(len(tr_edges) - 1, dtype=int)\n",
    "    base_df = pd.DataFrame({'tr_index': base_index, 'start_sec': tr_edges[:-1], 'end_sec': tr_edges[1:]})\n",
    "    category_df_raw = base_df.copy()\n",
    "    category_df_smoothed = base_df.copy()\n",
    "    if category_columns:\n",
    "        category_df_raw.loc[:, category_columns] = tr_values_raw\n",
    "        category_df_smoothed.loc[:, category_columns] = tr_values_smoothed\n",
    "    category_df = category_df_smoothed if smoothing_applied else category_df_raw\n",
    "    print(category_df.head())\n",
    "\n",
    "    assert len(category_df) == len(tr_buckets), 'Category dataframe row mismatch.'\n",
    "    assert len(category_columns) == len(category_names), 'Category column mismatch.'\n",
    "    if category_score_method == 'similarity':\n",
    "        finite_vals = category_df[category_columns].to_numpy(dtype=float)\n",
    "        finite_vals = finite_vals[np.isfinite(finite_vals)]\n",
    "        if finite_vals.size:\n",
    "            assert np.nanmin(finite_vals) >= -1.0001 and np.nanmax(finite_vals) <= 1.0001, 'Similarity scores out of bounds.'\n",
    "        nonempty_mask = np.array([len(b) > 0 for b in tr_buckets])\n",
    "        if nonempty_mask.any():\n",
    "            has_finite = np.isfinite(category_df.loc[nonempty_mask, category_columns].to_numpy()).any()\n",
    "            if not has_finite:\n",
    "                raise RuntimeError('All similarity scores are NaN despite non-empty TR buckets.')\n",
    "    else:\n",
    "        assert (category_df[category_columns].fillna(0.0) >= -1e-9).all().all(), 'Count scores must be non-negative.'\n",
    "\n",
    "    category_csv_path = output_root / 'category_timeseries.csv'\n",
    "    category_df.to_csv(category_csv_path, index=False)\n",
    "    if smoothing_applied:\n",
    "        category_raw_path = output_root / 'category_timeseries_raw.csv'\n",
    "        category_df_raw.to_csv(category_raw_path, index=False)\n",
    "    definition_path = output_root / 'category_definition.json'\n",
    "    with definition_path.open('w') as fh:\n",
    "        json.dump(ensure_serializable(category_definitions), fh, indent=2)\n",
    "    print(f'Saved category time series to {category_csv_path}')\n",
    "\n",
    "    trimmed_path = Path(paths.get('figs', 'figs')) / subject / story / 'day16_decoding' / 'semantic_pcs_trimmed.csv'\n",
    "    max_lag_primary = 0\n",
    "    day16_trim = None\n",
    "    if trimmed_path.exists():\n",
    "        day16_trim = pd.read_csv(trimmed_path)\n",
    "        expected_len = len(day16_trim)\n",
    "        if len(day16_trim) > len(category_df):\n",
    "            raise ValueError('Day16 trimmed series longer than category series; regenerate Day16 or rerun Day17.')\n",
    "        max_lag_primary = max(0, len(category_df) - expected_len)\n",
    "        print(f'Aligning with Day16 trim (max_lag_primary={max_lag_primary}).')\n",
    "    else:\n",
    "        tau_grid = [1, 2]\n",
    "        E_cap = 6\n",
    "        max_tau = max(tau_grid)\n",
    "        max_lag_primary = max_tau * (E_cap - 1)\n",
    "        max_lag_primary = min(max_lag_primary, len(category_df) - 1)\n",
    "        warnings.warn(f\"Day16 trimmed PCs not found; approximating max_lag_primary={max_lag_primary}.\")\n",
    "\n",
    "    trimmed_df = category_df.iloc[max_lag_primary:].reset_index(drop=True)\n",
    "    trimmed_out = trimmed_df[['tr_index']].copy()\n",
    "    trimmed_out.rename(columns={'tr_index': 'trim_index'}, inplace=True)\n",
    "    for col in category_columns:\n",
    "        trimmed_out[col] = trimmed_df[col].values\n",
    "    trimmed_csv_path = output_root / 'category_timeseries_trimmed.csv'\n",
    "    trimmed_out.to_csv(trimmed_csv_path, index=False)\n",
    "    print(f'Saved trimmed category series to {trimmed_csv_path}')\n",
    "\n",
    "    category_stats = {}\n",
    "    for col in category_columns:\n",
    "        values = category_df[col].to_numpy()\n",
    "        category_stats[col] = {\n",
    "            'mean': float(np.nanmean(values)),\n",
    "            'std': float(np.nanstd(values)),\n",
    "            'nan_fraction': float(np.mean(~np.isfinite(values)))\n",
    "        }\n",
    "\n",
    "    config_snapshot = {\n",
    "        **categories_cfg,\n",
    "        'category_set': category_set_name,\n",
    "        'cluster_csv_path': cluster_csv_path,\n",
    "        'temporal_weighting': tw_mode,\n",
    "        'prototype_weight_power': prototype_weight_power,\n",
    "        'canonical_smoothing_seconds': canonical_smoothing_seconds,\n",
    "        'canonical_smoothing_method': smoothing_method,\n",
    "        'canonical_gaussian_sigma_seconds': float(gaussian_sigma_seconds) if gaussian_sigma_seconds is not None else None,\n",
    "        'canonical_smoothing_kernel_size': int(smoothing_kernel.size),\n",
    "        'canonical_seconds_bin_width': seconds_bin_width,\n",
    "    }\n",
    "    config_path = output_root / 'config_used.yaml'\n",
    "    with config_path.open('w') as fh:\n",
    "        yaml.safe_dump(config_snapshot, fh, sort_keys=False)\n",
    "\n",
    "    meta = {\n",
    "        'subject': subject,\n",
    "        'story': story,\n",
    "        'tr_seconds': TR,\n",
    "        'n_tr': n_tr,\n",
    "        'category_set': category_set_name,\n",
    "        'category_score_method': category_score_method,\n",
    "        'overlap_weighting': overlap_mode,\n",
    "        'max_lag_primary': max_lag_primary,\n",
    "        'categories': category_stats,\n",
    "    }\n",
    "    import hashlib, json as _json\n",
    "    _cfg_hash = hashlib.md5(_json.dumps(config_snapshot, sort_keys=True).encode()).hexdigest()\n",
    "    meta['config_hash'] = _cfg_hash\n",
    "    meta.update({\n",
    "        'cluster_csv_path': cluster_csv_path,\n",
    "        'temporal_weighting': tw_mode,\n",
    "        'prototype_weight_power': prototype_weight_power,\n",
    "        'canonical_smoothing': {\n",
    "            'applied': bool(smoothing_applied),\n",
    "            'seconds': canonical_smoothing_seconds,\n",
    "            'method': smoothing_method,\n",
    "            'gaussian_sigma_seconds': float(gaussian_sigma_seconds) if gaussian_sigma_seconds is not None else None,\n",
    "            'kernel_size': int(smoothing_kernel.size),\n",
    "            'pad_mode': smoothing_pad,\n",
    "            'bin_width_seconds': seconds_bin_width,\n",
    "        },\n",
    "    })\n",
    "    meta_path = output_root / 'meta.json'\n",
    "    with meta_path.open('w') as fh:\n",
    "        json.dump(meta, fh, indent=2)\n",
    "    print(f'Meta statistics saved to {meta_path}')\n",
    "\n",
    "    print(\"  ,  \", (day16_trim is None) or (len(day16_trim) <= len(category_df)))\n",
    "    print(\"  ,  \", set(category_columns).issubset(set(category_df.columns)))\n",
    "    print(\"  ,  \", meta.get('config_hash', '')[:8])\n",
    "\n",
    "    smoothing_meta = {\n",
    "        'applied': bool(smoothing_applied),\n",
    "        'seconds': canonical_smoothing_seconds,\n",
    "        'method': smoothing_method,\n",
    "        'gaussian_sigma_seconds': float(gaussian_sigma_seconds) if gaussian_sigma_seconds is not None else None,\n",
    "        'kernel_size': int(smoothing_kernel.size),\n",
    "        'pad_mode': smoothing_pad,\n",
    "        'bin_width_seconds': seconds_bin_width,\n",
    "    }\n",
    "    print('=== Completed single-story run ===')\n",
    "    return {\n",
    "        'category_df': category_df,\n",
    "        'category_df_raw': category_df_raw,\n",
    "        'category_df_smoothed': category_df_smoothed,\n",
    "        'canonical_df': canonical_df_selected,\n",
    "        'canonical_df_raw': canonical_df_raw,\n",
    "        'canonical_df_smoothed': canonical_df_smoothed,\n",
    "        'tr_buckets': tr_buckets,\n",
    "        'canonical_buckets': canonical_buckets,\n",
    "        'category_states': category_states,\n",
    "        'category_columns': category_columns,\n",
    "        'temporal_weighting': tw_mode,\n",
    "        'tr_edges': tr_edges,\n",
    "        'canonical_edges': canonical_edges,\n",
    "        'output_root': output_root,\n",
    "        'canonical_root': canonical_root,\n",
    "        'config_snapshot': config_snapshot,\n",
    "        'smoothing': smoothing_meta,\n",
    "        'smoothing_kernel': smoothing_kernel.tolist(),\n",
    "        'seconds_bin_width': seconds_bin_width,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db417c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_temporal = run_category_featurization(\n",
    "    SUBJECT_SELECTED,\n",
    "    STORY_SELECTED,\n",
    "    cfg_base=cfg,\n",
    "    categories_cfg_base=categories_cfg_base,\n",
    "    cluster_csv_path=cluster_csv_path,\n",
    "    temporal_weighting=temporal_weighting,\n",
    "    prototype_weight_power=prototype_weight_power,\n",
    "    canonical_smoothing_seconds=canonical_smoothing_seconds,\n",
    "    canonical_smoothing_method=canonical_smoothing_method,\n",
    "    canonical_gaussian_sigma_seconds=canonical_gaussian_sigma_seconds,\n",
    "    temporal_weighting_override=temporal_weighting,\n",
    ")\n",
    "\n",
    "res_no_weight = run_category_featurization(\n",
    "    SUBJECT_SELECTED,\n",
    "    STORY_SELECTED,\n",
    "    cfg_base=cfg,\n",
    "    categories_cfg_base=categories_cfg_base,\n",
    "    cluster_csv_path=cluster_csv_path,\n",
    "    temporal_weighting=temporal_weighting,\n",
    "    prototype_weight_power=prototype_weight_power,\n",
    "    canonical_smoothing_seconds=canonical_smoothing_seconds,\n",
    "    canonical_smoothing_method=canonical_smoothing_method,\n",
    "    canonical_gaussian_sigma_seconds=canonical_gaussian_sigma_seconds,\n",
    "    temporal_weighting_override='none',\n",
    ")\n",
    "\n",
    "result_df = res_temporal['category_df']\n",
    "result_df_raw = res_temporal.get('category_df_raw', result_df)\n",
    "result_df_none = res_no_weight['category_df']\n",
    "result_df_none_raw = res_no_weight.get('category_df_raw', result_df_none)\n",
    "canonical_df_temporal = res_temporal['canonical_df']\n",
    "canonical_df_temporal_raw = res_temporal.get('canonical_df_raw', canonical_df_temporal)\n",
    "canonical_df_none = res_no_weight['canonical_df']\n",
    "canonical_df_none_raw = res_no_weight.get('canonical_df_raw', canonical_df_none)\n",
    "print('Temporal weighting mode:', res_temporal['temporal_weighting'])\n",
    "print('Non-weighted mode:', res_no_weight['temporal_weighting'])\n",
    "print('Temporal smoothing meta:', res_temporal.get('smoothing', {}))\n",
    "print('No-weight smoothing meta:', res_no_weight.get('smoothing', {}))\n",
    "result_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1930fffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize category time series with aligned time axes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "category_cols = [col for col in result_df.columns if col.startswith('cat_')]\n",
    "if not category_cols:\n",
    "    raise ValueError('No category columns found in result_df; run the notebook cell above first.')\n",
    "\n",
    "tr_edges = res_temporal['tr_edges']\n",
    "x_tr = tr_edges[:-1]\n",
    "temporal_smoothing = res_temporal.get('smoothing', {})\n",
    "none_smoothing = res_no_weight.get('smoothing', {})\n",
    "temporal_smoothed = bool(temporal_smoothing.get('applied'))\n",
    "none_smoothed = bool(none_smoothing.get('applied'))\n",
    "\n",
    "if widgets is not None:\n",
    "    cat_selector = widgets.SelectMultiple(options=category_cols, value=tuple(category_cols[: min(3, len(category_cols))]),\n",
    "                                          description='Categories', layout=widgets.Layout(width='50%'))\n",
    "    display(cat_selector)\n",
    "    selected_categories = list(cat_selector.value) or category_cols[: min(3, len(category_cols))]\n",
    "else:\n",
    "    selected_categories = category_cols[: min(3, len(category_cols))]\n",
    "\n",
    "if not selected_categories:\n",
    "    raise ValueError('No categories selected for plotting.')\n",
    "\n",
    "cmap = plt.get_cmap('tab10') if plt is not None else None\n",
    "color_lookup = {cat: cmap(idx % 10) for idx, cat in enumerate(selected_categories)} if cmap is not None else {}\n",
    "color_lookup_none = {cat: cmap((idx + len(selected_categories)) % 10) for idx, cat in enumerate(selected_categories)} if cmap is not None else {}\n",
    "\n",
    "label_temporal = res_temporal['temporal_weighting']\n",
    "label_none = res_no_weight['temporal_weighting']\n",
    "\n",
    "def _smooth_desc(meta: dict) -> str:\n",
    "    if not meta or not meta.get('applied'):\n",
    "        return 'none'\n",
    "    method = meta.get('method', 'unknown')\n",
    "    seconds = meta.get('seconds')\n",
    "    if seconds is None:\n",
    "        return method\n",
    "    return f\"{method} ({seconds}s)\"\n",
    "\n",
    "smooth_desc_temporal = _smooth_desc(temporal_smoothing)\n",
    "smooth_desc_none = _smooth_desc(none_smoothing)\n",
    "\n",
    "# Combined view\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "for cat in selected_categories:\n",
    "    color_main = color_lookup.get(cat)\n",
    "    ax.plot(x_tr, result_df[cat], label=f\"{cat} (TR {label_temporal})\", color=color_main)\n",
    "    if temporal_smoothed and cat in result_df_raw.columns:\n",
    "        ax.plot(x_tr, result_df_raw[cat], linestyle=':', color=color_main, alpha=0.6,\n",
    "                label=f\"{cat} (raw {label_temporal})\")\n",
    "    if result_df_none is not None and cat in result_df_none.columns:\n",
    "        color_alt = color_lookup_none.get(cat)\n",
    "        ax.plot(x_tr, result_df_none[cat], linestyle='--', color=color_alt, alpha=0.85,\n",
    "                label=f\"{cat} (TR {label_none})\")\n",
    "        if none_smoothed and cat in result_df_none_raw.columns:\n",
    "            ax.plot(x_tr, result_df_none_raw[cat], linestyle=':', color=color_alt, alpha=0.5,\n",
    "                    label=f\"{cat} (raw {label_none})\")\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Cosine similarity')\n",
    "ax.set_title(\n",
    "    f'Combined categories for {SUBJECT_SELECTED} / {STORY_SELECTED}\\n'\n",
    "    f'Temporal smoothing: {smooth_desc_temporal} | No-weight smoothing: {smooth_desc_none}'\n",
    ")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='upper right', ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Individual panels\n",
    "fig, axes = plt.subplots(len(selected_categories), 1, figsize=(12, 3 * len(selected_categories)), sharex=True)\n",
    "if len(selected_categories) == 1:\n",
    "    axes = [axes]\n",
    "for ax, cat in zip(axes, selected_categories):\n",
    "    color_main = color_lookup.get(cat)\n",
    "    ax.plot(x_tr, result_df[cat], label=f'TR ({label_temporal})', color=color_main)\n",
    "    if temporal_smoothed and cat in result_df_raw.columns:\n",
    "        ax.plot(x_tr, result_df_raw[cat], label='Raw canonical avg', color=color_main, linestyle=':', alpha=0.6)\n",
    "    if result_df_none is not None and cat in result_df_none.columns:\n",
    "        color_alt = color_lookup_none.get(cat)\n",
    "        ax.plot(x_tr, result_df_none[cat], label=f'TR ({label_none})', color=color_alt, alpha=0.7)\n",
    "        if none_smoothed and cat in result_df_none_raw.columns:\n",
    "            ax.plot(x_tr, result_df_none_raw[cat], label='Raw none avg', color=color_alt, linestyle=':', alpha=0.4)\n",
    "    ax.set_ylabel(cat)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='upper right')\n",
    "axes[-1].set_xlabel('Time (s)')\n",
    "fig.suptitle(f'Category time series (per category) for {SUBJECT_SELECTED} / {STORY_SELECTED}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45c134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect token-level contributions for a selected window\n",
    "def bucket_contributions(result: Dict[str, Any], category: str, index: int, *, use_canonical: bool = False):\n",
    "    buckets = result['canonical_buckets'] if use_canonical else result['tr_buckets']\n",
    "    edges = result['canonical_edges'] if use_canonical else result['tr_edges']\n",
    "    if index < 0 or index >= len(buckets):\n",
    "        raise IndexError('Index out of range for selected timeline.')\n",
    "    state_name = category if category in result['category_states'] else category[4:] if category.startswith('cat_') else category\n",
    "    if state_name not in result['category_states']:\n",
    "        raise KeyError(f'Category {category} not found in category_states')\n",
    "    state = result['category_states'][state_name]\n",
    "    prototype = state.get('prototype')\n",
    "    prototype_norm = state.get('prototype_norm') or 0.0\n",
    "    window_start = edges[index]\n",
    "    window_end = edges[index + 1]\n",
    "    rows = []\n",
    "    for item in buckets[index]:\n",
    "        emb = item.get('embedding')\n",
    "        emb_norm = item.get('embedding_norm') or 0.0\n",
    "        if emb is None or emb_norm < EPS or prototype is None or prototype_norm < EPS:\n",
    "            sim = float('nan')\n",
    "        else:\n",
    "            sim = float(np.dot(emb, prototype) / (emb_norm * prototype_norm))\n",
    "        rows.append({\n",
    "            'word': item.get('word', ''),\n",
    "            'token_start': item.get('token_start'),\n",
    "            'token_end': item.get('token_end'),\n",
    "            'overlap': item.get('overlap'),\n",
    "            'similarity': sim,\n",
    "            'weighted_similarity': sim * item.get('overlap', np.nan) if np.isfinite(sim) else np.nan,\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    if not df.empty:\n",
    "        df.sort_values('weighted_similarity', ascending=False, inplace=True, na_position='last')\n",
    "    mode_label = result['temporal_weighting']\n",
    "    timeline = 'Canonical' if use_canonical else 'TR'\n",
    "    print(f\"Mode: {mode_label} | {timeline} window {index} [{window_start:.2f}, {window_end:.2f}]\")\n",
    "    display(df.head(30))\n",
    "    if not df.empty and np.isfinite(df['weighted_similarity']).any():\n",
    "        print('Sum weighted similarity:', np.nansum(df['weighted_similarity']))\n",
    "\n",
    "if widgets is not None:\n",
    "    mode_toggle = widgets.ToggleButtons(\n",
    "        options=[('Temporal weighting', res_temporal['temporal_weighting']), ('No weighting', res_no_weight['temporal_weighting'])],\n",
    "        description='Mode:',\n",
    "    )\n",
    "    timeline_toggle = widgets.ToggleButtons(options=[('TR', False), ('Seconds', True)], description='Timeline:')\n",
    "    cat_dropdown = widgets.Dropdown(options=res_temporal['category_columns'], description='Category')\n",
    "    tr_slider = widgets.IntSlider(min=0, max=len(res_temporal['tr_buckets']) - 1, value=0, description='TR idx')\n",
    "    canonical_slider = widgets.IntSlider(min=0, max=len(res_temporal['canonical_buckets']) - 1, value=0, description='Sec idx')\n",
    "    display(mode_toggle, timeline_toggle, cat_dropdown, tr_slider, canonical_slider)\n",
    "\n",
    "    def _update(*_):\n",
    "        result = res_temporal if mode_toggle.value == res_temporal['temporal_weighting'] else res_no_weight\n",
    "        if timeline_toggle.value:\n",
    "            bucket_contributions(result, cat_dropdown.value, canonical_slider.value, use_canonical=True)\n",
    "        else:\n",
    "            bucket_contributions(result, cat_dropdown.value, tr_slider.value, use_canonical=False)\n",
    "\n",
    "    for control in (mode_toggle, timeline_toggle, cat_dropdown, tr_slider, canonical_slider):\n",
    "        control.observe(_update, names='value')\n",
    "    _update()\n",
    "else:\n",
    "    print('Widgets unavailable; showing first TR window contributions for temporal weighting.')\n",
    "    bucket_contributions(res_temporal, res_temporal['category_columns'][0], 0, use_canonical=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
