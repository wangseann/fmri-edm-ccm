{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 17 \u2013 Semantic Category Featurization\n",
    "\n",
    "Generate TR-aligned semantic category trajectories for each story, with optional concatenation and Day16 integration hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "np.random.seed(42)\n",
    "pd.options.display.max_columns = 50\n",
    "\n",
    "project_root = Path('/flash/PaoU/seann/fmri-edm-ccm')\n",
    "project_root.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(project_root)\n",
    "\n",
    "sys.path.append(str(project_root))\n",
    "sys.path.append('/flash/PaoU/seann/pyEDM/src')\n",
    "sys.path.append('/flash/PaoU/seann/MDE-main/src')\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    widgets = None\n",
    "    def display(obj):\n",
    "        print(obj)\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except Exception as exc:\n",
    "    plt = None\n",
    "    warnings.warn(f'Matplotlib unavailable: {exc}')\n",
    "\n",
    "EPS = 1e-12  # numeric guard\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_yaml\n",
    "from src.decoding import load_transcript_words\n",
    "from src.edm_ccm import English1000Loader\n",
    "\n",
    "cfg = load_yaml('configs/demo.yaml')\n",
    "categories_cfg = cfg.get('categories')\n",
    "default_categories_cfg = {\n",
    "    'embedding_source': 'english1000',\n",
    "    'word2vec_path': '',\n",
    "    'concat_all_stories': False,\n",
    "    'category_score_method': 'similarity',\n",
    "    'overlap_weighting': 'proportional',\n",
    "    'allow_single_seed': False,\n",
    "    'seconds_bin_width': 0.05,\n",
    "    'expansion': {\n",
    "        'enabled': True,\n",
    "        'top_k': 2000,\n",
    "        'min_sim': 0.35,\n",
    "    },\n",
    "    'sets': {\n",
    "        'basic_semantics': {\n",
    "            'inanimate': {'seeds': ['rock', 'table', 'cup', 'chair']},\n",
    "            'animate': {'seeds': ['man', 'woman', 'dog', 'child']},\n",
    "            'place': {'seeds': ['city', 'park', 'room', 'kitchen']},\n",
    "            'tool': {'seeds': ['hammer', 'knife', 'saw', 'scissors']},\n",
    "            'motion': {'seeds': ['run', 'walk', 'drive', 'fly']},\n",
    "            'emotion_pos': {'seeds': ['happy', 'joy', 'delight', 'pleased']},\n",
    "            'emotion_neg': {'seeds': ['sad', 'angry', 'fear', 'disgust']},\n",
    "        },\n",
    "    },\n",
    "}\n",
    "if categories_cfg is None:\n",
    "    categories_cfg = default_categories_cfg\n",
    "    print('No categories block found in config; using default template.')\n",
    "else:\n",
    "    categories_cfg = json.loads(json.dumps(categories_cfg))\n",
    "\n",
    "\n",
    "cluster_csv_path = (categories_cfg or {}).get('cluster_csv_path', '')  # CSV of clusters\n",
    "temporal_weighting = str((categories_cfg or {}).get('temporal_weighting', 'proportional')).lower()\n",
    "assert temporal_weighting in {'proportional', 'none'}, \"temporal_weighting must be 'proportional' or 'none'.\"\n",
    "prototype_weight_power = float((categories_cfg or {}).get('prototype_weight_power', 1.0))  # weight shaping\n",
    "SUBJECT = cfg.get('subject')\n",
    "STORY = cfg.get('story')\n",
    "paths = cfg.get('paths', {})\n",
    "TR = float(cfg.get('TR', 2.0))\n",
    "\n",
    "features_root = Path(paths.get('features', 'features'))  # features root\n",
    "features_root.mkdir(parents=True, exist_ok=True)\n",
    "if not paths.get('features') and Path('figs').exists():\n",
    "    warnings.warn(\"Using default 'features/' directory. Update configs/demo.yaml with paths.features.\")  # config hint\n",
    "output_root = features_root / 'subjects' / SUBJECT / STORY / 'day17_categories'\n",
    "output_root.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Writing outputs under {features_root}/subjects/{SUBJECT}/{STORY}/day17_categories\")  # location notice\n",
    "print(f'Output directory: {output_root}')\n",
    "\n",
    "category_sets = categories_cfg.get('sets', {})\n",
    "available_sets = sorted(category_sets.keys())\n",
    "category_set_name = categories_cfg.get('category_set') or (available_sets[0] if available_sets else None)\n",
    "if cluster_csv_path and not category_set_name:\n",
    "    category_set_name = 'csv_clusters'  # default label for CSV clusters\n",
    "    categories_cfg['category_set'] = category_set_name\n",
    "if cluster_csv_path:\n",
    "    categories_cfg.setdefault('sets', {})\n",
    "    categories_cfg['sets'].setdefault(category_set_name, {})\n",
    "    categories_cfg['category_score_method'] = 'similarity'\n",
    "    categories_cfg['allow_single_seed'] = True\n",
    "    categories_cfg['expansion'] = {'enabled': False}\n",
    "if cluster_csv_path:\n",
    "    # CSV-driven categories do not require seed sets\n",
    "    selected_set_spec = category_sets.get(category_set_name, {})\n",
    "    if category_set_name:\n",
    "        print(f'Using category set: {category_set_name}')\n",
    "else:\n",
    "    if widgets is not None and available_sets:\n",
    "        set_selector = widgets.Dropdown(options=available_sets, value=category_set_name, description='Category Set')\n",
    "        display(set_selector)\n",
    "        category_set_name = set_selector.value\n",
    "    print(f'Using category set: {category_set_name}')\n",
    "    assert category_set_name in category_sets, 'Selected category set not available in configuration.'\n",
    "    selected_set_spec = category_sets[category_set_name]\n",
    "category_score_method = str(categories_cfg.get('category_score_method', 'similarity')).lower()\n",
    "overlap_mode = str(categories_cfg.get('overlap_weighting', 'proportional')).lower()\n",
    "expansion_cfg = categories_cfg.get('expansion', {})\n",
    "config_used_path = output_root / 'config_used.yaml'\n",
    "with config_used_path.open('w') as fh:\n",
    "    yaml.safe_dump({\n",
    "        **categories_cfg,\n",
    "        'category_set': category_set_name,\n",
    "        'cluster_csv_path': cluster_csv_path,\n",
    "        'temporal_weighting': temporal_weighting,\n",
    "        'prototype_weight_power': prototype_weight_power,\n",
    "    }, fh, sort_keys=False)\n",
    "print(f'Wrote configuration snapshot to {config_used_path}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def load_story_words(paths: Dict, subject: str, story: str) -> List[Tuple[str, float, float]]:\n",
    "    \"\"\"Wrapper around load_transcript_words with sanity checks.\"\"\"\n",
    "    events = load_transcript_words(paths, subject, story)\n",
    "    if not events:\n",
    "        raise ValueError(f'No transcript events found for {subject} {story}.')\n",
    "    return [(str(word).strip(), float(start), float(end)) for word, start, end in events]\n",
    "\n",
    "\n",
    "# === NEW: load cluster words from CSV (category, word, [weight]) ===\n",
    "def load_clusters_from_csv(csv_path: str) -> Dict[str, Dict[str, List[Tuple[str, float]]]]:\n",
    "    from pathlib import Path  # local import to avoid global dependency\n",
    "    if not csv_path or not Path(csv_path).exists():\n",
    "        raise FileNotFoundError(f'Cluster CSV not found at {csv_path}')\n",
    "    df = pd.read_csv(csv_path)\n",
    "    cols = {c.lower().strip(): c for c in df.columns}\n",
    "    for needed in ('category', 'word'):\n",
    "        assert needed in cols, f\"CSV must contain '{needed}' column.\"\n",
    "    cat_col = cols['category']\n",
    "    word_col = cols['word']\n",
    "    weight_col = cols.get('weight')\n",
    "    if weight_col is None:\n",
    "        df['_weight'] = 1.0\n",
    "        weight_col = '_weight'\n",
    "    df = df[[cat_col, word_col, weight_col]].copy()\n",
    "    df[word_col] = df[word_col].astype(str).str.strip().str.lower()\n",
    "    df[cat_col] = df[cat_col].astype(str).str.strip().str.lower()\n",
    "    df[weight_col] = pd.to_numeric(df[weight_col], errors='coerce').fillna(1.0).clip(lower=0.0)\n",
    "    clusters: Dict[str, Dict[str, List[Tuple[str, float]]]] = {}\n",
    "    clusters: Dict[str, Dict[str, List[Tuple[str, float]]]] = {}\n",
    "    for cat, sub in df.groupby(cat_col):\n",
    "        bucket: Dict[str, float] = {}\n",
    "        for w, wt in zip(sub[word_col].tolist(), sub[weight_col].tolist()):\n",
    "            if not w:\n",
    "                continue\n",
    "            bucket[w] = float(wt)\n",
    "        pairs = sorted(bucket.items())\n",
    "        if pairs:\n",
    "            clusters[cat] = {'words': pairs}\n",
    "    if not clusters:\n",
    "        raise ValueError('No clusters parsed from CSV.')\n",
    "    return clusters\n",
    "\n",
    "# === NEW: build category prototypes from representative words (CSV) ===\n",
    "def build_states_from_csv(\n",
    "    clusters: Dict[str, Dict[str, List[Tuple[str, float]]]],\n",
    "    primary_lookup: Dict[str, np.ndarray],\n",
    "    fallback=None,\n",
    "    weight_power: float = 1.0\n",
    ") -> Tuple[Dict[str, Dict], Dict[str, Dict]]:\n",
    "    category_states: Dict[str, Dict] = {}\n",
    "    category_definitions: Dict[str, Dict] = {}\n",
    "    oov_counts: Dict[str, int] = {}\n",
    "    for cat, spec in clusters.items():\n",
    "        pairs = spec.get('words', [])\n",
    "        vecs: List[np.ndarray] = []\n",
    "        weights: List[float] = []\n",
    "        found_words: List[str] = []\n",
    "        missing_words: List[str] = []\n",
    "        for word, wt in pairs:\n",
    "            vec = lookup_embedding(word, primary_lookup, fallback)\n",
    "            if vec is None:\n",
    "                missing_words.append(word)\n",
    "                continue\n",
    "            vecs.append(vec.astype(float))\n",
    "            weights.append(float(max(0.0, wt)) ** float(weight_power))\n",
    "            found_words.append(word)\n",
    "        if not vecs:\n",
    "            warnings.warn(f\"[{cat}] no usable representative embeddings; prototype will be None.\")  # centroid warning\n",
    "            prototype = None\n",
    "            prototype_norm = None\n",
    "        else:\n",
    "            W = np.array(weights, dtype=float)\n",
    "            W = W / (W.sum() + 1e-12)\n",
    "            M = np.stack(vecs, axis=0)\n",
    "            prototype = (W[:, None] * M).sum(axis=0)\n",
    "            prototype_norm = float(np.linalg.norm(prototype))\n",
    "            if prototype_norm < EPS:\n",
    "                prototype = None\n",
    "                prototype_norm = None\n",
    "        rep_lex = {word: float(wt) for word, wt in pairs}\n",
    "        category_states[cat] = {\n",
    "            'name': cat,\n",
    "            'seeds': [],  # seeds unused in CSV mode\n",
    "            'found_seeds': found_words,\n",
    "            'missing_seeds': missing_words,\n",
    "            'prototype': prototype,\n",
    "            'prototype_norm': prototype_norm,\n",
    "            'lexicon': rep_lex,  # metadata/debug only\n",
    "            'expanded_count': 0,\n",
    "            'expansion_params': {'enabled': False, 'top_k': 0, 'min_sim': 0.0},\n",
    "        }\n",
    "        category_definitions[cat] = {\n",
    "        'from': 'csv',\n",
    "        'seeds': [],\n",
    "        'found_seeds': found_words,\n",
    "        'missing_seeds': missing_words,\n",
    "        'prototype_dim': int(prototype.shape[0]) if isinstance(prototype, np.ndarray) else 0,\n",
    "        'prototype_norm': prototype_norm,\n",
    "        'representative_words': rep_lex,\n",
    "        'lexicon': rep_lex,\n",
    "        'expanded_neighbors': {},\n",
    "        }\n",
    "        oov_counts[cat] = len(missing_words)\n",
    "    if any(oov_counts.values()):\n",
    "        warnings.warn(f\"OOV representative words: {oov_counts}\")\n",
    "    return category_states, category_definitions\n",
    "\n",
    "def build_tr_edges(word_events: Sequence[Tuple[str, float, float]], tr_s: float) -> np.ndarray:\n",
    "    \"\"\"Compute TR edges covering the full duration of the transcript.\"\"\"\n",
    "    if not word_events:\n",
    "        return np.arange(0, tr_s, tr_s)\n",
    "    max_end = max(end for _, _, end in word_events)\n",
    "    n_tr = max(1, int(math.ceil(max_end / tr_s)))\n",
    "    edges = np.arange(0.0, (n_tr + 1) * tr_s, tr_s, dtype=float)\n",
    "    if edges[-1] < max_end:\n",
    "        edges = np.append(edges, edges[-1] + tr_s)\n",
    "    if edges[-1] < max_end - 1e-9:\n",
    "        edges = np.append(edges, edges[-1] + tr_s)  # ensure coverage\n",
    "    return edges\n",
    "\n",
    "\n",
    "def lookup_embedding(token: str, primary_lookup: Dict[str, np.ndarray], fallback=None) -> Optional[np.ndarray]:\n",
    "    key = token.lower().strip()\n",
    "    if not key:\n",
    "        return None\n",
    "    vec = primary_lookup.get(key) if primary_lookup else None\n",
    "    if vec is not None:\n",
    "        return np.asarray(vec, dtype=float)\n",
    "    if fallback is not None:\n",
    "        try:\n",
    "            if hasattr(fallback, 'get_vector') and key in fallback:\n",
    "                return np.asarray(fallback.get_vector(key), dtype=float)\n",
    "            if hasattr(fallback, '__contains__') and key in fallback:\n",
    "                return np.asarray(fallback[key], dtype=float)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def make_category_prototype(seeds: Sequence[str], primary_lookup: Dict[str, np.ndarray], fallback=None, allow_single: bool = False) -> Tuple[Optional[np.ndarray], List[str], List[str]]:\n",
    "    found_vectors = []\n",
    "    found_words = []\n",
    "    missing_words = []\n",
    "    for seed in seeds:\n",
    "        vec = lookup_embedding(seed, primary_lookup, fallback)\n",
    "        if vec is None:\n",
    "            missing_words.append(seed)\n",
    "            continue\n",
    "        found_vectors.append(vec)\n",
    "        found_words.append(seed)\n",
    "    if not found_vectors:\n",
    "        return None, found_words, missing_words\n",
    "    if len(found_vectors) < 2 and not allow_single:\n",
    "        warnings.warn(f'Only {len(found_vectors)} usable seed(s); enable allow_single_seed to accept singleton prototypes.')\n",
    "        if not allow_single:\n",
    "            return None, found_words, missing_words\n",
    "    prototype = np.mean(found_vectors, axis=0)\n",
    "    return prototype, found_words, missing_words\n",
    "\n",
    "\n",
    "def expand_category(prototype: np.ndarray, vocab_embeddings: np.ndarray, vocab_words: Sequence[str], top_k: int, min_sim: float) -> Dict[str, float]:\n",
    "    if prototype is None or vocab_embeddings is None or vocab_words is None:\n",
    "        return {}\n",
    "    proto = np.asarray(prototype, dtype=float)\n",
    "    proto_norm = np.linalg.norm(proto)\n",
    "    if proto_norm == 0:\n",
    "        return {}\n",
    "    proto_unit = proto / proto_norm\n",
    "    vocab_norms = np.linalg.norm(vocab_embeddings, axis=1)\n",
    "    valid_mask = vocab_norms > 0\n",
    "    sims = np.full(vocab_embeddings.shape[0], -1.0, dtype=float)\n",
    "    sims[valid_mask] = (vocab_embeddings[valid_mask] @ proto_unit) / vocab_norms[valid_mask]\n",
    "    top_k_eff = min(top_k, len(sims))\n",
    "    if top_k_eff <= 0:\n",
    "        return {}\n",
    "    candidate_idx = np.argpartition(-sims, top_k_eff - 1)[:top_k_eff]\n",
    "    out = {}\n",
    "    for idx in candidate_idx:\n",
    "        score = float(sims[idx])\n",
    "        if score < min_sim:\n",
    "            continue\n",
    "        out[vocab_words[idx]] = score\n",
    "    return out\n",
    "\n",
    "\n",
    "def tr_token_overlap(token_start: float, token_end: float, tr_start: float, tr_end: float, mode: str = 'proportional') -> float:\n",
    "    token_start = float(token_start)\n",
    "    token_end = float(token_end)\n",
    "    if token_end <= token_start:\n",
    "        token_end = token_start + 1e-3\n",
    "    if mode == 'midpoint':\n",
    "        midpoint = 0.5 * (token_start + token_end)\n",
    "        return 1.0 if tr_start <= midpoint < tr_end else 0.0\n",
    "    overlap = max(0.0, min(token_end, tr_end) - max(token_start, tr_start))\n",
    "    duration = token_end - token_start\n",
    "    if duration <= 0:\n",
    "        return 1.0 if overlap > 0 else 0.0\n",
    "    return max(0.0, min(1.0, overlap / duration))\n",
    "\n",
    "\n",
    "def score_tr(token_payload: Sequence[Dict], method: str, *, lexicon: Optional[Dict[str, float]] = None, prototype: Optional[np.ndarray] = None, prototype_norm: Optional[float] = None) -> float:\n",
    "    if not token_payload:\n",
    "        return float('nan')\n",
    "    method = method.lower()\n",
    "    if method == 'count':\n",
    "        if not lexicon:\n",
    "            return float('nan')\n",
    "        total = 0.0\n",
    "        for item in token_payload:\n",
    "            weight = lexicon.get(item['word'].lower())\n",
    "            if weight is None:\n",
    "                continue\n",
    "            total += weight * item['overlap']\n",
    "        return float(total)\n",
    "    if method == 'similarity':\n",
    "        if prototype is None or prototype_norm is None or prototype_norm < EPS:\n",
    "            return float('nan')  # numeric guard\n",
    "        num = 0.0\n",
    "        denom = 0.0\n",
    "        for item in token_payload:\n",
    "            emb = item.get('embedding')\n",
    "            if emb is None:\n",
    "                continue\n",
    "            emb_norm = item.get('embedding_norm')\n",
    "            if emb_norm is None or emb_norm < EPS:\n",
    "                continue  # numeric guard\n",
    "            sim = float(np.dot(emb, prototype) / (emb_norm * prototype_norm))\n",
    "            num += sim * item['overlap']\n",
    "            denom += item['overlap']\n",
    "        if denom == 0:\n",
    "            return float('nan')\n",
    "        value = num / denom\n",
    "        return float(np.clip(value, -1.0, 1.0))\n",
    "    raise ValueError(f'Unknown scoring method: {method}')\n",
    "\n",
    "\n",
    "def ensure_serializable(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, (np.floating, np.integer)):\n",
    "        return obj.item()\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: ensure_serializable(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [ensure_serializable(v) for v in obj]\n",
    "    return obj\n",
    "\n",
    "\n",
    "def discover_story_dirs(features_root: Path, subject: str) -> List[Path]:\n",
    "    base = features_root / 'subjects' / subject  # subject root\n",
    "    if not base.exists():\n",
    "        return []\n",
    "    candidates = []\n",
    "    for child in base.iterdir():\n",
    "        if not child.is_dir() or child.name == 'ALL':\n",
    "            continue\n",
    "        ts_path = child / 'day17_categories' / 'category_timeseries.csv'\n",
    "        if ts_path.exists():\n",
    "            candidates.append(child)\n",
    "    return sorted(candidates)\n",
    "\n",
    "\n",
    "def build_token_buckets(edges: np.ndarray, event_records: Sequence[Dict], mode: str = 'proportional') -> List[List[Dict]]:\n",
    "    if edges.size < 2:\n",
    "        return []\n",
    "    buckets: List[List[Dict]] = [[] for _ in range(len(edges) - 1)]\n",
    "    for rec in event_records:\n",
    "        start = rec['start']\n",
    "        end = rec['end']\n",
    "        if end <= edges[0] or start >= edges[-1]:\n",
    "            continue\n",
    "        start_idx = max(0, int(np.searchsorted(edges, start, side='right')) - 1)\n",
    "        end_idx = max(0, int(np.searchsorted(edges, end, side='left')))\n",
    "        end_idx = min(end_idx, len(buckets) - 1)\n",
    "        for idx in range(start_idx, end_idx + 1):\n",
    "            bucket_start = edges[idx]\n",
    "            bucket_end = edges[idx + 1]\n",
    "            if mode == 'none':\n",
    "                overlap = 1.0 if not (end <= bucket_start or start >= bucket_end) else 0.0\n",
    "            else:\n",
    "                overlap = tr_token_overlap(start, end, bucket_start, bucket_end, 'proportional')\n",
    "            if overlap <= 0:\n",
    "                continue\n",
    "            buckets[idx].append({\n",
    "                'word': rec['word'],\n",
    "                'overlap': overlap,\n",
    "                'embedding': rec['embedding'],\n",
    "                'embedding_norm': rec['embedding_norm'],\n",
    "            })\n",
    "    return buckets\n",
    "\n",
    "def score_time_series(edges: np.ndarray, buckets: Sequence[Sequence[Dict]], category_states: Dict[str, Dict], category_names: Sequence[str], category_columns: Sequence[str], method: str, index_name: str) -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "    n_bins = len(buckets)\n",
    "    score_matrix = np.full((n_bins, len(category_names)), np.nan, dtype=float)\n",
    "    for col_idx, cat_name in enumerate(category_names):\n",
    "        state = category_states[cat_name]\n",
    "        lexicon = state['lexicon']\n",
    "        prototype = state['prototype']\n",
    "        prototype_norm = state['prototype_norm']\n",
    "        for bin_idx, bucket in enumerate(buckets):\n",
    "            score_matrix[bin_idx, col_idx] = score_tr(bucket, method, lexicon=lexicon, prototype=prototype, prototype_norm=prototype_norm)\n",
    "    data = {\n",
    "        index_name: np.arange(n_bins, dtype=int),\n",
    "        'start_sec': edges[:-1],\n",
    "        'end_sec': edges[1:],\n",
    "    }\n",
    "    for col_idx, col in enumerate(category_columns):\n",
    "        data[col] = score_matrix[:, col_idx]\n",
    "    df = pd.DataFrame(data)\n",
    "    return df, score_matrix\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transcript and build TR grid\n",
    "story_events = load_story_words(paths, SUBJECT, STORY)\n",
    "print(f'Loaded {len(story_events)} word events for {SUBJECT} {STORY}.')\n",
    "tr_edges = build_tr_edges(story_events, TR)\n",
    "n_tr = len(tr_edges) - 1\n",
    "print(f'TR edges: {len(tr_edges)} (n_tr={n_tr}) spanning {tr_edges[-1]:.2f} seconds.')\n",
    "assert n_tr > 0, 'No TRs derived from transcript timing.'\n",
    "assert np.all(np.diff(tr_edges) > 0), 'Non-monotone TR edges.'  # hard check\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding resources\n",
    "embedding_source = str(categories_cfg.get('embedding_source', 'english1000')).lower()\n",
    "english_loader = None\n",
    "english_lookup = {}\n",
    "english_vocab = []\n",
    "english_matrix = None\n",
    "if embedding_source in {'english1000', 'both'}:\n",
    "    english1000_path = Path(paths.get('data_root', '')) / 'derivative' / 'english1000sm.hf5'\n",
    "    if english1000_path.exists():\n",
    "        english_loader = English1000Loader(english1000_path)\n",
    "        english_lookup = english_loader.lookup\n",
    "        english_vocab = english_loader.vocab\n",
    "        english_matrix = english_loader.embeddings\n",
    "        print(f'Loaded English1000 embeddings from {english1000_path} (vocab={len(english_vocab)}).')\n",
    "    else:\n",
    "        raise FileNotFoundError(f'English1000 embeddings not found at {english1000_path}')\n",
    "else:\n",
    "    print('English1000 disabled by configuration.')\n",
    "\n",
    "word2vec_model = None\n",
    "if embedding_source in {'word2vec', 'both'}:\n",
    "    w2v_path = categories_cfg.get('word2vec_path')\n",
    "    if w2v_path:\n",
    "        w2v_path = Path(w2v_path)\n",
    "        if w2v_path.exists():\n",
    "            try:\n",
    "                from gensim.models import KeyedVectors\n",
    "                binary = w2v_path.suffix.lower() in {'.bin', '.gz'}\n",
    "                word2vec_model = KeyedVectors.load_word2vec_format(w2v_path, binary=binary)\n",
    "                print(f'Loaded Word2Vec fallback from {w2v_path}.')\n",
    "            except Exception as exc:\n",
    "                warnings.warn(f'Failed to load Word2Vec fallback: {exc}')\n",
    "        else:\n",
    "            warnings.warn(f'Word2Vec path does not exist: {w2v_path}')\n",
    "    else:\n",
    "        warnings.warn('Word2Vec fallback requested but no path provided.')\n",
    "else:\n",
    "    print('Word2Vec fallback disabled.')\n",
    "if embedding_source == 'word2vec' and expansion_cfg.get('enabled', True) and english_matrix is not None:\n",
    "    warnings.warn(\"Category expansion used English1000 while token embeddings use word2vec; cosine geometry may be inconsistent.\")  # geometry notice\n",
    "\n",
    "primary_lookup = english_lookup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build category prototypes and lexicons\n",
    "if 'set_selector' in globals() and hasattr(set_selector, 'value'):\n",
    "    category_set_name = set_selector.value\n",
    "selected_set_spec = category_sets.get(category_set_name, {})\n",
    "\n",
    "allow_single = bool(categories_cfg.get('allow_single_seed', False))\n",
    "exp_enabled = bool(expansion_cfg.get('enabled', True))\n",
    "exp_top_k = int(expansion_cfg.get('top_k', 2000))\n",
    "exp_min_sim = float(expansion_cfg.get('min_sim', 0.35))\n",
    "\n",
    "if cluster_csv_path:\n",
    "    category_score_method = 'similarity'\n",
    "    category_states, category_definitions = build_states_from_csv(\n",
    "        load_clusters_from_csv(cluster_csv_path),\n",
    "        primary_lookup,\n",
    "        word2vec_model,\n",
    "        weight_power=prototype_weight_power,\n",
    "    )\n",
    "    category_names = sorted(category_states.keys())\n",
    "    category_columns = [f'cat_{name}' for name in category_names]\n",
    "    print(f\"Loaded {len(category_names)} CSV-driven categories from {cluster_csv_path}: {category_names}\")\n",
    "    zero_norm = [k for k, v in category_states.items() if v.get('prototype') is not None and (v.get('prototype_norm') or 0.0) < EPS]\n",
    "    if zero_norm:\n",
    "        warnings.warn(f\"Zero-norm prototypes (check OOV/weights): {zero_norm}\")\n",
    "else:\n",
    "    category_states: Dict[str, Dict] = {}\n",
    "    category_definitions: Dict[str, Dict] = {}\n",
    "    seed_oov_counter = Counter()\n",
    "    for cat_name, cat_spec in selected_set_spec.items():\n",
    "        seeds = cat_spec.get('seeds', [])\n",
    "        explicit_words = cat_spec.get('words', [])\n",
    "        prototype = None\n",
    "        found_seeds: List[str] = []\n",
    "        missing_seeds: List[str] = []\n",
    "        if seeds:\n",
    "            prototype, found_seeds, missing_seeds = make_category_prototype(seeds, primary_lookup, word2vec_model, allow_single)\n",
    "            seed_oov_counter[cat_name] = len(missing_seeds)\n",
    "            if prototype is None and category_score_method == 'similarity':\n",
    "                warnings.warn(f\"Category '{cat_name}' has no usable prototype; TR scores will be NaN.\")  # warn missing proto\n",
    "        elif category_score_method == 'similarity':\n",
    "            warnings.warn(f'Category {cat_name} has no seeds; similarity method will yield NaNs.')\n",
    "        lexicon = {word.lower(): 1.0 for word in explicit_words}\n",
    "        for seed in found_seeds:\n",
    "            lexicon.setdefault(seed.lower(), 1.0)\n",
    "        prototype_norm = None\n",
    "        expanded_words = {}\n",
    "        if prototype is not None:\n",
    "            prototype_norm = float(np.linalg.norm(prototype))\n",
    "            if exp_enabled and english_matrix is not None:\n",
    "                expanded_words = expand_category(prototype, english_matrix, english_vocab, exp_top_k, exp_min_sim)\n",
    "                for word, weight in expanded_words.items():\n",
    "                    lexicon.setdefault(word.lower(), float(weight))\n",
    "        if not lexicon and category_score_method == 'count':\n",
    "            warnings.warn(f'Category {cat_name} lexicon is empty; counts will be NaN.')\n",
    "        category_states[cat_name] = {\n",
    "            'name': cat_name,\n",
    "            'seeds': seeds,\n",
    "            'found_seeds': found_seeds,\n",
    "            'missing_seeds': missing_seeds,\n",
    "            'prototype': prototype,\n",
    "            'prototype_norm': prototype_norm,\n",
    "            'lexicon': lexicon,\n",
    "            'expanded_count': len(expanded_words),\n",
    "            'expansion_params': {\n",
    "                'enabled': exp_enabled,\n",
    "                'top_k': exp_top_k,\n",
    "                'min_sim': exp_min_sim,\n",
    "            },\n",
    "        }\n",
    "        category_definitions[cat_name] = {\n",
    "            'seeds': seeds,\n",
    "            'found_seeds': found_seeds,\n",
    "            'missing_seeds': missing_seeds,\n",
    "            'prototype_dim': int(prototype.shape[0]) if isinstance(prototype, np.ndarray) else 0,\n",
    "            'prototype_norm': prototype_norm,\n",
    "            'expanded_neighbors': ensure_serializable(expanded_words),\n",
    "            'lexicon': {word: float(weight) for word, weight in sorted(category_states[cat_name]['lexicon'].items())},\n",
    "        }\n",
    "    if exp_enabled:\n",
    "        for _cn, _st in category_states.items():\n",
    "            if _st.get('seeds') and _st.get('expanded_count', 0) == 0:\n",
    "                warnings.warn(f\"No neighbors met min_sim for '{_cn}' (min_sim={exp_min_sim}).\")  # expansion check\n",
    "    print('Category seeds missing counts:', dict(seed_oov_counter))\n",
    "    category_names = sorted(category_states.keys())\n",
    "    category_columns = [f'cat_{name}' for name in category_names]\n",
    "    print(f'Prepared {len(category_names)} categories: {category_names}')\n",
    "\n",
    "tag_label = '[huth2016_12]' if cluster_csv_path else '[categories]'\n",
    "for _cn, _st in category_states.items():\n",
    "    if _st.get('prototype') is None or (_st.get('prototype_norm') or 0.0) < EPS:\n",
    "        warnings.warn(f\"{tag_label} Cluster '{_cn}' has no valid centroid (all seeds OOV?). Scores will be NaN.\")\n",
    "    print(f\"{tag_label} {_cn}: {len(_st.get('found_seeds', []))} in-vocab / {len(_st.get('missing_seeds', []))} OOV\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare continuous (story-timebase) category series\n",
    "seconds_bin_width = float(categories_cfg.get('seconds_bin_width', 0.05))\n",
    "if seconds_bin_width <= 0:\n",
    "    raise ValueError('seconds_bin_width must be positive.')\n",
    "\n",
    "embedding_cache: Dict[str, Optional[np.ndarray]] = {}\n",
    "event_records: List[Dict] = []\n",
    "tokens_with_embeddings = 0\n",
    "for word, onset, offset in story_events:\n",
    "    token = word.strip()\n",
    "    if not token:\n",
    "        continue\n",
    "    key = token.lower()\n",
    "    if key not in embedding_cache:\n",
    "        embedding_cache[key] = lookup_embedding(token, primary_lookup, word2vec_model)\n",
    "    emb = embedding_cache[key]\n",
    "    emb_norm = float(np.linalg.norm(emb)) if emb is not None else None\n",
    "    if emb is not None:\n",
    "        tokens_with_embeddings += 1\n",
    "    event_records.append({\n",
    "        'word': token,\n",
    "        'start': float(onset),\n",
    "        'end': float(offset),\n",
    "        'embedding': emb,\n",
    "        'embedding_norm': emb_norm,\n",
    "    })\n",
    "\n",
    "total_tokens = len(event_records)\n",
    "print(f'Tokens with embeddings: {tokens_with_embeddings}/{total_tokens} (OOV rate={(total_tokens - tokens_with_embeddings) / max(total_tokens, 1):.2%}).')\n",
    "\n",
    "if not event_records:\n",
    "    raise ValueError('No token events available for category featurization.')\n",
    "max_end_time = max(rec['end'] for rec in event_records)\n",
    "canonical_edges = np.arange(0.0, max_end_time + seconds_bin_width, seconds_bin_width, dtype=float)\n",
    "if canonical_edges[-1] < max_end_time:\n",
    "    canonical_edges = np.append(canonical_edges, canonical_edges[-1] + seconds_bin_width)\n",
    "assert np.all(np.diff(canonical_edges) > 0), 'Non-monotone canonical edges.'  # hard check\n",
    "\n",
    "canonical_buckets = build_token_buckets(canonical_edges, event_records, temporal_weighting)\n",
    "empty_canonical = sum(1 for bucket in canonical_buckets if not bucket)\n",
    "print(f'Canonical bins without tokens: {empty_canonical}/{len(canonical_buckets)}')\n",
    "\n",
    "canonical_df, canonical_matrix = score_time_series(canonical_edges, canonical_buckets, category_states, category_names, category_columns, category_score_method, index_name='bin_index')\n",
    "canonical_root = features_root / 'stories' / STORY / 'day17_categories'  # story-level features\n",
    "canonical_root.mkdir(parents=True, exist_ok=True)\n",
    "canonical_csv_path = canonical_root / 'category_timeseries_seconds.csv'\n",
    "canonical_df.to_csv(canonical_csv_path, index=False)\n",
    "print(f'Saved canonical story time series to {canonical_csv_path}')\n",
    "\n",
    "canonical_definition_path = canonical_root / 'category_definition.json'\n",
    "with canonical_definition_path.open('w') as fh:\n",
    "    json.dump(ensure_serializable(category_definitions), fh, indent=2)\n",
    "\n",
    "canonical_config_path = canonical_root / 'config_used.yaml'\n",
    "with canonical_config_path.open('w') as fh:\n",
    "    yaml.safe_dump({**categories_cfg, 'category_set': category_set_name}, fh, sort_keys=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project canonical series onto subject-specific TR grid\n",
    "tr_buckets = build_token_buckets(tr_edges, event_records, temporal_weighting)\n",
    "empty_tr = sum(1 for bucket in tr_buckets if not bucket)\n",
    "print(f'TRs without tokens: {empty_tr}/{len(tr_buckets)}')\n",
    "\n",
    "category_df, score_matrix = score_time_series(tr_edges, tr_buckets, category_states, category_names, category_columns, category_score_method, index_name='tr_index')\n",
    "print(category_df.head())\n",
    "if category_score_method == 'similarity':\n",
    "    nonempty_mask = np.array([len(b) > 0 for b in tr_buckets])\n",
    "    if nonempty_mask.any():\n",
    "        has_finite = np.isfinite(category_df.loc[nonempty_mask, category_columns].to_numpy()).any()\n",
    "        if not has_finite:\n",
    "            raise RuntimeError('All similarity scores are NaN despite non-empty TR buckets.')  # hard check\n",
    "assert len(category_df) == len(tr_buckets), 'Category dataframe row mismatch.'\n",
    "assert len(category_columns) == len(category_names), 'Category column mismatch.'\n",
    "if category_score_method == 'similarity':\n",
    "    finite_vals = category_df[category_columns].to_numpy().astype(float)\n",
    "    finite_vals = finite_vals[np.isfinite(finite_vals)]\n",
    "    if finite_vals.size:\n",
    "        assert np.nanmin(finite_vals) >= -1.0001 and np.nanmax(finite_vals) <= 1.0001, 'Similarity scores out of bounds.'\n",
    "else:\n",
    "    assert (category_df[category_columns].fillna(0.0) >= -1e-9).all().all(), 'Count scores must be non-negative.'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save per-story outputs and metadata\n",
    "category_csv_path = output_root / 'category_timeseries.csv'\n",
    "category_df.to_csv(category_csv_path, index=False)\n",
    "print(f'Saved category time series to {category_csv_path}')\n",
    "definition_path = output_root / 'category_definition.json'\n",
    "with definition_path.open('w') as fh:\n",
    "    json.dump(ensure_serializable(category_definitions), fh, indent=2)\n",
    "print(f'Saved category definitions to {definition_path}')\n",
    "print(f'Canonical assets available at {canonical_root}')\n",
    "\n",
    "trimmed_path = Path(paths.get('figs', 'figs')) / SUBJECT / STORY / 'day16_decoding' / 'semantic_pcs_trimmed.csv'\n",
    "max_lag_primary = 0\n",
    "day16_trim = None\n",
    "if trimmed_path.exists():\n",
    "    day16_trim = pd.read_csv(trimmed_path)\n",
    "    expected_len = len(day16_trim)\n",
    "    if len(day16_trim) > len(category_df):\n",
    "        raise ValueError('Day16 trimmed series longer than category series; regenerate Day16 or rerun Day17.')  # hard check\n",
    "    max_lag_primary = max(0, len(category_df) - expected_len)\n",
    "    print(f'Aligning with Day16 trim (max_lag_primary={max_lag_primary}).')\n",
    "else:\n",
    "    tau_grid = [1, 2]\n",
    "    E_cap = 6\n",
    "    max_tau = max(tau_grid)\n",
    "    max_lag_primary = max_tau * (E_cap - 1)\n",
    "    max_lag_primary = min(max_lag_primary, len(category_df) - 1)\n",
    "    warnings.warn(f\"Day16 trimmed PCs not found; approximating max_lag_primary={max_lag_primary}.\")\n",
    "\n",
    "trimmed_df = category_df.iloc[max_lag_primary:].reset_index(drop=True)\n",
    "trimmed_out = trimmed_df[['tr_index']].copy()\n",
    "trimmed_out.rename(columns={'tr_index': 'trim_index'}, inplace=True)\n",
    "for col in category_columns:\n",
    "    trimmed_out[col] = trimmed_df[col].values\n",
    "trimmed_csv_path = output_root / 'category_timeseries_trimmed.csv'\n",
    "trimmed_out.to_csv(trimmed_csv_path, index=False)\n",
    "print(f'Saved trimmed category series to {trimmed_csv_path}')\n",
    "\n",
    "category_stats = {}\n",
    "for col in category_columns:\n",
    "    values = category_df[col].to_numpy()\n",
    "    category_stats[col] = {\n",
    "        'mean': float(np.nanmean(values)),\n",
    "        'std': float(np.nanstd(values)),\n",
    "        'nan_fraction': float(np.mean(~np.isfinite(values)))\n",
    "    }\n",
    "meta = {\n",
    "    'subject': SUBJECT,\n",
    "    'story': STORY,\n",
    "    'tr_seconds': TR,\n",
    "    'n_tr': n_tr,\n",
    "    'category_set': category_set_name,\n",
    "    'category_score_method': category_score_method,\n",
    "    'overlap_weighting': overlap_mode,\n",
    "    'max_lag_primary': max_lag_primary,\n",
    "    'categories': category_stats,\n",
    "}\n",
    "import hashlib, json as _json\n",
    "_cfg_hash = hashlib.md5(_json.dumps({**categories_cfg, 'category_set': category_set_name}, sort_keys=True).encode()).hexdigest()  # config hash\n",
    "meta['config_hash'] = _cfg_hash\n",
    "meta.update({\n        'cluster_csv_path': cluster_csv_path,\n        'temporal_weighting': temporal_weighting,\n        'prototype_weight_power': prototype_weight_power,\n    })\n",
    "meta_path = output_root / 'meta.json'\n",
    "with meta_path.open('w') as fh:\n",
    "    json.dump(meta, fh, indent=2)\n",
    "print(f'Meta statistics saved to {meta_path}')\n",
    "# sanity check prints\n",
    "print(\"  ,  \", (day16_trim is None) or (len(day16_trim) <= len(category_df)))\n",
    "print(\"  ,  \", set(category_columns).issubset(set(category_df.columns)))\n",
    "print(\"  ,  \", meta.get('config_hash', '')[:8])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional concatenation across stories\n",
    "if bool(categories_cfg.get('concat_all_stories', False)):\n",
    "    story_dirs = discover_story_dirs(features_root, SUBJECT)\n",
    "    if not story_dirs:\n",
    "        story_dirs = [output_root.parent]\n",
    "    frames = []\n",
    "    global_idx = 0\n",
    "    for story_dir in story_dirs:\n",
    "        ts_path = story_dir / 'day17_categories' / 'category_timeseries.csv'\n",
    "        if not ts_path.exists():\n",
    "            continue\n",
    "        df_story = pd.read_csv(ts_path)\n",
    "        df_story.insert(0, 'story_id', story_dir.name)\n",
    "        df_story['tr_index_global'] = np.arange(global_idx, global_idx + len(df_story), dtype=int)\n",
    "        global_idx += len(df_story)\n",
    "        frames.append(df_story)\n",
    "    if frames:\n",
    "        concat_dir = features_root / 'subjects' / SUBJECT / 'ALL' / 'day17_categories'\n",
    "        concat_dir.mkdir(parents=True, exist_ok=True)\n",
    "        concat_df = pd.concat(frames, ignore_index=True)\n",
    "        concat_path = concat_dir / 'category_timeseries_concat.csv'\n",
    "        concat_df.to_csv(concat_path, index=False)\n",
    "        print(f'Concatenated category series saved to {concat_path}')\n",
    "    else:\n",
    "        print('No per-story category outputs found for concatenation.')\n",
    "else:\n",
    "    print('Concatenation disabled by configuration.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity plots\n",
    "if plt is None:\n",
    "    print('Matplotlib unavailable; skipping plots.')\n",
    "else:\n",
    "    sample_cols = category_columns[:min(4, len(category_columns))]\n",
    "    if not sample_cols:\n",
    "        print('No category columns to plot.')\n",
    "    else:\n",
    "        rows = len(sample_cols)\n",
    "        fig, axes = plt.subplots(rows, 1, figsize=(12, 3 * rows), sharex=True)\n",
    "        if rows == 1:\n",
    "            axes = [axes]\n",
    "        for ax, col in zip(axes, sample_cols):\n",
    "            ax.plot(category_df['start_sec'], category_df[col])\n",
    "            ax.set_ylabel(col)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        axes[-1].set_xlabel('Time (s)')\n",
    "        fig.suptitle('Sample category trajectories')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration demo with Day16 outputs\n",
    "trimmed_csv_path = output_root / 'category_timeseries_trimmed.csv'\n",
    "day16_trim_path = Path(paths.get('figs', 'figs')) / SUBJECT / STORY / 'day16_decoding' / 'semantic_pcs_trimmed.csv'\n",
    "if trimmed_csv_path.exists() and day16_trim_path.exists():\n",
    "    categories_trim = pd.read_csv(trimmed_csv_path)\n",
    "    sem_trim = pd.read_csv(day16_trim_path)\n",
    "    merged = pd.merge(sem_trim[['trim_index', 'sem_pc1']], categories_trim, on='trim_index', how='left')\n",
    "    display(merged.head())\n",
    "    first_cat = category_columns[0] if category_columns else None\n",
    "    if first_cat:\n",
    "        corr = merged['sem_pc1'].corr(merged[first_cat])\n",
    "        print(f'Correlation between sem_pc1 and {first_cat}: {corr:.3f}')\n",
    "else:\n",
    "    print('Trimmed category or Day16 semantic PCs not available; skipping merge demo.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README & Next Steps\n",
    "\n",
    "- **Category definitions** combine user-provided seeds with optional embedding expansion (cosine top-k). Edit `configs/demo.yaml` under `categories` or adjust `config_used.yaml` for future runs.\n",
    "- **Scoring mode** toggles via `category_score_method` (`similarity` for cosine averages, `count` for weighted lexicon counts).\n",
    "- **Canonical story timeline**: `figs/stories/{story}/day17_categories/category_timeseries_seconds.csv` captures the stimulus in seconds; resample this per subject to their TR grid and apply subject/run-specific shifts before decoding.\n",
    "- **Word2Vec fallback**: enable by setting `embedding_source` to `both` or `word2vec` and pointing `word2vec_path` to a compatible model. Missing models are skipped with warnings.\n",
    "- **Concatenation**: set `concat_all_stories: true` to produce `figs/{subject}/ALL/day17_categories/category_timeseries_concat.csv`; rerun after generating per-story outputs.\n",
    "- **Day16 integration**: use `category_timeseries_trimmed.csv` (aligned `trim_index`) to swap targets (`sem_pc1` \u2192 `cat_*`) or append as auxiliary predictors inside the Day16 pipeline.\n",
    "- **Customization**: extend `categories.sets` with additional taxonomy groups (e.g., Gallant lab categories) and rerun to materialize new feature matrices.\n",
    "- **Quality checks**: inspect `meta.json` for per-category means/stds, and consult `category_definition.json` for the exact lexicon used in scoring."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10",
   "mimetype": "text/x-python",
   "pygments_lexer": "ipython3",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}