{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 4 \u2014 Phase 1 ROI Attractor Sandbox\n",
        "\n",
        "Focus on quickly prototyping Takens embeddings inside language-related ROIs for a single subject/story."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 1 Checklist\n",
        "- load ROI \u00d7 time fMRI matrices for selected narratives\n",
        "- collapse to language-sensitive ROIs per hemisphere\n",
        "- visualize ROI time courses and build Takens attractors\n",
        "- compare simple attractor summaries across stories as a sanity check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n\n",
        "REPO_ROOT = Path.cwd().parent\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(REPO_ROOT))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n\n",
        "try:\n",
        "    import h5py\n",
        "except ImportError as exc:\n",
        "    raise RuntimeError('Install h5py before running this notebook.') from exc\n\n",
        "try:\n",
        "    import nibabel.freesurfer.io as fsio\n",
        "    HAVE_NIBABEL = True\n",
        "except ImportError:\n",
        "    HAVE_NIBABEL = False\n",
        "    print('Nibabel not installed; run `pip install nibabel` for ROI surface labels.')\n\n",
        "from src.io_ds003020 import list_stories_for_subject\n\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n\n",
        "DATA_ROOT = Path('/bucket/PaoU/seann/openneuro/ds003020')\n",
        "PREPROC_ROOT = DATA_ROOT / 'derivative' / 'preprocessed_data'\n",
        "FREESURFER_ROOT = DATA_ROOT / 'derivative' / 'freesurfer_subjdir'\n\n",
        "SUBJECT_ID = 'sub-UTS01'  # change as needed\n",
        "SUBJECT_FS = SUBJECT_ID.replace('sub-', '')\n",
        "TR = 2.0\n\n",
        "RESULTS_DIR = (REPO_ROOT / 'derivatives' / 'results' / f'day4_{SUBJECT_ID}').resolve()\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n\n",
        "LANGUAGE_ROIS = {\n",
        "    'lh': ['parsopercularis', 'parstriangularis', 'superiortemporal', 'middletemporal', 'temporalpole', 'bankssts', 'inferiorparietal', 'supramarginal'],\n",
        "    'rh': ['parsopercularis', 'parstriangularis', 'superiortemporal', 'middletemporal', 'temporalpole', 'bankssts', 'inferiorparietal', 'supramarginal'],\n",
        "}\n",
        "DEFAULT_STORIES = ['adventuresinsayingyes', 'adollshouse']  # edit per subject\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "story_records = list_stories_for_subject(DATA_ROOT, SUBJECT_ID)\n",
        "story_df = pd.DataFrame(story_records)\n",
        "if story_df.empty:\n",
        "    raise RuntimeError(f'No stories located for {SUBJECT_ID}.')\n",
        "story_df['preproc_path'] = story_df['story_id'].apply(\n",
        "    lambda sid: PREPROC_ROOT / SUBJECT_FS / f'{sid}.hf5'\n",
        ")\n",
        "story_df['has_hf5'] = story_df['preproc_path'].apply(lambda p: p.exists())\n",
        "display(story_df[['story_id', 'session', 'run', 'has_hf5']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def build_language_roi_map(subject_fs: str, rois: dict, atlas: str = 'aparc.annot'):\n",
        "    \"\"\"Return vertex indices for each requested ROI per hemisphere.\n",
        "\n",
        "    Requires nibabel to read FreeSurfer annotation files.\"\"\"\n",
        "    if not HAVE_NIBABEL:\n",
        "        raise RuntimeError('Nibabel missing; install it to map ROIs.')\n",
        "    label_dir = FREESURFER_ROOT / subject_fs / 'label'\n",
        "    roi_vertices = {}\n",
        "    hemi_counts = {}\n",
        "    for hemi in ('lh', 'rh'):\n",
        "        annot_path = label_dir / f'{hemi}.{atlas}'\n",
        "        if not annot_path.exists():\n",
        "            raise FileNotFoundError(f'Missing annotation: {annot_path}')\n",
        "        labels, _, names = fsio.read_annot(str(annot_path))\n",
        "        names = [n.decode('utf-8') for n in names]\n",
        "        name_to_index = {name: idx for idx, name in enumerate(names)}\n",
        "        hemi_counts[hemi] = labels.shape[0]\n",
        "        for roi in rois.get(hemi, []):\n",
        "            label_index = None\n",
        "            for candidate in (roi, f'ctx-{hemi}-{roi}', f'{hemi}-{roi}'):\n",
        "                if candidate in name_to_index:\n",
        "                    label_index = name_to_index[candidate]\n",
        "                    break\n",
        "            if label_index is None:\n",
        "                print(f'ROI `{roi}` not found in {annot_path.name}; skipping.')\n",
        "                continue\n",
        "            vert_idx = np.where(labels == label_index)[0]\n",
        "            if vert_idx.size == 0:\n",
        "                print(f'ROI `{roi}` has no vertices; skipping.')\n",
        "                continue\n",
        "            roi_key = f'{hemi}-{roi}'\n",
        "            roi_vertices[roi_key] = vert_idx\n",
        "    return roi_vertices, hemi_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def load_story_bold(subject_fs: str, story_id: str):\n",
        "    h5_path = PREPROC_ROOT / subject_fs / f'{story_id}.hf5'\n",
        "    if not h5_path.exists():\n",
        "        raise FileNotFoundError(f'Missing preprocessed file: {h5_path}')\n",
        "    with h5py.File(h5_path, 'r') as hf:\n",
        "        data = hf['data'][:]  # shape (TR, voxels/vertices)\n",
        "    return data\n\n",
        "def extract_roi_timeseries(bold_matrix: np.ndarray, roi_map: dict, tr: float):\n",
        "    n_tr, n_vertices = bold_matrix.shape\n",
        "    frame = pd.DataFrame(index=np.arange(n_tr))\n",
        "    for roi, idx in roi_map.items():\n",
        "        valid_idx = np.asarray(idx, dtype=int)\n",
        "        valid_idx = valid_idx[(valid_idx >= 0) & (valid_idx < n_vertices)]\n",
        "        if valid_idx.size == 0:\n",
        "            continue\n",
        "        frame[roi] = bold_matrix[:, valid_idx].mean(axis=1)\n",
        "    frame.insert(0, 'Time', np.arange(n_tr) * tr)\n",
        "    frame.attrs['source'] = 'roi_mean' if frame.shape[1] > 1 else 'empty'\n",
        "    return frame.reset_index(drop=True)\n\n",
        "def compute_pca_components(bold_matrix: np.ndarray, tr: float, n_components: int = 6):\n",
        "    bold_centered = bold_matrix - bold_matrix.mean(axis=0, keepdims=True)\n",
        "    u, s, _ = np.linalg.svd(bold_centered, full_matrices=False)\n",
        "    comps = u[:, :n_components] * s[:n_components]\n",
        "    frame = pd.DataFrame({f'PC{i+1}': comps[:, i] for i in range(comps.shape[1])})\n",
        "    frame.insert(0, 'Time', np.arange(frame.shape[0]) * tr)\n",
        "    frame.attrs['source'] = 'pca_fallback'\n",
        "    return frame\n\n",
        "def zscore(arr: np.ndarray):\n",
        "    arr = np.asarray(arr, dtype=float)\n",
        "    if arr.size == 0:\n",
        "        return arr\n",
        "    std = arr.std()\n",
        "    if not np.isfinite(std) or std == 0:\n",
        "        return arr - arr.mean()\n",
        "    return (arr - arr.mean()) / std\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "try:\n",
        "    roi_vertices  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    print('ROI map not found; building with LANGUAGE_ROIS ...')\n",
        "    roi_vertices, hemi_counts = build_language_roi_map(\n",
        "        SUBJECT_FS,\n",
        "        LANGUAGE_ROIS,\n",
        "        atlas='aparc.annot'\n",
        "    )\n",
        "\n",
        "expected_vertices = None\n",
        "if 'hemi_counts' in globals():\n",
        "    expected_vertices = sum(hemi_counts.values())\n",
        "\n",
        "STORY_IDS = DEFAULT_STORIES\n",
        "story_roi_timeseries = {}\n",
        "for story_id in STORY_IDS:\n",
        "    bold = load_story_bold(SUBJECT_FS, story_id)\n",
        "    if expected_vertices is None:\n",
        "        expected_vertices = bold.shape[1]\n",
        "    elif bold.shape[1] != expected_vertices:\n",
        "        print(f'Warning: vertex/voxel count mismatch for {story_id} ({bold.shape[1]} vs {expected_vertices})')\n",
        "    roi_frame = extract_roi_timeseries(bold, roi_vertices, TR)\n",
        "    if roi_frame.shape[1] <= 1:\n",
        "        print(f'No ROI vertices matched for {story_id}; falling back to PCA components.')\n",
        "        roi_frame = compute_pca_components(bold, TR, n_components=6)\n",
        "    story_roi_timeseries[story_id] = roi_frame\n",
        "    source = roi_frame.attrs.get('source', 'unknown')\n",
        "    print(f'{story_id}: {roi_frame.shape[0]} TRs, {roi_frame.shape[1] - 1} features ({source})')\n",
        "if not story_roi_timeseries:\n",
        "    raise RuntimeError('No ROI time series extracted.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_If FreeSurfer vertex indices do not map cleanly onto the preprocessed matrices, the notebook falls back to PCA components (`compute_pca_components`) so that downstream attractor diagnostics still run. Update `build_language_roi_map` or provide a custom index list once you have a definitive ROI \u2192 data mapping._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "first_story_id, first_frame = next(iter(story_roi_timeseries.items()))\n",
        "feature_cols = [col for col in first_frame.columns if col != 'Time']\n",
        "if not feature_cols:\n",
        "    raise RuntimeError('No features available for plotting.')\n",
        "selected_features = feature_cols[: min(6, len(feature_cols))]\n",
        "n_rows = len(selected_features)\n",
        "fig, axes = plt.subplots(n_rows, 1, figsize=(10, 2.2 * n_rows), sharex=True)\n",
        "if n_rows == 1:\n",
        "    axes = [axes]\n",
        "for axis, feature in zip(axes, selected_features):\n",
        "    for story_id, roi_frame in story_roi_timeseries.items():\n",
        "        axis.plot(\n",
        "            roi_frame['Time'],\n",
        "            zscore(roi_frame[feature]),\n",
        "            label=story_id\n",
        "        )\n",
        "    axis.set_ylabel(f'{feature}\n(z-score)')\n",
        "    axis.axhline(0, color='black', linewidth=0.8, alpha=0.5)\n",
        "axes[-1].set_xlabel('Time (s)')\n",
        "axes[0].legend(loc='upper right', ncol=len(STORY_IDS))\n",
        "fig.suptitle('Feature dynamics across stories', y=1.02)\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def takens_embedding(series: pd.Series, E: int = 3, tau: int = 1):\n",
        "    data = np.asarray(series, dtype=float)\n",
        "    window = (E - 1) * tau\n",
        "    if data.size <= window:\n",
        "        raise ValueError(f'Not enough samples for embedding: len={data.size}, E={E}, tau={tau}')\n",
        "    n_rows = data.size - window\n",
        "    cols = []\n",
        "    for delay in range(0, E * tau, tau):\n",
        "        cols.append(data[delay:delay + n_rows])\n",
        "    return np.stack(cols, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "target_feature = selected_features[0]\n",
        "E = 3\n",
        "TAU = 1\n",
        "fig = plt.figure(figsize=(7, 5))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "for story_id, roi_frame in story_roi_timeseries.items():\n",
        "    embedded = takens_embedding(zscore(roi_frame[target_feature]), E=E, tau=TAU)\n",
        "    ax.plot(embedded[:, 0], embedded[:, 1], embedded[:, 2], label=story_id, alpha=0.8)\n",
        "ax.set_xlabel('x(t)')\n",
        "ax.set_ylabel('x(t-\u03c4)')\n",
        "ax.set_zlabel('x(t-2\u03c4)')\n",
        "ax.set_title(f'{target_feature} attractor (E={E}, \u03c4={TAU})')\n",
        "ax.legend(loc='upper right')\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for story_id, roi_frame in story_roi_timeseries.items():\n",
        "    embedded = takens_embedding(zscore(roi_frame[target_feature]), E=E, tau=TAU)\n",
        "    fig = plt.figure(figsize=(6, 4))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.plot(embedded[:, 0], embedded[:, 1], embedded[:, 2], color='tab:blue', alpha=0.85)\n",
        "    ax.set_xlabel('x(t)')\n",
        "    ax.set_ylabel('x(t-\u03c4)')\n",
        "    ax.set_zlabel('x(t-2\u03c4)')\n",
        "    ax.set_title(f'{story_id} \u2014 {target_feature} (E={E}, \u03c4={TAU})')\n",
        "    plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import itertools\n\n",
        "summary_rows = []\n",
        "for story_id, roi_frame in story_roi_timeseries.items():\n",
        "    embedded = takens_embedding(zscore(roi_frame[target_feature]), E=E, tau=TAU)\n",
        "    centroid = embedded.mean(axis=0)\n",
        "    spread = embedded.std(axis=0)\n",
        "    summary_rows.append({\n",
        "        'story_id': story_id,\n",
        "        'centroid_x': centroid[0],\n",
        "        'centroid_y': centroid[1],\n",
        "        'centroid_z': centroid[2],\n",
        "        'spread_x': spread[0],\n",
        "        'spread_y': spread[1],\n",
        "        'spread_z': spread[2],\n",
        "    })\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "display(summary_df)\n\n",
        "pair_rows = []\n",
        "for (story_a, frame_a), (story_b, frame_b) in itertools.combinations(story_roi_timeseries.items(), 2):\n",
        "    emb_a = takens_embedding(zscore(frame_a[target_feature]), E=E, tau=TAU)\n",
        "    emb_b = takens_embedding(zscore(frame_b[target_feature]), E=E, tau=TAU)\n",
        "    k = min(len(emb_a), len(emb_b))\n",
        "    diff = emb_a[:k] - emb_b[:k]\n",
        "    rms = np.sqrt((diff ** 2).mean())\n",
        "    pair_rows.append({\n",
        "        'feature': target_feature,\n",
        "        'story_a': story_a,\n",
        "        'story_b': story_b,\n",
        "        'rms_distance': rms,\n",
        "    })\n",
        "pair_df = pd.DataFrame(pair_rows)\n",
        "display(pair_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Next steps\n",
        "- Sweep additional ROIs (`target_roi = ...`) and compare attractor summaries to decide whether to pool stories.\n",
        "- Expand `pair_df` into a matrix for quick visual comparison (heatmap).\n",
        "- Stage CCM runs per ROI pair and log outputs for later Phase 2 cross-subject checks."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}