{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "82d7bdf1",
      "metadata": {},
      "source": [
        "# Day 3 \u2014 EDM/CCM Drivers for `sub-UTS01`\n",
        "\n",
        "Focus: single-subject semantic forecasting diagnostics with **word**, **phoneme**, **acoustic**, and **English1000** trajectories.\n",
        "\n",
        "Checklist\n",
        "- [ ] Load drivers (envelope, word-rate, phoneme-rate, semantic PCs)\n",
        "- [ ] Run per-driver Simplex sweeps (E = 2\u20136)\n",
        "- [ ] Compare multi-driver forecasts (word + acoustics / semantics)\n",
        "- [ ] Run S-Map nonlinearity test for the best semantic channel\n",
        "- [ ] Perform CCM library sweeps across candidate drivers\n",
        "- [ ] Save artifacts under `derivatives/results/day3_sub-UTS01/`\n",
        "- [ ] Record recommendations and future semantic embedding TODOs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95e63dfd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_ROOT = Path.cwd().parent\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(REPO_ROOT))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bac2e9a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "from IPython.display import display\n",
        "\n",
        "try:\n",
        "    import pyEDM\n",
        "    HAVE_PYEDM = True\n",
        "except ImportError:\n",
        "    HAVE_PYEDM = False\n",
        "    print('pyEDM not installed. Install with `pip install pyEDM` before running EDM cells.')\n",
        "\n",
        "from src.io_ds003020 import list_stories_for_subject\n",
        "from src.qc_viz import (\n",
        "    HAVE_AUDIO,\n",
        "    HAVE_TEXTGRID,\n",
        "    ensure_dir,\n",
        "    normalize,\n",
        ")\n",
        "from src.edm_ccm import (\n",
        "    English1000Loader,\n",
        "    StoryDriver,\n",
        "    aggregate_driver_summary,\n",
        "    load_subject_stories,\n",
        "    multi_segment_library_lengths,\n",
        ")\n",
        "\n",
        "plt.style.use('default')\n",
        "plt.rcParams.update({'figure.figsize': (8, 4)})\n",
        "\n",
        "DATA_ROOT = Path('/bucket/PaoU/seann/openneuro/ds003020')\n",
        "SUBJECT_ID = 'sub-UTS01'\n",
        "TR = 2.0\n",
        "RESULTS_DIR = ensure_dir(REPO_ROOT / 'derivatives' / 'results' / f'day3_{SUBJECT_ID}')\n",
        "DRIVER_CACHE_PATH = RESULTS_DIR / f'{SUBJECT_ID}_story_drivers.pkl'\n",
        "ENGLISH1000_PATH = DATA_ROOT / 'derivative' / 'english1000sm.hf5'\n",
        "SEMANTIC_COMPONENTS = 64\n",
        "SEMANTIC_MULTI_TOPK = 3\n",
        "\n",
        "print(f'AUDIO deps: {HAVE_AUDIO}, TEXTGRID deps: {HAVE_TEXTGRID}, pyEDM: {HAVE_PYEDM}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06e4615e",
      "metadata": {},
      "outputs": [],
      "source": [
        "subject_records = list_stories_for_subject(DATA_ROOT, SUBJECT_ID)\n",
        "story_df = pd.DataFrame(subject_records)\n",
        "if story_df.empty:\n",
        "    raise RuntimeError(f'No stories found for {SUBJECT_ID}.')\n",
        "\n",
        "story_df['has_textgrid'] = story_df['textgrid'].notna()\n",
        "print(f\"Total stories: {len(story_df)} | with TextGrid: {story_df['has_textgrid'].sum()}\")\n",
        "display(story_df[['story_id', 'session', 'run', 'has_textgrid']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47a4b14f",
      "metadata": {},
      "outputs": [],
      "source": [
        "if not HAVE_AUDIO or not HAVE_TEXTGRID:\n",
        "    raise RuntimeError('Missing audio/TextGrid dependencies; install librosa, soundfile, textgrid.')\n",
        "\n",
        "usable_df = story_df[story_df['has_textgrid']].copy()\n",
        "print(f'Candidate stories with WAV+TextGrid: {len(usable_df)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce30337c",
      "metadata": {},
      "outputs": [],
      "source": [
        "english_loader = None\n",
        "if ENGLISH1000_PATH.exists():\n",
        "    try:\n",
        "        english_loader = English1000Loader(ENGLISH1000_PATH)\n",
        "        print('English1000 embeddings loaded.')\n",
        "    except Exception as exc:\n",
        "        print('Unable to load English1000 embeddings:', exc)\n",
        "else:\n",
        "    print('English1000 file not found; semantic PCs will be skipped.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c61c1d8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "cache_meta = {\n",
        "    'tr': TR,\n",
        "    'semantic_components': SEMANTIC_COMPONENTS if english_loader is not None else 0,\n",
        "    'semantic_loader': 'english1000' if english_loader is not None else None,\n",
        "    'semantic_source': str(ENGLISH1000_PATH) if english_loader is not None else None,\n",
        "    'story_ids': sorted(usable_df['story_id'].tolist()),\n",
        "}\n",
        "\n",
        "story_driver_list: List[StoryDriver]\n",
        "if DRIVER_CACHE_PATH.exists():\n",
        "    try:\n",
        "        with DRIVER_CACHE_PATH.open('rb') as fh:\n",
        "            cached = pickle.load(fh)\n",
        "        cached_meta = cached.get('metadata', {})\n",
        "        if cached_meta == cache_meta:\n",
        "            story_driver_list = cached['drivers']\n",
        "            print(f\"Loaded story drivers from cache: {DRIVER_CACHE_PATH}\")\n",
        "        else:\n",
        "            print('Driver cache metadata mismatch; regenerating.')\n",
        "            story_driver_list = None  # type: ignore\n",
        "    except Exception as exc:\n",
        "        print(f'Unable to read driver cache ({exc}); regenerating.')\n",
        "        story_driver_list = None  # type: ignore\n",
        "else:\n",
        "    story_driver_list = None  # type: ignore\n",
        "\n",
        "if story_driver_list is None:\n",
        "    start = time.time()\n",
        "    story_driver_list = load_subject_stories(\n",
        "        usable_df.to_dict('records'),\n",
        "        tr=TR,\n",
        "        semantic_loader=english_loader,\n",
        "        semantic_components=SEMANTIC_COMPONENTS if english_loader is not None else None,\n",
        "    )\n",
        "    elapsed = time.time() - start\n",
        "    print(f'Regenerated story drivers in {elapsed/60:.1f} min; caching to {DRIVER_CACHE_PATH}')\n",
        "    payload = {'metadata': cache_meta, 'drivers': story_driver_list}\n",
        "    with DRIVER_CACHE_PATH.open('wb') as fh:\n",
        "        pickle.dump(payload, fh)\n",
        "else:\n",
        "    print('Driver cache matches current configuration; using cached drivers.')\n",
        "\n",
        "print(f'Stories with driver series: {len(story_driver_list)}')\n",
        "if not story_driver_list:\n",
        "    raise RuntimeError('No driver series available for EDM analysis.')\n",
        "\n",
        "driver_cache: Dict[str, StoryDriver] = {s.story_id: s for s in story_driver_list}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b678c5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "summary_rows = aggregate_driver_summary(story_driver_list)\n",
        "summary_df = pd.DataFrame(summary_rows).sort_values('n_tr', ascending=False).reset_index(drop=True)\n",
        "summary_path = RESULTS_DIR / f'{SUBJECT_ID}_driver_summary.csv'\n",
        "summary_df.to_csv(summary_path, index=False)\n",
        "print(f'Summary saved to {summary_path}')\n",
        "display(summary_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f55704f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "target_story_id = None\n",
        "for candidate in summary_df['story_id']:\n",
        "    drv = driver_cache.get(candidate)\n",
        "    if drv is None:\n",
        "        continue\n",
        "    if drv.drivers.semantic is not None:\n",
        "        target_story_id = candidate\n",
        "        break\n",
        "if target_story_id is None:\n",
        "    target_story_id = summary_df.iloc[0]['story_id']\n",
        "\n",
        "print(f'Target story: {target_story_id}')\n",
        "target_story = driver_cache[target_story_id]\n",
        "series = target_story.drivers\n",
        "\n",
        "word_rate = series.word_rate if series.word_rate is not None else np.zeros(series.n_tr, dtype=float)\n",
        "phoneme_rate = series.phoneme_rate if series.phoneme_rate is not None else np.zeros(series.n_tr, dtype=float)\n",
        "semantic_cols = []\n",
        "if series.semantic is not None and series.semantic_labels is not None:\n",
        "    semantic_cols = [f'Semantic_{label}' for label in series.semantic_labels]\n",
        "\n",
        "time_index = np.arange(series.n_tr) * TR\n",
        "data = {\n",
        "    'Time': time_index,\n",
        "    'Envelope': series.envelope.astype(float),\n",
        "    'WordRate': word_rate.astype(float),\n",
        "    'PhonemeRate': phoneme_rate.astype(float),\n",
        "}\n",
        "\n",
        "if semantic_cols:\n",
        "    for idx, label in enumerate(semantic_cols):\n",
        "        data[label] = series.semantic[:, idx].astype(float)\n",
        "\n",
        "data_df = pd.DataFrame(data)\n",
        "\n",
        "for col in data_df.columns:\n",
        "    if col == 'Time':\n",
        "        continue\n",
        "    data_df[col] = data_df[col].fillna(0.0)\n",
        "\n",
        "data_path = RESULTS_DIR / f'{SUBJECT_ID}_{target_story_id}_drivers.csv'\n",
        "data_df.to_csv(data_path, index=False)\n",
        "print(f'Driver time series saved to {data_path}')\n",
        "data_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b468b10",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 3))\n",
        "ax.plot(data_df['Time'], normalize(data_df['WordRate']), label='WordRate (norm)')\n",
        "ax.plot(data_df['Time'], normalize(data_df['Envelope']), label='Envelope (norm)')\n",
        "if 'PhonemeRate' in data_df.columns and data_df['PhonemeRate'].std() > 0:\n",
        "    ax.plot(data_df['Time'], normalize(data_df['PhonemeRate']), label='PhonemeRate (norm)')\n",
        "ax.set_xlabel('Time (s)')\n",
        "ax.set_ylabel('Normalized amplitude')\n",
        "ax.set_title(f'{SUBJECT_ID} | {target_story_id} \u2014 Driver Overview')\n",
        "ax.legend(loc='upper right')\n",
        "fig_path = RESULTS_DIR / f'{SUBJECT_ID}_{target_story_id}_drivers.png'\n",
        "fig.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
        "plt.close(fig)\n",
        "print(f'Saved {fig_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "E_VALUES = [2, 3, 4, 5, 6]\n",
        "TAU = 1\n",
        "TP = 1\n",
        "\n",
        "def _has_variance(column: str) -> bool:\n",
        "    return column in data_df.columns and float(data_df[column].std()) > 0\n",
        "\n",
        "single_driver_configs = []\n",
        "if 'WordRate' in data_df.columns:\n",
        "    single_driver_configs.append({\n",
        "        'name': 'WordRate\u2192WordRate',\n",
        "        'columns': 'WordRate',\n",
        "        'target': 'WordRate',\n",
        "    })\n",
        "if _has_variance('Envelope'):\n",
        "    single_driver_configs.append({\n",
        "        'name': 'Envelope\u2192WordRate',\n",
        "        'columns': 'Envelope',\n",
        "        'target': 'WordRate',\n",
        "    })\n",
        "if _has_variance('PhonemeRate'):\n",
        "    single_driver_configs.append({\n",
        "        'name': 'PhonemeRate\u2192WordRate',\n",
        "        'columns': 'PhonemeRate',\n",
        "        'target': 'WordRate',\n",
        "    })\n",
        "for col in semantic_cols[:SEMANTIC_MULTI_TOPK]:\n",
        "    if _has_variance(col):\n",
        "        single_driver_configs.append({\n",
        "            'name': f'{col}\u2192WordRate',\n",
        "            'columns': col,\n",
        "            'target': 'WordRate',\n",
        "        })\n",
        "\n",
        "multi_configs = []\n",
        "seen_pairs = set()\n",
        "base_drivers = [col for col in ['Envelope', 'PhonemeRate'] if _has_variance(col)]\n",
        "top_semantic = [col for col in semantic_cols[:SEMANTIC_MULTI_TOPK] if _has_variance(col)]\n",
        "\n",
        "candidates = []\n",
        "if base_drivers:\n",
        "    candidates.append(base_drivers)\n",
        "for k in range(1, len(top_semantic) + 1):\n",
        "    candidates.append(base_drivers + top_semantic[:k])\n",
        "\n",
        "for cols in candidates:\n",
        "    unique_cols = []\n",
        "    for col in cols:\n",
        "        if col not in unique_cols:\n",
        "            unique_cols.append(col)\n",
        "    if not unique_cols:\n",
        "        continue\n",
        "    name = '+'.join(unique_cols) + '\u2192WordRate'\n",
        "    key = tuple(unique_cols)\n",
        "    if key in seen_pairs:\n",
        "        continue\n",
        "    seen_pairs.add(key)\n",
        "    multi_configs.append({\n",
        "        'name': name,\n",
        "        'columns': ' '.join(unique_cols),\n",
        "        'target': 'WordRate',\n",
        "    })\n",
        "\n",
        "if not single_driver_configs and not multi_configs:\n",
        "    raise RuntimeError('No driver configurations available for simplex sweep.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4403328",
      "metadata": {},
      "outputs": [],
      "source": [
        "lib_range = f\"1 {len(data_df)}\"\n",
        "pred_range = lib_range\n",
        "\n",
        "simplex_records = []\n",
        "for cfg in single_driver_configs + multi_configs:\n",
        "    for E in E_VALUES:\n",
        "        try:\n",
        "            res = pyEDM.Simplex(\n",
        "                dataFrame=data_df,\n",
        "                E=E,\n",
        "                tau=TAU,\n",
        "                Tp=TP,\n",
        "                columns=cfg['columns'],\n",
        "                target=cfg['target'],\n",
        "                lib=lib_range,\n",
        "                pred=pred_range,\n",
        "                ignoreNan=True,\n",
        "                verbose=False,\n",
        "            )\n",
        "            rho = float(res['rho'].values[-1]) if 'rho' in res else np.nan\n",
        "        except Exception as exc:\n",
        "            print(f\"Skip {cfg['name']} (E={E}): {exc}\")\n",
        "            rho = np.nan\n",
        "        simplex_records.append({'name': cfg['name'], 'columns': cfg['columns'], 'target': cfg['target'], 'E': E, 'rho': rho})\n",
        "\n",
        "simplex_df = pd.DataFrame(simplex_records)\n",
        "simplex_path = RESULTS_DIR / f'{SUBJECT_ID}_{target_story_id}_simplex_comparison.csv'\n",
        "simplex_df.to_csv(simplex_path, index=False)\n",
        "print(f'Simplex comparison saved to {simplex_path}')\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "for name, sub in simplex_df.groupby('name'):\n",
        "    ax.plot(sub['E'], sub['rho'], marker='o', label=name)\n",
        "ax.set_xlabel('Embedding dimension E')\n",
        "ax.set_ylabel('Forecast skill (rho)')\n",
        "ax.set_title(f'Simplex comparison | {SUBJECT_ID} {target_story_id}')\n",
        "ax.axhline(0, color='black', linewidth=1)\n",
        "ax.legend(loc='best')\n",
        "fig_path = RESULTS_DIR / f'{SUBJECT_ID}_{target_story_id}_simplex_comparison.png'\n",
        "fig.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
        "plt.close(fig)\n",
        "print(f'Saved {fig_path}')\n",
        "\n",
        "wordrate_mask = simplex_df['target'] == 'WordRate'\n",
        "if wordrate_mask.any():\n",
        "    best_word_row = simplex_df[wordrate_mask].sort_values('rho', ascending=False).iloc[0]\n",
        "else:\n",
        "    best_word_row = simplex_df.sort_values('rho', ascending=False).iloc[0]\n",
        "\n",
        "try:\n",
        "    best_E = int(best_word_row['E'])\n",
        "except (TypeError, ValueError):\n",
        "    best_E = E_VALUES[0]\n",
        "\n",
        "print(f\"Best WordRate config: {best_word_row['name']} (E={best_E}, rho={best_word_row['rho']:.3f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0974d894",
      "metadata": {},
      "outputs": [],
      "source": [
        "simplex_run = pyEDM.Simplex(\n",
        "    dataFrame=data_df,\n",
        "    E=best_E,\n",
        "    tau=1,\n",
        "    Tp=1,\n",
        "    columns='WordRate',\n",
        "    target='WordRate',\n",
        "    lib=f\"1 {len(data_df)}\",\n",
        "    pred=f\"1 {len(data_df)}\",\n",
        "    ignoreNan=True,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "forecast_df = pd.DataFrame({\n",
        "    'Time': simplex_run['Time'],\n",
        "    'Observed': simplex_run['Observations'],\n",
        "    'Forecast': simplex_run['Predictions'],\n",
        "})\n",
        "forecast_path = RESULTS_DIR / f'{SUBJECT_ID}_{target_story_id}_simplex_wordrate_forecast.csv'\n",
        "forecast_df.to_csv(forecast_path, index=False)\n",
        "print(f'Forecast series saved to {forecast_path}')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 3))\n",
        "ax.plot(forecast_df['Time'], forecast_df['Observed'], label='Observed')\n",
        "ax.plot(forecast_df['Time'], forecast_df['Forecast'], label='Forecast', alpha=0.8)\n",
        "ax.set_xlabel('Time index (TR)')\n",
        "ax.set_ylabel('WordRate')\n",
        "ax.set_title(f'WordRate Simplex forecast (E={best_E}, Tp=1)')\n",
        "ax.legend(loc='upper right')\n",
        "fig_path = RESULTS_DIR / f'{SUBJECT_ID}_{target_story_id}_simplex_wordrate_forecast.png'\n",
        "fig.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
        "plt.close(fig)\n",
        "print(f'Saved {fig_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afbe8338",
      "metadata": {},
      "outputs": [],
      "source": [
        "theta_values = [0, 0.5, 1.0, 1.5, 2.0]\n",
        "sm_records = []\n",
        "for theta in theta_values:\n",
        "    res = pyEDM.SMap(\n",
        "        dataFrame=data_df,\n",
        "        E=best_E,\n",
        "        tau=1,\n",
        "        Tp=1,\n",
        "        columns='WordRate',\n",
        "        target='WordRate',\n",
        "        theta=theta,\n",
        "        lib=f\"1 {len(data_df)}\",\n",
        "        pred=f\"1 {len(data_df)}\",\n",
        "        ignoreNan=True,\n",
        "        verbose=False,\n",
        "    )\n",
        "    rho = float(res['rho'].values[-1]) if 'rho' in res else np.nan\n",
        "    sm_records.append({'theta': theta, 'rho': rho})\n",
        "\n",
        "smap_df = pd.DataFrame(sm_records)\n",
        "smap_path = RESULTS_DIR / f'{SUBJECT_ID}_{target_story_id}_smap_wordrate.csv'\n",
        "smap_df.to_csv(smap_path, index=False)\n",
        "print(f'S-Map results saved to {smap_path}')\n",
        "\n",
        "display(smap_df)\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(smap_df['theta'], smap_df['rho'], marker='o')\n",
        "ax.set_xlabel('Nonlinearity (theta)')\n",
        "ax.set_ylabel('Forecast skill (rho)')\n",
        "ax.set_title(f'WordRate S-Map | E={best_E}, Tp=1')\n",
        "ax.axhline(0, color='black', linewidth=1)\n",
        "fig_path = RESULTS_DIR / f'{SUBJECT_ID}_{target_story_id}_smap_wordrate.png'\n",
        "fig.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
        "plt.close(fig)\n",
        "print(f'Saved {fig_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb096afb",
      "metadata": {},
      "outputs": [],
      "source": [
        "semantic_cols = [col for col in data_df.columns if col.startswith('Semantic_')]\n",
        "ccm_pairs = []\n",
        "if 'Envelope' in data_df.columns and data_df['Envelope'].std() > 0:\n",
        "    ccm_pairs.append(('Envelope', 'WordRate'))\n",
        "if 'PhonemeRate' in data_df.columns and data_df['PhonemeRate'].std() > 0:\n",
        "    ccm_pairs.append(('PhonemeRate', 'WordRate'))\n",
        "if semantic_cols:\n",
        "    ccm_pairs.append((semantic_cols[0], 'WordRate'))\n",
        "\n",
        "lib_sizes = np.arange(50, min(600, series.n_tr), 50)\n",
        "if lib_sizes.size == 0:\n",
        "    print('Library sizes too small for CCM sweep; skipping CCM.')\n",
        "else:\n",
        "    ccm_frames = []\n",
        "    for driver, target in ccm_pairs:\n",
        "        try:\n",
        "            forward = pyEDM.CCM(\n",
        "                dataFrame=data_df,\n",
        "                E=best_E,\n",
        "                tau=1,\n",
        "                columns=driver,\n",
        "                target=target,\n",
        "                Tp=0,\n",
        "                librarySizes=lib_sizes,\n",
        "                sample=100,\n",
        "                random=True,\n",
        "                verbose=False,\n",
        "            )\n",
        "            forward['pair'] = f'{driver}\u2192{target}'\n",
        "            ccm_frames.append(forward)\n",
        "\n",
        "            reverse = pyEDM.CCM(\n",
        "                dataFrame=data_df,\n",
        "                E=best_E,\n",
        "                tau=1,\n",
        "                columns=target,\n",
        "                target=driver,\n",
        "                Tp=0,\n",
        "                librarySizes=lib_sizes,\n",
        "                sample=100,\n",
        "                random=True,\n",
        "                verbose=False,\n",
        "            )\n",
        "            reverse['pair'] = f'{target}\u2192{driver}'\n",
        "            ccm_frames.append(reverse)\n",
        "        except Exception as exc:\n",
        "            print(f'Skip CCM pair {driver}->{target}: {exc}')\n",
        "\n",
        "    if ccm_frames:\n",
        "        ccm_df = pd.concat(ccm_frames, ignore_index=True)\n",
        "        ccm_path = RESULTS_DIR / f'{SUBJECT_ID}_{target_story_id}_ccm_comparison.csv'\n",
        "        ccm_df.to_csv(ccm_path, index=False)\n",
        "        print(f'CCM results saved to {ccm_path}')\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        for pair, sub in ccm_df.groupby('pair'):\n",
        "            ax.plot(sub['LibSize'], sub['Rho'], marker='o', label=pair)\n",
        "        ax.set_xlabel('Library size (TR)')\n",
        "        ax.set_ylabel('CCM \u03c1')\n",
        "        ax.set_title(f'CCM library sweep | {SUBJECT_ID} {target_story_id}')\n",
        "        ax.legend(loc='best')\n",
        "        fig_path = RESULTS_DIR / f'{SUBJECT_ID}_{target_story_id}_ccm_comparison.png'\n",
        "        fig.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close(fig)\n",
        "        print(f'Saved {fig_path}')\n",
        "    else:\n",
        "        print('No CCM pairs computed; all drivers missing variance or pyEDM error.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15574f02",
      "metadata": {},
      "outputs": [],
      "source": [
        "library_lengths = summary_df['n_tr'].values\n",
        "lib_depth = multi_segment_library_lengths(library_lengths, exclusion=best_E)\n",
        "rows = []\n",
        "for E in [2, 3, 4, 5, 6]:\n",
        "    effective = np.maximum(library_lengths - E, 0)\n",
        "    rows.append({\n",
        "        'E': E,\n",
        "        'median_library': float(np.median(effective)),\n",
        "        'min_library': int(np.min(effective)),\n",
        "        'stories_ge_200': int((effective >= 200).sum()),\n",
        "    })\n",
        "embedding_df = pd.DataFrame(rows)\n",
        "display(embedding_df)\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(embedding_df['E'], embedding_df['median_library'], marker='o', label='Median usable TRs')\n",
        "ax.plot(embedding_df['E'], embedding_df['min_library'], marker='s', label='Minimum usable TRs')\n",
        "ax.axhline(200, color='black', linestyle='--', linewidth=1)\n",
        "ax.set_xlabel('Embedding dimension E')\n",
        "ax.set_ylabel('Usable TR count per story')\n",
        "ax.set_title(f'{SUBJECT_ID} \u2014 Library depth vs embedding dimension')\n",
        "ax.legend()\n",
        "fig_path = RESULTS_DIR / f'{SUBJECT_ID}_{target_story_id}_library_vs_E.png'\n",
        "fig.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
        "plt.close(fig)\n",
        "print(f'Saved {fig_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0beddeef",
      "metadata": {},
      "outputs": [],
      "source": [
        "EMBED_E = max(3, best_E)\n",
        "TAU_ATTR = 1\n",
        "window = (EMBED_E - 1) * TAU_ATTR\n",
        "if len(data_df) <= window:\n",
        "    print(f'Skipping attractor plot: insufficient TRs for E={EMBED_E}.')\n",
        "else:\n",
        "    values = data_df['WordRate'].values\n",
        "    embedded = []\n",
        "    for idx in range(len(values) - window):\n",
        "        embedded.append([values[idx + j * TAU_ATTR] for j in range(EMBED_E)])\n",
        "    embedded = np.asarray(embedded)\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.plot(embedded[:, 0], embedded[:, 1], embedded[:, 2], lw=0.6, alpha=0.9)\n",
        "    ax.set_xlabel('WordRate(t)')\n",
        "    ax.set_ylabel('WordRate(t-\u03c4)')\n",
        "    ax.set_zlabel('WordRate(t-2\u03c4)')\n",
        "    ax.set_title(f'{SUBJECT_ID} | {target_story_id} \u2014 Attractor (E={EMBED_E}, \u03c4={TAU_ATTR})')\n",
        "    fig_path = RESULTS_DIR / f'{SUBJECT_ID}_{target_story_id}_attractor.png'\n",
        "    fig.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    print(f'Saved {fig_path}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dbb8599",
      "metadata": {},
      "outputs": [],
      "source": [
        "best_rho_by_name = simplex_df.sort_values('rho', ascending=False).groupby('name').first()\n",
        "if not best_rho_by_name.empty:\n",
        "    best_rho_dict = {name: float(row['rho']) for name, row in best_rho_by_name.iterrows()}\n",
        "else:\n",
        "    best_rho_dict = {}\n",
        "\n",
        "smap_best_theta = float(smap_df.sort_values('rho', ascending=False).iloc[0]['theta']) if not smap_df.empty else float('nan')\n",
        "ccm_pairs_summary = ccm_df['pair'].unique().tolist() if 'ccm_df' in locals() else []\n",
        "\n",
        "summary_notes = {\n",
        "    'subject': SUBJECT_ID,\n",
        "    'story': target_story_id,\n",
        "    'simplex_best_rho': best_rho_dict,\n",
        "    'best_wordrate_E': int(best_word_row['E']),\n",
        "    'best_wordrate_rho': float(best_word_row['rho']),\n",
        "    'smap_best_theta': smap_best_theta,\n",
        "    'ccm_pairs': ccm_pairs_summary,\n",
        "    'semantic_components': int(SEMANTIC_COMPONENTS) if english_loader is not None else 0,\n",
        "    'semantic_columns_used': semantic_cols[:SEMANTIC_MULTI_TOPK] if semantic_cols else [],\n",
        "}\n",
        "\n",
        "notes_path = RESULTS_DIR / f'{SUBJECT_ID}_{target_story_id}_day3_notes.json'\n",
        "import json\n",
        "notes_path.write_text(json.dumps(summary_notes, indent=2))\n",
        "print('Notes saved to', notes_path)\n",
        "summary_notes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d49967c6",
      "metadata": {},
      "source": [
        "### Future embedding TODOs\n",
        "- Evaluate Word2Vec and GloVe embeddings aligned via TextGrid timings.\n",
        "- Test contextual LMs (e.g., GPT/BERT) and QA-style features (Huth & Tang, 2025).\n",
        "- Run conditional CCM controlling for envelope/phoneme drivers when forecasting semantic PCs.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}