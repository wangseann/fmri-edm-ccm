{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f78bffc2",
   "metadata": {},
   "source": [
    "# Day 5 — Language ROI Attractors (pycortex-aligned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5684de7",
   "metadata": {},
   "source": [
    "Focus: pull language ROI masks from the pycortex database that ships with ds003020, align them with the ~81k-vertex preprocessed matrices, and generate Takens attractors for sample stories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c18228c",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- `pip install pycortex` (once per environment).\n",
    "- Ensure `/bucket/.../ds003020/derivative/pycortex-db` and `preprocessed_data` are mounted.\n",
    "- Confirm `derivative/subject_xfms.json` contains your subject (e.g., `\"UTS01\": \"UTS01_auto\"`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c617a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ROOT = Path.cwd().parent\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a268c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "try:\n",
    "    import cortex\n",
    "    HAVE_PYCORTEX = True\n",
    "except ImportError:\n",
    "    HAVE_PYCORTEX = False\n",
    "    print('pycortex missing — install with `pip install pycortex`.')\n",
    "\n",
    "import h5py\n",
    "\n",
    "from src.io_ds003020 import list_stories_for_subject\n",
    "from src.qc_viz import ensure_dir\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39455f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path('/bucket/PaoU/seann/openneuro/ds003020')\n",
    "PREPROC_ROOT = DATA_ROOT / 'derivative' / 'preprocessed_data'\n",
    "PYCORTEX_DB = DATA_ROOT / 'derivative' / 'pycortex-db'\n",
    "FREESURFER_SUBJECTS = DATA_ROOT / 'derivative' / 'freesurfer_subjdir'\n",
    "TRANSFORM_PATH = DATA_ROOT / 'derivative' / 'subject_xfms.json'\n",
    "\n",
    "SUBJECT_ID = 'sub-UTS01'  # change as needed\n",
    "SUBJECT_FS = SUBJECT_ID.replace('sub-', '')\n",
    "TR = 2.0\n",
    "STORY_IDS = ['adventuresinsayingyes', 'adollshouse']\n",
    "\n",
    "ROI_FILTER = None  # set to dict{'lh': [...], 'rh': [...]} to limit ROIs\n",
    "\n",
    "RESULTS_DIR = ensure_dir(REPO_ROOT / 'derivatives' / 'results' / f'day5_{SUBJECT_ID}')\n",
    "ROI_CACHE = RESULTS_DIR / f'{SUBJECT_ID}_roi_masks.npz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d9d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not HAVE_PYCORTEX:\n",
    "    raise RuntimeError('Install pycortex before fetching ROI masks.')\n",
    "if not PYCORTEX_DB.exists():\n",
    "    raise FileNotFoundError(f'Missing pycortex DB at {PYCORTEX_DB}')\n",
    "\n",
    "cortex.config.default_db = str(PYCORTEX_DB)\n",
    "cortex.config.default_filestore = str(PYCORTEX_DB)\n",
    "cortex.config.default_subject = SUBJECT_FS\n",
    "cortex.database.default_filestore = str(PYCORTEX_DB)\n",
    "# rebuild the global database handles so downstream API uses the dataset filestore\n",
    "cortex.database.db = cortex.database.Database(str(PYCORTEX_DB))\n",
    "cortex.db = cortex.database.db\n",
    "try:\n",
    "    import cortex.dataset\n",
    "    cortex.dataset.db = cortex.database.db\n",
    "    import cortex.dataset.braindata as _braindata\n",
    "    _braindata.db = cortex.database.db\n",
    "    import cortex.dataset.views as _views\n",
    "    _views.db = cortex.database.db\n",
    "except ImportError:\n",
    "    pass\n",
    "# clear cached subject list and force a refresh so `UTS01` is visible\n",
    "def _refresh_pycortex_subjects():\n",
    "    cortex.database.db._subjects = None\n",
    "    subjects = cortex.database.db.subjects\n",
    "    print(f'pycortex subjects: {list(subjects.keys())[:5]}... (total={len(subjects)})')\n",
    "    if SUBJECT_FS not in subjects:\n",
    "        raise KeyError(f'pycortex filestore {PYCORTEX_DB} does not contain subject {SUBJECT_FS}')\n",
    "\n",
    "_refresh_pycortex_subjects()\n",
    "\n",
    "with TRANSFORM_PATH.open() as fh:\n",
    "    transform_map = json.load(fh)\n",
    "TRANSFORM_ID = transform_map.get(SUBJECT_FS)\n",
    "if TRANSFORM_ID is None:\n",
    "    raise KeyError(f'No transform entry for {SUBJECT_FS} in {TRANSFORM_PATH}')\n",
    "\n",
    "print(f'Subject {SUBJECT_ID}: using transform {TRANSFORM_ID}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0383d6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "\n",
    "ROI_NAME_OVERRIDES = {\n",
    "    # Map shorthand to pycortex atlas names if needed\n",
    "    'parsopercularis': ['parsopercularis', 'G_front_inf-Opercular'],\n",
    "    'parstriangularis': ['parstriangularis', 'G_front_inf-Triangul'],\n",
    "    'superiortemporal': ['superiortemporal', 'G_temporal_sup'],\n",
    "    'middletemporal': ['middletemporal', 'G_temporal_middle'],\n",
    "    'temporalpole': ['temporalpole', 'Pole_temporal'],\n",
    "    'bankssts': ['bankssts', 'S_temporal_sup-Lateral'],\n",
    "    'inferiorparietal': ['inferiorparietal', 'G_pariet_inf-Angular'],\n",
    "    'supramarginal': ['supramarginal', 'G_pariet_inf-Supramar'],\n",
    "}\n",
    "\n",
    "PYCORTEX_ROI_TYPES = ('atlas', 'labels', 'freesurfer')\n",
    "ROI_LABEL_VARIANTS = (\n",
    "    '{hemi}-{token}',\n",
    "    '{hemi}_{token}',\n",
    "    '{hemi}.{token}',\n",
    "    '{token}-{hemi}',\n",
    "    '{token}_{hemi}',\n",
    "    '{token}.{hemi}',\n",
    "    'ctx-{hemi}-{token}',\n",
    "    'ctx_{hemi}_{token}',\n",
    "    'ctx.{hemi}.{token}',\n",
    "    'ctx-{token}',\n",
    "    'ctx_{token}',\n",
    "    '{token}',\n",
    ")\n",
    "\n",
    "\n",
    "def _call_with_xfm(func, subject_fs: str, value, roi_type: str, transform: str):\n",
    "    \"\"\"Call a pycortex helper, trying xfmname/transform keywords as needed.\"\"\"\n",
    "    if transform is None:\n",
    "        return func(subject_fs, value, type=roi_type)\n",
    "    last_error = None\n",
    "    for key in ('xfmname', 'transform'):\n",
    "        try:\n",
    "            return func(subject_fs, value, type=roi_type, **{key: transform})\n",
    "        except TypeError as exc:\n",
    "            if 'unexpected keyword argument' not in str(exc):\n",
    "                last_error = exc\n",
    "                break\n",
    "            last_error = exc\n",
    "            continue\n",
    "    if last_error is not None:\n",
    "        raise last_error\n",
    "    return func(subject_fs, value, type=roi_type)\n",
    "\n",
    "\n",
    "def _format_roi_label(entry) -> str:\n",
    "    if isinstance(entry, (tuple, list)):\n",
    "        return '-'.join(str(part) for part in entry)\n",
    "    return str(entry)\n",
    "\n",
    "\n",
    "def list_available_pyctx_rois(subject_fs: str, transform: str):\n",
    "    summary: Dict[str, list] = {}\n",
    "    for roi_type in PYCORTEX_ROI_TYPES:\n",
    "        try:\n",
    "            names = cortex.db.get_rois(subject_fs, type=roi_type)\n",
    "        except Exception as exc:\n",
    "            print(f'Unable to fetch ROI list via pycortex ({roi_type}): {exc}')\n",
    "            continue\n",
    "        if not names:\n",
    "            continue\n",
    "        summary[roi_type] = names\n",
    "        preview = ', '.join(_format_roi_label(name) for name in names[:10])\n",
    "        suffix = ' ...' if len(names) > 10 else ''\n",
    "        print(f'pycortex {roi_type} entries ({len(names)}): {preview}{suffix}')\n",
    "    return summary\n",
    "\n",
    "\n",
    "def _auto_roi_spec(available_by_type: Dict[str, list]) -> Dict[str, list]:\n",
    "    auto_spec: Dict[str, list] = {}\n",
    "    prefixes = (\n",
    "        '{hemi}-',\n",
    "        '{hemi}_',\n",
    "        '{hemi}.',\n",
    "        'ctx-{hemi}-',\n",
    "        'ctx_{hemi}_',\n",
    "        'ctx.{hemi}.',\n",
    "    )\n",
    "    for entries in available_by_type.values():\n",
    "        for entry in entries:\n",
    "            label = _format_roi_label(entry)\n",
    "            lowered = label.lower()\n",
    "            hemi = None\n",
    "            trimmed = label\n",
    "            for candidate in ('lh', 'rh'):\n",
    "                matched = False\n",
    "                for template in prefixes:\n",
    "                    prefix = template.format(hemi=candidate)\n",
    "                    if lowered.startswith(prefix):\n",
    "                        hemi = candidate\n",
    "                        trimmed = label[len(prefix):]\n",
    "                        matched = True\n",
    "                        break\n",
    "                if matched:\n",
    "                    break\n",
    "            if hemi is None:\n",
    "                continue\n",
    "            trimmed = trimmed.strip()\n",
    "            if not trimmed:\n",
    "                continue\n",
    "            auto_spec.setdefault(hemi, [])\n",
    "            if trimmed not in auto_spec[hemi]:\n",
    "                auto_spec[hemi].append(trimmed)\n",
    "    return auto_spec\n",
    "\n",
    "\n",
    "def _normalize_label(text: str) -> str:\n",
    "    return ''.join(ch for ch in text.lower() if ch.isalnum())\n",
    "\n",
    "\n",
    "def _match_token_to_label(token: str, label_names) -> Optional[str]:\n",
    "    tokens = [token]\n",
    "    tokens.extend(ROI_NAME_OVERRIDES.get(token, []))\n",
    "    lookup = {_normalize_label(name): name for name in label_names}\n",
    "    for candidate in tokens:\n",
    "        norm = _normalize_label(candidate)\n",
    "        if norm in lookup:\n",
    "            return lookup[norm]\n",
    "    return None\n",
    "\n",
    "\n",
    "def _build_masks_from_pycortex(\n",
    "    subject_fs: str,\n",
    "    roi_spec: Optional[Dict[str, list]],\n",
    "    transform: str,\n",
    "    available_by_type: Dict[str, list],\n",
    "):\n",
    "    working_spec = roi_spec\n",
    "    if not working_spec:\n",
    "        working_spec = _auto_roi_spec(available_by_type)\n",
    "        if not working_spec:\n",
    "            raise KeyError('Unable to derive hemisphere-specific ROI labels from pycortex DB entries.')\n",
    "        print(f'Auto-selected {sum(len(v) for v in working_spec.values())} ROI labels from pycortex DB.')\n",
    "\n",
    "    masks: Dict[str, np.ndarray] = {}\n",
    "    for hemi, roi_list in working_spec.items():\n",
    "        for roi in roi_list:\n",
    "            key = f'{hemi}-{roi}'\n",
    "            token_seq = [roi]\n",
    "            token_seq.extend(ROI_NAME_OVERRIDES.get(roi, []))\n",
    "            unique_tokens = []\n",
    "            for token in token_seq:\n",
    "                canonical = str(token).lower().strip()\n",
    "                if canonical not in unique_tokens:\n",
    "                    unique_tokens.append(canonical)\n",
    "            candidates = []\n",
    "            for token in unique_tokens:\n",
    "                token_variants = {\n",
    "                    token,\n",
    "                    token.replace('-', '_'),\n",
    "                    token.replace('-', '.'),\n",
    "                    token.replace('_', '-'),\n",
    "                }\n",
    "                for token_variant in token_variants:\n",
    "                    for template in ROI_LABEL_VARIANTS:\n",
    "                        candidates.append(template.format(hemi=hemi, token=token_variant))\n",
    "                candidates.append((hemi, token))\n",
    "            seen_labels = set()\n",
    "            mask = None\n",
    "            for cand in candidates:\n",
    "                if cand in seen_labels:\n",
    "                    continue\n",
    "                seen_labels.add(cand)\n",
    "                for roi_type in PYCORTEX_ROI_TYPES:\n",
    "                    try:\n",
    "                        raw = _call_with_xfm(cortex.db.get_roi_mask, subject_fs, cand, roi_type, transform)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    arr = np.asarray(raw)\n",
    "                    if arr.ndim == 2:\n",
    "                        arr = arr[0] if hemi == 'lh' else arr[-1]\n",
    "                    arr = arr.astype(bool).ravel()\n",
    "                    if arr.any():\n",
    "                        mask = arr\n",
    "                        print(f'{key}: {arr.sum()} vertices (pycortex label `{cand}` [{roi_type}])')\n",
    "                        break\n",
    "                if mask is not None:\n",
    "                    break\n",
    "            if mask is None:\n",
    "                preview = {\n",
    "                    roi_type: [\n",
    "                        _format_roi_label(name)\n",
    "                        for name in names[:10]\n",
    "                    ]\n",
    "                    for roi_type, names in available_by_type.items()\n",
    "                }\n",
    "                raise KeyError(f'ROI {key} not found in pycortex DB. Available entries sample: {preview}')\n",
    "            masks[key] = mask\n",
    "    return masks\n",
    "\n",
    "\n",
    "def _load_freesurfer_annotations(subject_fs: str, atlas: str = 'aparc'):\n",
    "    fs_label_dir = FREESURFER_SUBJECTS / subject_fs / 'label'\n",
    "    if not fs_label_dir.exists():\n",
    "        raise FileNotFoundError(f'Missing FreeSurfer labels for {subject_fs}: {fs_label_dir}')\n",
    "    try:\n",
    "        from nibabel.freesurfer import read_annot\n",
    "    except ImportError as exc:\n",
    "        raise RuntimeError('Install nibabel to enable FreeSurfer ROI fallback (e.g. `pip install nibabel`).') from exc\n",
    "\n",
    "    annotations = {}\n",
    "    for hemi in ('lh', 'rh'):\n",
    "        annot_path = fs_label_dir / f'{hemi}.{atlas}.annot'\n",
    "        if not annot_path.exists():\n",
    "            raise FileNotFoundError(f'Missing FreeSurfer annotation file: {annot_path}')\n",
    "        labels, _, names = read_annot(str(annot_path))\n",
    "        decoded_names = []\n",
    "        for name in names:\n",
    "            if isinstance(name, bytes):\n",
    "                decoded_names.append(name.decode('utf-8'))\n",
    "            else:\n",
    "                decoded_names.append(str(name))\n",
    "        name_to_index = {name: idx for idx, name in enumerate(decoded_names)}\n",
    "        annotations[hemi] = {\n",
    "            'labels': labels,\n",
    "            'names': decoded_names,\n",
    "            'name_to_index': name_to_index,\n",
    "        }\n",
    "    return annotations\n",
    "\n",
    "\n",
    "def _auto_roi_spec_from_fs(fs_annotations: Dict[str, Dict[str, object]]) -> Dict[str, list]:\n",
    "    auto_spec: Dict[str, list] = {}\n",
    "    for hemi, info in fs_annotations.items():\n",
    "        names = info['names']\n",
    "        keep = []\n",
    "        for name in names:\n",
    "            lower = name.lower()\n",
    "            if not name or lower in {'unknown', 'corpuscallosum'}:\n",
    "                continue\n",
    "            keep.append(name)\n",
    "        auto_spec[hemi] = keep\n",
    "    return auto_spec\n",
    "\n",
    "\n",
    "def _build_masks_from_freesurfer(\n",
    "    subject_fs: str,\n",
    "    roi_spec: Optional[Dict[str, list]],\n",
    "):\n",
    "    fs_annotations = _load_freesurfer_annotations(subject_fs)\n",
    "    working_spec = roi_spec\n",
    "    if not working_spec:\n",
    "        working_spec = _auto_roi_spec_from_fs(fs_annotations)\n",
    "        if not working_spec:\n",
    "            raise KeyError('Unable to derive ROI labels from FreeSurfer annotations.')\n",
    "        print(f'Auto-selected {sum(len(v) for v in working_spec.values())} ROI labels from FreeSurfer annotations.')\n",
    "\n",
    "    n_lh = fs_annotations['lh']['labels'].shape[0]\n",
    "    n_rh = fs_annotations['rh']['labels'].shape[0]\n",
    "\n",
    "    masks: Dict[str, tuple[np.ndarray, np.ndarray]] = {}\n",
    "    for hemi, roi_list in working_spec.items():\n",
    "        labels = fs_annotations[hemi]['labels']\n",
    "        name_to_index = fs_annotations[hemi]['name_to_index']\n",
    "        for roi in roi_list:\n",
    "            match_name = _match_token_to_label(roi, name_to_index.keys())\n",
    "            if match_name is None:\n",
    "                preview = list(name_to_index.keys())[:10]\n",
    "                raise KeyError(f'ROI {hemi}-{roi} not found in FreeSurfer annotation. Sample entries: {preview}')\n",
    "            label_index = name_to_index[match_name]\n",
    "            hemi_indices = np.where(labels == label_index)[0]\n",
    "            lh_mask = np.zeros(n_lh, dtype=bool)\n",
    "            rh_mask = np.zeros(n_rh, dtype=bool)\n",
    "            if hemi == 'lh':\n",
    "                lh_mask[hemi_indices] = True\n",
    "            else:\n",
    "                rh_mask[hemi_indices] = True\n",
    "            if not hemi_indices.size:\n",
    "                print(f'Warning: ROI {hemi}-{roi} matched label `{match_name}` but contains no vertices.')\n",
    "            else:\n",
    "                print(f'{hemi}-{roi}: {hemi_indices.size} vertices (FreeSurfer label `{match_name}`)')\n",
    "            masks[f'{hemi}-{roi}'] = (lh_mask, rh_mask)\n",
    "    return masks\n",
    "\n",
    "\n",
    "def _resample_fs_masks_to_transform(\n",
    "    fs_masks: Dict[str, tuple[np.ndarray, np.ndarray]],\n",
    "    subject_fs: str,\n",
    "    transform: str,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    if transform is None:\n",
    "        raise RuntimeError('A valid pycortex transform is required to resample FreeSurfer ROI masks.')\n",
    "    try:\n",
    "        import cortex\n",
    "    except ImportError as exc:\n",
    "        raise RuntimeError('Install pycortex to resample FreeSurfer ROI masks (e.g. `pip install pycortex`).') from exc\n",
    "\n",
    "    resampled: Dict[str, np.ndarray] = {}\n",
    "    for key, (lh_mask, rh_mask) in fs_masks.items():\n",
    "        surf = cortex.Vertex((lh_mask.astype(float), rh_mask.astype(float)), subject_fs)\n",
    "        mapped = surf.to_xfm(transform)\n",
    "        data = mapped.data\n",
    "        if isinstance(data, tuple):\n",
    "            arrays = []\n",
    "            for arr in data:\n",
    "                arr = np.asarray(arr)\n",
    "                if arr.ndim == 2:\n",
    "                    arrays.append(arr[0].ravel())\n",
    "                    arrays.append(arr[1].ravel())\n",
    "                else:\n",
    "                    arrays.append(arr.ravel())\n",
    "            flat = np.concatenate(arrays)\n",
    "        else:\n",
    "            arr = np.asarray(data)\n",
    "            if arr.ndim == 2:\n",
    "                flat = np.concatenate([arr[0].ravel(), arr[1].ravel()])\n",
    "            else:\n",
    "                flat = arr.ravel()\n",
    "        resampled[key] = (flat > 0.5)\n",
    "    return resampled\n",
    "\n",
    "\n",
    "def fetch_language_masks(\n",
    "    subject_fs: str,\n",
    "    roi_spec: Optional[Dict[str, list]],\n",
    "    transform: str,\n",
    "    cache_path: Path,\n",
    "):\n",
    "    if cache_path.exists():\n",
    "        with np.load(cache_path, allow_pickle=True) as data:\n",
    "            files = list(data.files)\n",
    "            if '__meta_version' not in files:\n",
    "                print(f'ROI cache {cache_path} missing metadata; rebuilding.')\n",
    "            else:\n",
    "                masks = {key: data[key] for key in files if not key.startswith('__meta_')}\n",
    "                print(f'Loaded cached ROI masks from {cache_path} (backend={data[\"__meta_backend\"].item()})')\n",
    "                return masks\n",
    "        cache_path.unlink(missing_ok=True)\n",
    "\n",
    "    available_by_type = list_available_pyctx_rois(subject_fs, transform)\n",
    "    masks = None\n",
    "    backend = 'pycortex'\n",
    "\n",
    "    if available_by_type:\n",
    "        try:\n",
    "            masks = _build_masks_from_pycortex(subject_fs, roi_spec, transform, available_by_type)\n",
    "        except KeyError as exc:\n",
    "            print(f'pycortex ROI lookup failed: {exc}')\n",
    "            masks = None\n",
    "\n",
    "    if masks is None:\n",
    "        print('Falling back to FreeSurfer annotations for ROI masks.')\n",
    "        fs_masks = _build_masks_from_freesurfer(subject_fs, roi_spec)\n",
    "        masks = _resample_fs_masks_to_transform(fs_masks, subject_fs, transform)\n",
    "        backend = 'freesurfer->pycortex'\n",
    "\n",
    "    np.savez_compressed(\n",
    "        cache_path,\n",
    "        __meta_version=np.array(2, dtype=int),\n",
    "        __meta_backend=np.array(backend),\n",
    "        **masks,\n",
    "    )\n",
    "    print(f'Saved ROI masks to {cache_path} ({backend}).')\n",
    "    return masks\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff907357",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_masks = fetch_language_masks(SUBJECT_FS, ROI_FILTER, TRANSFORM_ID, ROI_CACHE)\n",
    "roi_summary = pd.DataFrame({\n",
    "    'roi': list(roi_masks.keys()),\n",
    "    'n_vertices': [int(mask.sum()) for mask in roi_masks.values()],\n",
    "}).sort_values('roi')\n",
    "display(roi_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e0ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_story_bold(subject_fs: str, story_id: str):\n",
    "    h5_path = PREPROC_ROOT / subject_fs / f'{story_id}.hf5'\n",
    "    if not h5_path.exists():\n",
    "        raise FileNotFoundError(f'Missing preprocessed file: {h5_path}')\n",
    "    with h5py.File(h5_path, 'r') as hf:\n",
    "        data = hf['data'][:]\n",
    "    return data\n",
    "\n",
    "\n",
    "def roi_timeseries_from_masks(bold_matrix: np.ndarray, masks: Dict[str, np.ndarray], tr: float):\n",
    "    n_tr, n_vertices = bold_matrix.shape\n",
    "    frame = pd.DataFrame(index=np.arange(n_tr))\n",
    "    for roi, mask in masks.items():\n",
    "        mask = mask.astype(bool)\n",
    "        if mask.shape[0] != n_vertices:\n",
    "            raise ValueError(f'ROI {roi} mask length {mask.shape[0]} != data vertices {n_vertices}')\n",
    "        frame[roi] = bold_matrix[:, mask].mean(axis=1)\n",
    "    frame.insert(0, 'Time', np.arange(n_tr) * tr)\n",
    "    return frame.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def zscore(arr: np.ndarray):\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    mean = np.nanmean(arr)\n",
    "    std = np.nanstd(arr)\n",
    "    if std == 0 or not np.isfinite(std):\n",
    "        return arr - mean\n",
    "    return (arr - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d31cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_records = list_stories_for_subject(DATA_ROOT, SUBJECT_ID)\n",
    "story_df = pd.DataFrame(story_records)\n",
    "story_df['preproc_path'] = story_df['story_id'].apply(lambda sid: PREPROC_ROOT / SUBJECT_FS / f'{sid}.hf5')\n",
    "filtered_df = story_df[story_df['story_id'].isin(STORY_IDS)].copy()\n",
    "if filtered_df.empty:\n",
    "    raise RuntimeError('No matching stories found; update STORY_IDS.')\n",
    "display(filtered_df[['story_id', 'preproc_path']])\n",
    "\n",
    "story_roi_timeseries: Dict[str, pd.DataFrame] = {}\n",
    "for story_id in STORY_IDS:\n",
    "    bold = load_story_bold(SUBJECT_FS, story_id)\n",
    "    frame = roi_timeseries_from_masks(bold, roi_masks, TR)\n",
    "    story_roi_timeseries[story_id] = frame\n",
    "    print(f\"Loaded {story_id}: {frame.shape[0]} TRs, {frame.shape[1] - 1} ROIs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_story_id, first_frame = next(iter(story_roi_timeseries.items()))\n",
    "feature_cols = [col for col in first_frame.columns if col != 'Time']\n",
    "display(pd.Series(feature_cols, name='available_features'))\n",
    "selected_features = feature_cols[: min(8, len(feature_cols))]\n",
    "print('Plotting features:', selected_features)\n",
    "\n",
    "n_rows = len(selected_features)\n",
    "fig, axes = plt.subplots(n_rows, 1, figsize=(10, 2.2 * n_rows), sharex=True)\n",
    "if n_rows == 1:\n",
    "    axes = [axes]\n",
    "for axis, feature in zip(axes, selected_features):\n",
    "    for story_id, roi_frame in story_roi_timeseries.items():\n",
    "        axis.plot(roi_frame['Time'], zscore(roi_frame[feature]), label=story_id)\n",
    "    axis.set_ylabel(f'{feature}\n",
    "(z-score)')\n",
    "    axis.axhline(0, color='black', linewidth=0.7, alpha=0.5)\n",
    "axes[-1].set_xlabel('Time (s)')\n",
    "axes[0].legend(loc='upper right', ncol=len(STORY_IDS))\n",
    "fig.suptitle('Language ROI dynamics (pycortex-aligned)')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd744663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def takens_embedding(series: pd.Series, E: int = 3, tau: int = 1):\n",
    "    data = np.asarray(series, dtype=float)\n",
    "    window = (E - 1) * tau\n",
    "    if data.size <= window:\n",
    "        raise ValueError(f'Not enough samples for embedding: len={data.size}, E={E}, tau={tau}')\n",
    "    n_rows = data.size - window\n",
    "    cols = []\n",
    "    for delay in range(0, E * tau, tau):\n",
    "        cols.append(data[delay:delay + n_rows])\n",
    "    return np.stack(cols, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50facd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = selected_features[0]\n",
    "E = 3\n",
    "TAU = 1\n",
    "\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for story_id, roi_frame in story_roi_timeseries.items():\n",
    "    embedded = takens_embedding(zscore(roi_frame[target_feature]), E=E, tau=TAU)\n",
    "    ax.plot(embedded[:, 0], embedded[:, 1], embedded[:, 2], label=story_id, alpha=0.8)\n",
    "ax.set_xlabel('x(t)')\n",
    "ax.set_ylabel('x(t-τ)')\n",
    "ax.set_zlabel('x(t-2τ)')\n",
    "ax.set_title(f'{target_feature} attractor (E={E}, τ={TAU})')\n",
    "ax.legend(loc='upper right')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1fffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for story_id, roi_frame in story_roi_timeseries.items():\n",
    "    embedded = takens_embedding(zscore(roi_frame[target_feature]), E=E, tau=TAU)\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot(embedded[:, 0], embedded[:, 1], embedded[:, 2], color='tab:blue', alpha=0.85)\n",
    "    ax.set_xlabel('x(t)')\n",
    "    ax.set_ylabel('x(t-τ)')\n",
    "    ax.set_zlabel('x(t-2τ)')\n",
    "    ax.set_title(f'{story_id} — {target_feature} (E={E}, τ={TAU})')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6228c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "summary_rows = []\n",
    "for story_id, roi_frame in story_roi_timeseries.items():\n",
    "    embedded = takens_embedding(zscore(roi_frame[target_feature]), E=E, tau=TAU)\n",
    "    centroid = embedded.mean(axis=0)\n",
    "    spread = embedded.std(axis=0)\n",
    "    summary_rows.append({\n",
    "        'story_id': story_id,\n",
    "        'feature': target_feature,\n",
    "        'centroid_x': centroid[0],\n",
    "        'centroid_y': centroid[1],\n",
    "        'centroid_z': centroid[2],\n",
    "        'spread_x': spread[0],\n",
    "        'spread_y': spread[1],\n",
    "        'spread_z': spread[2],\n",
    "    })\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "display(summary_df)\n",
    "\n",
    "pair_rows = []\n",
    "for (story_a, frame_a), (story_b, frame_b) in itertools.combinations(story_roi_timeseries.items(), 2):\n",
    "    emb_a = takens_embedding(zscore(frame_a[target_feature]), E=E, tau=TAU)\n",
    "    emb_b = takens_embedding(zscore(frame_b[target_feature]), E=E, tau=TAU)\n",
    "    k = min(len(emb_a), len(emb_b))\n",
    "    diff = emb_a[:k] - emb_b[:k]\n",
    "    rms = np.sqrt((diff ** 2).mean())\n",
    "    pair_rows.append({\n",
    "        'feature': target_feature,\n",
    "        'story_a': story_a,\n",
    "        'story_b': story_b,\n",
    "        'rms_distance': rms,\n",
    "    })\n",
    "pair_df = pd.DataFrame(pair_rows)\n",
    "display(pair_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef5a46b",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "- Iterate across `selected_features` to export attractors for every ROI.\n",
    "- Feed these aligned ROI series into the EDM/CCM routines from Day 3.\n",
    "- Build pairwise distance heatmaps to compare story- or subject-level geometry.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
